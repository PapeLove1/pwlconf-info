    Zeeshan:  You've made it thus far, brains have been exploded. Natural numbers have be discussed. Next year we'll have a talk on integers. To keep with the theme. OK, so our next speaker was one of the -- early on, too, when we were coming up with a list for curating this, was a speaker like, you know, we wanted to do something on distributed systems, and then, you know, Heidi's dissertation came out, which covered all this work around distributed consensus and specifically all the implementations and things around Paxos, and everything you need, you don't have to read all these papers anymore, you could just read her dissertation, and then the work she's done, you know, based on Paxos, and I really want to recommend The Morning Paper. Kind of went through her dissertation and talked about various chapters if you want the cliff notes, I guess, about Paxos, and different implementations that have come out over the years. Heidi is at the University of Cambridge now. We had dahlia at the first conference, and Dalia had talked really well about Papers We Love so everything comes full circumstancing at Papers We Love 0 Conf, so without further ado, Heidi Howard.
    
    [applause]
    
    All right, good afternoon, everybody.
     AUDIENCE:  Good afternoon.
    >> Thank you, thank you, thank you so much for that introduction, and thank you to you guys for choosing to brave the depths of distributed consensus with me today. distributed consensus is the problem of how to make decisions in a distributed system.
    How to make decisions in a distributed system without assuming synchrons we can ...
    In fact, we see this in practice every day. I would claim that most distributed systems make use of consensus one way other another. Some use it directly to provide strong consistency, for example, over transactions on a database, others don't seem to use it at all, except they're containers that are then deployed and Kubernetes manages them and Kubernetes uses consensus is however famously tricky to understand. When I say I work on consensus, people look at me like I'm crazy, check I'm real. And they say things like oh, it's magic to me. So if you don't get every single aspect of every little thing that goes on in this talk, you are not alone, you are absolutely in the majority. Do feel free to come and talk to me, to contact with me, to speak with me at the end and ask plenty of questions. That's how we learn.
    So before we dive into the technical depths of distributed consensus, I wanted to talk a bit about my story so far and how I ended up on the stage today. So back in 2016, when Dalia was here talking to you guys, I was working at VMWare as an intern and I was working on how we can improve Paxos, and make it perform better, I wrote up this result in this paper, flexible Paxos, and then a few years later, I wrote my thesis on distributed consensus.
    It's 150 pages. It's quite an intense read. I was very impressed to see the morning paper covering it in such depth.
    And when I was about 3/4 of the way through my thesis which was all about improving the performance of Paxos, I said to my mentor at the time, I've got this greatened' said, are you done writing your thesis? And I said no, no, no, I've got this idea. At the time I was writing all these optimizations and I was getting stuck in the proofs. There was all these moving parts in consensus, I was beginning to understand it, but I was starting to not understand it, it was all looking very ugly and as someone who started as a functional programmer, I thought what if I moved from representing state as a mutable move to representing state as immutable. That would help me understand what was going on, it would make my proofs way cleaner, so I said to my mentors I've got this idea, I'm going to switch how I'm representing state and it's going to require me to rewrite my whole thesis
    [laughter]
    And he just went no. So I didn't do it. I needed to finish and I submitted my thesis on time, thankfully and passed my VIBR in Jan of this year but as soon as I was gone, I was right, I'm free, I'm going to write the stuff I wanted to write all along.
    And that is this paper here. So this is taking the work from the earlier paper, the Flexible Paxos paper, and my thesis, and talking about it using immutable state. And that is what I'm going to be discussing in more detail today. So I first came across the problem of consensus when I was an undergrad. I had this dream that I was going to build this amazing distributed system, it's going to be super high performant, low latency, high through-put, scalable. We're going to none of those wak consistency things, we don't need those, it's going to be super reliable. It's going to tolerate any kind of faults, it will be amazing. Oh, I was young and naive. The wakeup call shortly followed.
    So these two papers are a very great example of this. So one of these papers titles 100 impossibility results in distributed computing. That's it, just a paper that tells you a thing you can't do, and then it does it again 100 times in total.
    Thanks for that.
    And on the other side we've got this really infamous paper, so this paper is informally known as like the FLP paper or the FLP result and this says that solving distributed consensus is impossible.
    This is probably one of the like joint for the most famous paper in distributed consensus, and I would see this as being the first paper in distributed consensus.
    So if you have a technical problem, and you want researchers to spend decades working on that problem, write a paper claiming that it's impossible.
    [laughter]
    And they will put in a lot of effort to proving you wrong, and myself included in that.
    [laughter]
    And it's not just in old theory papers we see that we've got these problems with achieving consistency, and achieving consensus in distributed systems.
    This is a figure that I love, I use it to give students nightmares, it's not mine. It's from a really got paper from called Consistency in nontransactional distributed storage systems. It's giving an overview of the consistency levels that are available in distributed systems, even when they don't support transaction, so if you add transactions to this, it becomes far more of a mess.
    At the very top we've got this guarantee, which is linearizability, so this is the guarantee that when you send a request to the system, you get back a response and it will appear as if that request was executed atomically at some point between when you sent it and when you got it. That's a great guarantee, I mean our distributed systems look like it's not distributed at all. Wonderful!  Let's use that!  The problem in doing that is that we often need to solve problems like it consensus, or similar problems like atomic broadcasts in order to actually implement this in practice, and they're really kind of expensive and painful.
    And so we have this whole plethora of alternative guarantees that provide better performance or better fault tolerance than systems that implement linearizability, and this survey showed more than 50 of them.
    So to avoid getting too into the weeds today, this whole talk is just going to be about deciding a single value. That's it. You think that's not much for a talk. That was also the entirety of my thesis.
    
    [laughter]
    System. It's got two components: Servers, they're like servers, they're long-running, they have persistent storage, they're passive. Clients, they're like normal clients. They're active, they send some messages, they come, they go, that's it. And all we're going to do is just add a value. Now, I gave this talk earlier this year.
    And I had an attendee come up to me and tell me that I had stolen an hour of his life because he was expecting a blockchain talk because this talk was about consensus and I hadn't made it clear that I wasn't talking about blockchain. And I promised him, I said I'll make it clearer next time. I'm making it clearer!
    [laughter]
    [applause]
    
    When I say distributed consensus, I'm specifically talking about what we know as nonByzantine fault tolerance, so in my world, all the machines do what I tell them. That's not actually what my world is like at all. But they might be slow, they might be a little bit unreliable, and there is asynchrony.
    
    The problem we're talking about today seems so simple on the surface. We need to guarantee two things to solve consensus: Firstly, safety. All the clients in the system learn the same decided value.
    That seems reasonable, right? And progress eventually. A client will learn the value that's been decided. What makes this more tricky is the fact that safety has to hold regardless. It doesn't matter if the machines fail, if they come back again or not. If messages are dropped, or if they're reordered, we can't place a bound on how quickly a message is going to be received, how quickly a machine is going to respond, we can't rely on clocks, and we have to ensure safety regardless. However, when it comes to proving progress, we do have to to make some assumptions about the liveness of the system.
    So I said the FLP result was joint first for the most famous paper in distributed consensus. This is the other one. This is part-time Parliament by Leslie Lamport, if you haven't read this paper, I encourage you to read T it's also pretty far down my list of recommendations for actually learning about consensus, however.
    A lot of people have written much better explanations of Lamport's algorithms, including himself. It's quite a dense and peculiar read.
    So let's see what Lamport has to say about Paxos. The Paxos algorithm when present in plain Englishing is very simple. Brilliant!  Love a good simple algorithm. This among is among the simplest and most obvious of distributed algorithms. Think about that problem I showed you and you would derive Paxos on your own. Uh couldn't help it. It's like you'd fall over it. It's just there. Great.
    Why do I need a Ph.D. in this? His view, however, is not shared more broadly.
    [laughter]
    There's a reason why he got a Turing Award. The engineering community had some different things about Paxos, so these quotes are taken from the RAFT consensus paper. Paxos is exceptionally difficult to understand. we concluded that Paxos does not provide a good foundation either for system building or for education. He's quite brave to put that in a paper. So what I've hoped to have convinced you so far Paxos is difficult to understand. It's subtle. That's why people have problems with it, and secondly, Paxos doesn't perform well. It's slow to actually make any decisions. And that's why we can't have all our systems linearize it provides some degree of fault tolerance, but it doesn't provide us with amazing fault tolerance. So since Lamport published that paper and since the original FLP paper, there have been so many papers in consensus, I've read a lot of them. So I've really felt the pain of the number of papers there have been in consensus, literally hundreds of them and they're either trying to make Paxos perform greater, great, or they're trying to make Paxos more understandable, good. But they rarely try to do both and that's what I wanted to do and that's the pain, the wall that I hit whilst I was writing my thesis.
    So I'm going to go back to basics and change the way that we think about consensus in two key ways, the first, the more obvious one that I mentioned earlier was is immutability. So I'm going to use a system to write one registers when talking about consensus. This makes it so much easier for me to understand what's going on about the system and to reason what is and isn't safe behavior
    And second is generality. So when it comes to distributed systems, we can't have our cake and eat it. And so I don't think there's any point of improving packsos and trying to get a silver bullet, a better than Paxos system that will always give you better performance and better fault tolerance and consistency and just be everything. So instead what I want to do is counter with a general algorithm that is configured to take advantage of the different properties of different systems and to get different performance. Paxos is a one size fits all solution where what I want is a whole shop of different sizes.
    So today's talk has got three parts: Firstly we're going to reframe the problem of distributed consensus, using these immutable registers. Secondly, I'm going to explain this like famous and scary Paxos algorithm, and I'm also going to explain a substantial generalization over the Paxos algorithm that weakens its requirements and thirdly I'm going to put this generalization to some use hopefully and give an example of where we could use this in like a particular system. So, let's get on with it. Consider a world where we just have one server.
     AUDIENCE:  [inaudible]
    As one server, it's got one write ones register in persistent storage and we're going to try and achieve consensus. Client a long: They have a value that they want to be decided, in this case, A and they're going to talk to the server. I propose we decide this value of A, and server's like, OK, we can do that. We'll decide A. I'll keep a record of T I'm going to write it in a register of mine. I've accepted your proposal and the client is like, yay, great. I won, I got what I wanted. It's A. You can probably see what's going to happen now. Another client comes along. This time input value B, conflict.
    And so the client is going to send a message to the server and say, I propose B and server's like, nah, we already reached consensus, for safety, we can only decide one value, so I'm going to tell you, I already accepted A. I'm going to tell you about what happened.
    So the client learns the decided value was A. They didn't get the value that they wanted, but at least as a side effect, they've learned what the value is.
    This is great, in fact. This actually solves how I would define distributed consensus. As long as that server is up, we have progress. As long as that server is up. I wish I lived in that world. We don't.
    So let's make our system distributed and let's have three servers instead of one. Now let's try to apply a similar algorithm again. We could say you have to write, a client has to write to one register to decide about you, but then we could decide different values. We could say a client has to write to all the registers to decide a value, but then that would be even worse than the original algorithm, because now we have three servers and if any one of the three fails, we won't be able to make progress. So let's write to a majority of servers, two or three, right? That means that there would be an overthat between any two majorities and if one failed, we could still reach a decision. So let's look at that in practice. Client comes alodge, value A, proposes it to all three servers, the servers write the value A to their register. Registers, sorry and they respond with the accepted A message.
    Client learns that A was decided. Great. Now, the client comes along, input value B proposes. Servers this time is again, nah, we've already reached consensus, sorry. It was A that was decided.
    And the client learns the input value was A.
    We have got safety, which is great. Unfortunately, we don't actually have the progress property. So we could have a scenario like this, three clients come along, and each of these three clients talks to one of the three servers first and then we've got one register with the value A, one register with the value B, and one register with the value C. We said we wanted immutability property. We're pretty stuck now, there's nothing we can do without overwriting these registers, so we don't have progress. The progress means we couldn't get ourselves stuck in a deadlock like this, so we need a way of resolving this situation. Now, instead of switching to mutable registers where we're then going to have difficulty in ensuring safety, I want to switch to a world where we have a series of write-once registers. We're going to try to come to a consensus and if we end up in a split scenario like that, let's try again, we can try again until we succeed. And to save me drawing out far too many diagrams, I'm going to introduce the idea why of a state table. The state table represents the state of servers in a system. Each column is one of the servers, so we've got three servers here, and each row is a set of registers, and you can see these registers contain the same values as the previous slide.
    You'll note on here that some of these boxes contain a hyphen. This hyphen represents a special null value. We'll see what the null value is useful for specifically later, but at a high level, a null value is a way for a client to basically block a register, freeze it, and stop any of the other clients from et going in there and writing to that register.
    So we've got some registers, some clients that may or may not do some writes or some reads of these registers, but like when do we actually get some consensus? When does a client learn the decided value? This is where the generality idea comes in. We're going to say that a value is decided when it has been written to the same register on a non-empty. It should say non-empty there set of servers known as a quorum. So if you're familiar with distributed systems, you'll be thinking, quorum, majorities, read-write quorums, things like that. Right now there's no reason for them to need to intersect. We'll see later whether we do or do not need any intersection properties between them.
    And we'll represent our choice of quorums using a quorum table. So as I said, they can be any sets, they don't have to intersect. So here's just a random example that I picked.
    We'll see later that the ones that you pick will depend on the kind of system you're trying to deploy consensus on. So here register 0 has a quorum that's composed of two servers, register 1 has two different servers, register 2 onwards has either of those two pairs.
    And putting together the state table and the quorum table, we can see whether a system has reached consensus or not. So here's the majority quorums example that we saw earlier, so regardless of what register we're talking b we're going to use majority, so two or three servers. Here's an example state table and in blue we can see where a decision has been reached. Here we see two registers that are the same register that contain the same value, and we say that that means a decision has been reached. If a client sees this, they're good to go. Here's a different example of a quorum table and here's an example of a corresponding state table. Here we can see that two decisions have been reached, that are both the value A. The advantage of allowing multiple decisions to be reached is that if one of the servers is unavailable, we may still be able to make progress. So in this case, if server S3 failed, and another client came along, they could reach consensus again, on the value A, even though S3 has failed. I'll go into the details of how that works but just as a high level, it's useful to be able to have more than one decision on different quorums, as long as they all decide the same value.
    They don't necessarily all decide the same value, though.
    So here we have a quorum table, some quorums, and here we have a problematic scenario, right? The clients are free to write whatever values they want to write and read whatever is there, and they've decided to write two different values. This isn't safe the all.
    And it's not just on the same register set that we can have this problem. We can also have this problem across register sets, so here we see both the value A and the value B have been decided.
    So we can't live in this free and happy world where clients can just write to whatever registers they want and as soon as they see that a quorum contains the same value, they're good to go, we're going to have to place a restriction on when clients can write and this is that re striction.
    Before a client writes a value to register i, it must ensure that no other values could be decided in registers 0 up to i, so I'm going to write into register i, but it's my responsibility before I do that to check that none of the previous registers could decide a different value. So everyone has to check that they're not violating safety. Each of the clients needs to make that check and then overall we're never going to get two different decisions.
    So that's the end of part 1. We've defined consensus with immutable state. But then we tried to get a solution for 2, it didn't quite work out, we saw a vague safety rule, we're not really sure what's going on, we haven't solved the problem yet. This is where Paxos comes in. We're going to split that safety rule into two parts. We're going to have the -- before a client writes a value to a register i, it must ensure that no other values are decided in the register set i, so for any given row in that table, I need to check that no other decisions are going to be made in that row for a different value, and secondly, I need to check that the previous registers to check that a decision isn't going to be made.
    And Paxos handles each of these separately. So a solution to the first one is referred to as the register allocation rule.
    Paxos allocates registers to clients in a round-robin fashion, and then tells those clients only write one value to that register.
    What that means in practice is that only one value is ever going to be written to any set of registers. If only one value is going to be written to any set of registers, only one value is ever going to be decided by that set of registers.
    it's actually a stronger guarantee than we needed. So that's how we've solved the first of these two requirements. That was the easier one to solve. Now we're going to look at the second righter. How can a client ensure that it's not going to write a value that's different from a previous decision, and this is how Paxos implements this.
    Paxos requires clients to read at least one register from the quorums of register sets 0 to i-1, so if I'm a client I'm going to write at i, which is full, say, I need to talk to all the previous forums, I need to talk to at least one person in each of those sets and ask them what happened, what is in your register, tell me about the state of the system before I arrived here in the system.
    Firstly all those registers need to be written. If they're unwritten, another client could come in, write a different value, and cause a safety violation.
    But what if they're not written? That's where with the null value we saw earlier comes in. So what the client is going to do, is they're going to read them and if they're empty, they're going to write null, they're going to freeze the system, they're going to say stop, whatever happened happened, and I'm going to learn about that but don't start writing new values to these registers, don't make decision, because it might be different from what I'm going to do, so I'm going to write nil to block that register, stop anyone else getting in there.
    Secondly, the client, when they do the write, has to write the same value as they saw from the greatest register that they read in this first part. So that means that decisions may have been made in the past, and so I'm going to read the values and if I see a value, I'm going to write the same value. That's kind of sad for clients, right? They may not get to write the value they actually want to write. They might be left picking up the pieces from another client. They could be running at the same time as another client, the other client could have just failed or gaven up and walked away, so they're going to clean up the mess and learn from the decision that was made. But they have to for the purposes of safety to write the same value to make sure there's not a conflict in decisions.
    So that's how we deal with the second of these two safety criteria. So at the high level, Paxos is a two-phase algorithm, which means it can make a decision in two round trips. It may take more than that, but two is the minimum that we're going to have to do.
    And this first phase of the algorithm just ensures safety. Doesn't actually do anything useful towards consensus, all it does is implement those two rules we saw before. The second phase is where the client actually gets to start writing values into registers and knowing what they've been written, can say like, oh, a decision has been reached, so the real like the work gets done in the second phase and the first phase is just checking for safety, ensuring that we're not going to violate safety. We didn't care about safety. We can just skip that phase altogether. And Paxos requires all quorums to intersect. And so it tends to use majorities, so like an obvious, you want all your quorums to intersect, people understands majority, and then if a minority failed, the system should be up, that's simple enough, right
    So now we're going to delve deeper again. It's like an onion, we're peeling off all these layers, we going to delve deeper into how Paxos works, one phase at a time. This is Phase I read the registers, and freeze it. So write some nil in there, if we see in two registers, so it's basically read and freeze -- freeze and read, actually, so you know what you can do safely in the next phase.
    So to begin, a client chooses an allocated register set i, note the word allocated there. This client just can't use whatever register they want, they have to use one of the ones that were allocated and they're going to send a message, prepare i to all the servers.
    When a server receives the message, prepare i, it's going to look at its registers. If there are any unwritten registers between 0 and i-1, the server is going to write nil into those registers, it's going to block them out, stop them from changing. And then that server is going to respond to the client with the message, promised. Promised includes the register set that we're talking about here. And then if the server has any registers that contain values, it's going to return the register with the highest register number and that is going to help us implement the second rule that we saw.
    So the client waits and collects these promised messages from the servers and once it's got enough messages, a quorum of messages, it's going to choose the value v from the greatest register that it saw, from these promised messages. If it doesn't see anything, if all the messages are empty, then it could propose whatever value it likes, which would be the input value. So at this stage we haven't written anything of use to the system, but we have learned, what register can I safely write to and what value can I safely write to that register.
    In the second phase we actually get to start writing to registers. So the client sends the propose message to all servers, this is similar to the propose message we saw earlier, except now it includes i, which is the register. When the server receives the message, propose iv, then it writes the value v to the register and responds with accepted. So accepted means, like I have done that, you're good to go.
    The client terminates when it's received accepted messages from a quorum of servers, so it knows that -- I've been successful, I've written that value to enough registers, and I've learned that that is the decided value. I can output that value.
    That might have been a lot to take in. So we'll work through an example now.
    So in this example we've got three servers. These are the circles, and we've got a state table on the side. Currently empty. We've not done anything. System has just woken up and is new to the world.
    Client comes along. As you've seen before, client 1, input value A and they're going to start the Phase I of Paxos, they're going to send the message, prepare 1. The servers are going to receive this message, look at their registers and each of them are going to see that they've got a register that's unwritten so they're going to write null into. And the servers are all going to respond. Once we've received two of three, once we've received a quorum, we can proceed to the second phase and we've learned that the client can propose its own value, A, to register 1. So we see that in the proposed message. The servers write the value A into register R1 and they respond, saying I've accepted your proposal, I've done as you've asked me to do, and so when the client sees this from two or three servers, they're like ooh, consensus reached, I'm going to output A.
    Another client comes along into the system. They're going to send the message, prepare R2. Registers were allocated round-robin, say. The servers don't need to change their state at all. There's no unwritten registers, but they are going to respond with the greatest register and the value inside it.
    So in this case they're responding with the greatest register, R1, and the value that's in R1A so they're letting the client know, basically will this message reads: If you want to write to R2, you should know that the greatest value I've seen is A in R1. And the client collects these messages again, once it's received two of them, it can safely proceed to the second phase. Unfortunately this client isn't going to be write their own value B, they're going to have to write the value A. So the client sends the message, propose R1, R2, A, write the value as requested, and server responds saying, hey, I've accepted your proposal.
    And the client learns the output value.
    So Paxos requires each of these two phases to use the same quorum, an intersecting quorum, regardless of whether you're Phase I or Phase II, it doesn't matter whether you're writing into register 7 or register 4. It doesn't matter, you need intersecting quorums, if you need intersecting quorums, majority is an obvious choice, right? But in the beginning I said don't assume they intersect, you can use any set you want. And in fact, you can use any set that you want. So we have a new, much weaker quorum intersection requirement. This is what was shown in the Flexible Paxos paper that I showed earlier. When you're doing a quorum table, you can put whatever quorums you want in there. Any non-empty sets, don't have to intersect at all. The only thing is that when a client wants to reach consensus at register i, when they want to write to that, they need to get at least one participant from the previous quorums to participate in their first phase, but that is it. There is no quorum intersection requirement between two quorums for the second phase of the algorithm, say.
    So that was a lot of very kind of general, high-level things. Let's have a look at an actual example of this.
    So classic Paxos, as I mentioned earlier, uses two phases. Any client can decide a value.
    If you have like seen a previous consensus system, however, like Raft or Zookeeper, any of those, you know that you don't run two phases of the algorithm every time you want to get something done. Instead you elect a first phase to elect a leader.
    And this means we can reach consensus in just one round trip. That's great. One round trip is half of two. The problem is there's only one client who can do that. The client who's a leader. If I'm a client who's not a leader, I have to send a message to a leader and then the leader has to send me a message, which is a lot like a round trip. So we ended up in the same place again where in practice we really need two round trips again. Wouldn't it be great if we could have an algorithm that could allow any client in the system to achieve consensus in just one round trip? Now, it depends on how important this is, it depends on the system you're deploying, if you're deploying over a large geographic area, this might be very useful. If your three machines are on the same rack, which they shouldn't be if you're deploying consensus, because if the rack fails, then you're out. Then maybe this doesn't matter. Maybe two round trips doesn't matter. But I think in some cases I think this is important.
    So as I said with coming up with very general algorithm, we're going to specialize it for the properties of a specific system. So I'm going to assume two things, firstly I'm going to assume that these failures are rare. We will tolerate failures, in fact, we'll tolerate the same failures as Paxos, but we're going to assume that they don't most of the time so we're going to make use of more nodes but we're going to tolerate the fact that a minority of servers may fail.
    And secondly I'm going to assume that each server, physical server, hosts both a client and a server, so this is how most of these systems are set up. Consensus is used to implement atomic broadcast or primary backup replication and here it's often the case that the servers are actually making decisions between themselves. This has the property you have a server that you can talk to for free. They're running on the same host as you. You don't have to send a message over a network, message might be slow, get delayed, you've got free access to a message system, but everyone else is over a network, they're expensive.
    So here's an example of what a quorum table might look like for a system such as this. We have three participants and we've partitioned at 9, arbitrary number. Don't worry about it. The first set, so that's 0 to 9, I've said if you want to reach consensus, you have to invoke everybody. All three servers have to be on board for you to make a decision. As a client you've got to write into three registers. Register 10 onwards, however, just two. Two is fine. Just like Paxos, just like we saw before, the familiar world that we had.
    So what this does is gives clients two options as to how to reach consensus, there's firstly what I'd call the fast path. Again, slightly dependent on what system you're using, if waiting for the extra server is actually really slow, this wouldn't be the fast path, but this is about optimizing for a particular system.
    The in first path, the client executes Phase I just with its local server. It doesn't need to wait. You only need to intersect with quorums from lower register sets. If we include everyone, then everyone includes me, so I can just talk to my local server for execute Phase I, nice and quickly, all is good, and Phase II, I'll include everybody.
    At the same time, we still have option to do the Paxos the way we always did it.  We're going to include a majority of people in Phase I and a majority of people in Phase II. We have a new best-case scenario, we have a new option for clients to take, a new path that they can take, but the old one is still there. We just go back to doing Paxos, doing consensus the way that we were always do doing it. The advantage of this is a client can now -- any client in the system can now reach consensus in just one round trip. The down sides is that we've inceased the quorums to majorities for all the participants and we may need more rounds if failures occur. That is just one example of how we can use this. This is I think a new more interesting example. There is, if you look online a more common example of how Flexible Paxos is used with multi-Paxos, but I included a different example from usual just for fun.
    So what have we learnt? Immutability, I think really helps with reasoning about what's going on in our distributed system and I think it could help more broadly in areas beyond distributed consensus, and generality allowed us to specialize the algorithm for the needs of a particular system, as we saw with the all board consensus. Paxos can chill out. It can relax. It can relax its requirements for quorums to have intersect and in my thesis I show a bunch of other relaxations of how we don't need quorums at all, you can just proceed safely in many cases without quorums, how the client doesn't need to follow that rule, etc., etc., there's a lot more things we can do to play with this algorithm. And in terms of that playing there's a lot we can achieve, as well. Different quorum systems give us different tradeoffs for different scenarios.
    Paxos with this is one approach to consensus based on two phases and majorities, but there's loads of ways we can solve this problem. You don't have to take this one size fits all solution.
    That's it for me. And I will be up here afterwards if you want to ask any questions.
    
    [applause]
    
    Zeeshan: You can take like two. You could do like two questions.
    
     AUDIENCE:  Thank you so much. That was great. To your last point, is there any example you can think of that would really benefit from distributed consensus that's absolutely not well served by Paxos, or sort of similar algorithms?
    I think the main area where Paxos falls down is over a large geographic area. When you care about latency, having two round trips is painful. So I think -- I think, you know, splitting when your system goes from being in one datacenter to across multiple data centers, which is a really good thing to do when you care about fault tolerance. I think that's a nice use case. Paxos for a long time was relegated to configuration management and it was very much like a side line things and you wouldn't put Paxos on like the critical path of your system whereas since the Raft paper, now they're putting Paxos right on the critical path of the system and so generally with this optimization and with the more normal uses of it, you can reduce the quorum that you normally do, the second-phase quorum from a majority to a smaller group and that allows you to reach consensus just faster, which is useful if you're going to put Paxos right in the middle of your application.
    >>
     AUDIENCE:  So with the extensions that you've put onto Paxos in this discussion, is there still a strong case for using Raft in live systems versus the implementations that you've discussed here?
    >> That's a great question. I -- I'm -- I'll try not to get too deep into the answer because I could talk all day about this. So Raft isn't that different from Paxos, mostly what the Raft paper did is an incredibly good job of explaining what was going on, and being specific about how to use Paxos to implement state machine replication and therefore, the difference between multi-Paxos and Raft is not very big, so it really doesn't matter which one you're using in practice. The only thing I would say is to make Raft more understandable, they've used a more strong leadership model. So they elect one of the participants as a leader and they're like, you're in charge, you do everything, it's all your job. Which makes the leader which was already a bottleneck an even bigger bottleneck, and if you care about performance, you probably shouldn't be using Raft in the first place. You should be using a more normal, kind of multi-Paxos, or like Zookeeper kind of thing, with a weaker leadership model.
    
    ZEESHAN:  We'll take you offline for the next talk. Heidi is here for all of Strange Loop, as well, so she will be around and you can talk Paxos all week if Heidi wants, of course. Thank you so much.
    [applause]
    
