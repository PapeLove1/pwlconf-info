    
    What About Natural Numbers?
    José Manuel Calderón Trilla.
    Zeeshan: All right, let's get on in!  cool, all right, so for this next talk, José here, was the first speaker that we confirmed for Papers We Love 2019, and we did it in 2018 at the last Strange Loop, there was ICFP was co-located here, for some of you who might remember, and I remember randomly we were talking about Papers We Love and he was talking about a paper and he was like, there's this anniversary of a paper next year, it will be 30 years and it's about natural numbers and he goes, I really want to talk about this paper and I was like, that sounds great and
    [laughter]
     And then we've been talking for a while about this talk and I came back and I'm like, are you still ready to do the talk? This is now 2019 and José was like, yes!  So also kind of funny, it's my second time being an emcee for José here, who is just an awesome speaker and so we were really happy to have somebody confirmed last year and we were like, José is definitely going to do it and he was like, this is the paper. So is everybody here excited for natural numbers?
    [cheers and applause]
    
    Is
    >> Thank you. Thank you. Yeah. All right, everyone can hear me all right? OK, good, yeah, so the natural numbers, I've never had a hype man for natural numbers before
    [laughter]
    So this is a new experience for me, as well. I want to talk a little bit about how I discovered this paper, you know, have some personal anecdotes. I did my Ph.D. in York, in the United Kingdom, and York is an interesting town. It's actually quite small, it's in the north of England, it's very, very old and one thing that's kind of special about York is it still has its Medieval walls, and you know, originally they were built to keep out Scottish people, but actually, you know, it kind of makes York feel very like calm and safe because you're always within these big wall, and it's not that much of a bustling town, the pace is kind of relaxed which I really like, which means is you don't feel guilty to really think about things, you know, like, what about the natural numbers
    [laughter]
     And the person who wrote this paper was my advisor during my Ph.D. and he wrote this paper exactly 30 years ago, also the year Taylor Swift was born. So 1989, huge year for modern life. And so the natural numbers, really, you know, what are they? Right? Well if you ask a mathematician, they might draw this and say this is a set of natural numbers. For most people they kind of know it as integers, but without negative values, which is fair. But really what about them? Right?
    [laughter]
    Why are we talking about the natural numbers? They've been around for a long time.
    [laughter]
    And I'm here in 2019 and I'm saying you should really consider them.
    [laughter]
    OK. So here's a quote from the paper. This is actually how the paper starts, you know, some 30 years in the history of machine-independent programming language design, the treatment of numbers is still problematic. That's a hip word, right? We say problematic all the time these days, but Colin was using it 30 years ago, and I think this quote is interesting for a few reasons. So for one reason, it's kind of weird to think about how machine-independent programming languages was, you know, a thing someone had to come up with and when this paper was written, it had only been around for 30 years, they were still kind of figuring it out, and a lot of ideas were still yet to be discovered. But interestingly, I -- I we're here 30 years later, and the numbers in our programming languages are still problematic, so I, you know, that's me. I said that.
    [laughter]
    Lets really make sure we think about this. OK, so if you wake out of this room and you mostly think, wow, that guy is really excited about the natural numbers, that's a fair takeaway, but what's my hope that you take away, right? And what I want you to walk away with is that the numbers we use in our programs should relate to the structures we're using in our programs, right?
    So for some domains, the reals may be the appropriate number system. Right?
    You know, if you're doing physics or something like that, and you're dealing with volume or speed or mass, then the reals, or some approximation of them, is the appropriate number system. I don't think you should use the natural numbers for that. And for many problems,* intergers are appropriate, so for example, you may decide we have a fixed domain, like in digital signal processing and you may say, actually, if we represent all of our possible spaces as a 16-bit integer, we know what our error is, and we know what the noise floor is and so you've decided up front this is the appropriate representation for our domain. And then of course, what else could integers be used forb? Maybe your bank account balance, that's unfortunate, but they canning negative, so really you should go to your bank and say, what about the natural numbers?
    [laughter]
    So what's the main thesis of this paper? It's for many of the discrete structures involved in day to day practice of programming, the natural numbers are the most appropriate number system. So what are these discrete structures he's talking about? He's talking about graphs, he's talking about tree, he's talking about lists, things like that, our kind of bread and butter for structures.
    And he's saying, even though the natural numbers most closely relate to those structures, we're not using them, which is odd.
    And so how am I going to give this takeaway, right? So we're going to think about the natural numbers, we're going to talk about the natural numbers and in that process we'll develop a small API of arithmetic and some other other systems, and we'll see how being very careful about the number system we chose it kind of forces us to make certain decisions or pushes us in our API design just by the choice of saying, you know, we really want these to be natural numbers. I want to convince you that the natural numbers are good and proper, and maybe as a bonus, I'll convince you that laziness is nice, but I admit that's a long shot, and I want to demonstrate that even simple choices in APIs, like using a NAT instead of an int, have profound consequences. There's kind of a mantra on Twitter these days, make illegal states unrepresentable or make invalid states unrepresentative. With a show of hums how many have heard that before?
    (Humming)
    I work at Galois, that's this one here, and we do this 2348 our eyes bleed.
    Really what I want is that you ask this question all the time: What about the natural numbers? OK, so I want to give you a little path, because I tend to meander, so in order to keep myself honest so if you know if I'm going way off the rails, here's what we're going to do, we're going to look at the natural numbers themselves. We're going to talk about programming with Nat, which are not natural numbers, we're going to talk about doing arithmetic with Nat and some properties we may care about. We're going to talk about how does Nat influence API design and we're going to talk about implementation designs, because someone has to implement these things that semanticists dream up. And we're going to conclude and you'll either clap because I'm finally done or because you really liked it, one or the other, but definitely clap, all right?
    There is a few definitions of natural numbers, right? If you've taken a theory of composition course or if you're really into Wikipedia, you might see there's a set theoretic definitions. Or Peano, I don't know how to pronounce t I should have looked that up.
    So let's look at set theoretic definitions. It says 0 is the empty set. Ooh, you all say, wow!, OK. And then you make the next number by taking the previous number and doing a set union with the previous number inside another set.
    OK, that's weird. And then you just keep doing this, but I'm kind of cheating here, because aim using the syntactic kind of Arabic numerals, but what if we -- oh, my gosh, we don't like that.
    So instead of setting yourself up for success, let's set yourself up for SUCCess,
    [laughter]
    So in 1989, which was 100 years before Colin's paper, right so really it's kind of like a double anniversary if you don't think about it very much. And there were many axioms that were proposed about the two we care about is that they're axioms, so we get to declare what's important about the. That's what nice about axioms, and we're saying O is a natural number. You don't have to agree with t we're just saying it and we're saying if you give me any had natural number, any, I can put an S in front of it and you've got another natural number and you can do nothing to stop me. That's what these axioms are saying. So fair enough, this may be a little less eye-bleedy than the set one, right? I think so at least.
    So those are national numbers but really I keep talking about the practice of programming and I'm here talking about sets or I've got upside-down A, like, what the heck's going on? OK. So let's represent the natural numbers as a data type.
    Now, this isn't a language that exists. This is my idealized functional programming in the style of José that if I ever put my money where my mouth is, I'd implement it, but I can put it on slides and force you to read it.
    So what this is saying is that Nat has two constructors, one of them is zed. My wife is Canadian, so I have to say zed and the other is succ, which takes a natural number and returns a natural number. This kind of looks like those axioms, right? We're saying zed is a natural number, you don't have to do anything to it, and if you give me a natural number, I can wrap succ around it and that's another natural number.
    [laughter]
    Hm.
    [laughter]
    So using that data type, we can represent any natural number we'd like. Now, why am I saying that? I want to be careful, you know, because if you take enough programming language theory or you go through the onslaught that is a Ph.D., you have to be very careful with these semanticists and what I'm trying to say is that the data type I just defined is not the natural numbers, it's a representation of the natural numbers, but we can show the correspondence, so these fancy brackets -- oh, my!  OK, these fancy brackets, they're called Oxford brackets and they just mean the meaning of. What's interesting about Oxford is that they have a comma and a bracket named after that. If I were on their marketing team I'd really push that.
    So the meaning of zed is 0, the natural number, read, and the meaning of the successor is whatever the meaning of that value is plus one, right? Fair enough. OK, so now I've convinced you that we can represent any natural number we want with this data type.
    The talk's over, thank you.
    OK, no, no, no,
    [applause]
    
    Yeah, thank you, thank you, yes, yeah, yeah, the ergonomics really suck on this one, right? That's right, so to represent 3, you have to have that. And then even just look everything at this, my wrists start burning, right?
    This is not nice, right? This is not what we'd like in programming. So what do we do with other data structures? How do we solve that? We solve it with this notion of syntactic sugar, right, to make something a little sweeter. So lists are a good example. Here is the definition of polymorphic lists in my idealized language. You have the empty list, which is a list, and then you have the cons operator, which takes some element, takes another list and gives you another list. It's straightforward. But it's the same exact program. Even though we like lists and functional programmers can talk all day about lists and what they represent.
    And you can say a list is just a string of characters and you're like, let me make a string and actually true story, I was going to do PLWC for Conf and I got so tired of typing that I stopped at PWL. And this was such a big problem that early on, compiler writers were like, OK, that's not feasible, let's have some syntactical sugar, right, so you can write a string with double quotes and the compiler for you translates it into this lists cons structure and it does that without any loss of information, right? You know exactly how it's going to do it and you can still pattern match. And furthermore, if you're in a language like Haskell, you can have this fancy range notation where you can say oh, I want a list that has all the numbers, 1, 2, 3, and originally I was going to do up to 10, but then I got tired of typing the other things, so -- so this is why syntactic sugar is nice.
    So of course we can do syntactic sugar for the natural numbers, right? We don't have to be writing all these succs, or zeds or whatever, we can say let's put more work on the compiler writers, right? They say they like it, so --
    [laughter]
    OK, all right, so we lose nothing. We get to write in this syntactic sugar that's nice and ergonomic, but we still get the reasoning power of the use is a. So you can say something like is the length of x is less than 5 and that 5 can be a Nat, right, and you can at the same time write these nice pattern-matching definitions where you're forced to deal with all the constructors and really reason about the meaning of your function, right? To me this is a big win. There's a lot of talk about what's functional programming? Did you know the higher -- I hate all that crap, right, so really to me, the big win is algebraic data types, higher order functions and pattern matching, right? Right? Oh, yeah, that's right. I may be fired. But you know, but yeah, so I love that this is win-win. We get the syntactic sugar, we don't have to break our wrists, but at the same time we get this nice pattern matching.
    OK, so I've convinced you that just because we're using this recursive data structure doesn't mean we have to get RSI, OK, now let's talk about arithmetic. So buckle up folks, we're talking about some functions here.
    OK, so a lot of programmers for some reason expect arithmetic to be useful on their numbers and in particular, I think a lot of people would complain if you didn't have addition, subextraction, multiplication, division, right?
    So when programming with the structures *
    We want a correspondence between the number system we're using and the structures we're reasoning about, right and the reason I bring this up now when we're about to talk about arithmetic is this is the core idea. The claim is that the natural numbers correspond to the structures we care about. So when we're designing the API for our arithmetic, we should make sure that we hold that correspondence, right? That's the core of this talk. So think, for example, array indices, or the size of some structure, the size of a map or a size of some set. What language would ever do that? So this comes from the Haskell prelude and I feel very comfortable making fun of Haskell because I'm on the Haskell committee so it's like kinda my fault, I don't know. But here you ask for the length of some finite structure and it returns an integer. Come on!  Colin has been talking about this for 30 years, why isn't anyone listening? So think about how many APIs you've used that return in, right? And how many of those are just using the negative numbers to signal errors, right? By word of hum?
    Audience: Hum.
    >> Yeah, I know, it makes me sad, too. Yeah, OK. All right, so now we know we want this correspondence, we know we don't want negative values, so let's think a little bit. What do we want out of our arithmetic, and one thing we may conclude, or two things, actually is we want them to be total, and we want them to be closed. If you don't know what those mean, that's OK, I'll explain.
    But basically these two in combination means that whenever we operate on two natural numbers, we get a natural number back and it's with those two properties, right? This isn't true for all of arithmetic over every number system. But that's OK, what's weird is that some numbers fail even where it should be true, right? So a function being total tells us that for any valid inputs, you get a valid output. It doesn't throw an exception or halt or anything like that.
    And for a function to be closed, means that the result values lie in the same number system as their arguments, when we're talking about arithmetic and number function, so let's go back to Colin, he's the reason we're talking about all of this. The aim is a total closed system of arithmetic with results that can be safely interpreted in the context of the discrete structures in functional programming. That's what we're going for.
    So back to arithmetic. You have addition multiplication, no problem. But what about subtraction? This is the big hiccup, that whenever I rant about t most people don't invite me to talk about it a year later, most people are like, but have you thought about subtraction?
    [laughter]
    So we can do saturating subtraction, and this isn't always the right call, but the fact is for the functions we're talking about, it is the right answer. I it looks like a nervous person hanging upside down.
    Saturating means you would never go below 0, so a lot of languages don't have this, right? They're underflow or they'll just cast to an integer or do all sorts of things, but they won't saturate, and the claim here is that for the discrete structures we care about, saturation is the right call.
    Let's make that correspondence concrete, right, so in a lot of functional languages, you have this function called drop that works over lists and you have some lists and you say drop the first n elements, right? And here's a definition, you know, using the natural numbers, so if we're trying to drop 0 from a list, you just return the list. Makes sense. If you're trying to drop anything from an empty list, then the result is an empty list. And if you're trying to drop some number greater than 0 from some list that isn't empty, then you just drop that element and keep going, make sense? We all agree?
    So here's a correspondence made concrete, right? What we're saying by agreeing to saturate a subtraction is that we think the length of dropping should be equal from taking the length of the list and then subtracting n from that number. I see some heads nodding.
    These are the sorts of properties that we use when programming all the time. We may not make them implicitly. We're not proving theorems about our programming all the time. As much as I know some of you wish you were.
    But we're also asking questions like is it correct to refactor this in this way, right? So when dealing with with discrete structures, this one is you can have in your tool belt -- or toolbox, whatever, not everyone wears belts.
    What about division? Subtraction I've convinced you all, what about division? Well, what's interesting about division is it's already closed over natural numbers where it's defined, right? And what I mean by that is there's the divide by 0 problem.
    Yes, this is a -- this is a deep problem. It's so deep that some mathematicians say natural numbers start from 1, right, and what if we did that. Well, let's think about that. It would solve the divide by 0 problem. So it's tempting.
    But then we lose the correspondence with data structures, right?
    So again, this is why we really want to think about what are we representing with the number system and constantly use that when we're designing our APIs, right? So quick digression, why is 0 important? It's because 0 is not nothing, and this is a subtle point. Like the abstraction that is 0 wasn't always with us in civilization, someone had to come up with it. And here's an example. If I don't class my result is nothing. But I can get a zero if I take a class. I have done that.
    So Runciman proposes two solutions. One is division by slicing and the other uses lazy Nats. So I know when I get to lazy Nat, it's going to be in the more controversial part of the talk so let's delay that a little longer.
    So if we think of dividing x by y as in cutting x and y in places what would that give us? Let's imagine into existence, some prim it have operation that is the division we know and love, which is if you pass it a 0, it will blow up your computer. OK.
    We can still get total divisionusing that as a primitive. And here's how. You are just add 1 to whatever you're dividing by. I know, you're all impressed.
    And you say, why would you do that? It only costs you that it's wrong at every other number
    [laughter]
    Which maybe isn't ideal, but it depends. To each their own. But we can get back to the correct thing by saturating our subtraction, so this division with dots around it we'll call that natural division or SLICing division and what we're claiming is well, slicing division is using a primitive where we subtract one from the divisor,
    If it was 1, we go to 0 and then we add one, OK, and now you're thinking you're a coward, you coward, you were telling us that the natural numbers were all beautiful and pretty and you just hand-waved away the problem and you're right, and I have to confess.
    So if you think I'm being lazy, wait until I teach you about laziness, OK, so here's the other solution: Which is, let's have our numbers be lazy.
    Now, who's familiar with laziness? Yeah, some. OK.
    Lazy data structures are data structures where you don't compute the parts that aren't needed by the rest of your program.
    The only kind of well known lazy language is Haskell. There's a few others. There's Clean, there's Miranda, there was lazy ML in the '80s, but Haskell is a big one. Colin is a big advocate for laziness, but one of the nice things about laziness is you can have infinite structures, so if you can have infinite structures, why not have infinite naturals, and in a lazy language, that is completely valid definition of infinity, right? It doesn't matter how much you try to traverse down this structure, it will always create more. Now, you may say, but José, memory is finite and I would say, why are you doing functional programming?
    [laughter]
    OK, now, so we can do division in a lazy way, but this way, we can say if you're dividing by 0, it's infinity, and that's OK, this will still terminate, that's fine.
    And otherwise, you can just divide the normal way and you may say, OK, well, that looks like some weird exceptional value, I don't know if I really like it, what are you up to, José and actually we can define it this way, we don't have to use any special primitive function, right? So here we're saying if X is less than Y, then the result is 0, right?
    Because we're talking about slicing. And otherwise, it's a repeated subtraction, right? That's kind of what division is. And if you'll notice here, if you're dividing by 0, this would result in infinity.
    And we haven't cheated, we haven't used some silly primitive function that the compiler has to write. These are just using our, you know, saturating subtraction. And recursion, of course.
    OK. Fair enough. What about the power, like taking something to the power exponentiation, that's what they call T so what's interesting is it's not closed over the integers, and for this this, Haskell has like three component operators. Pascal didn't even try.
    So but over the naturals, it is closed. Right?
    And we can show it right here. If you -- if you take something to the 0th power, it results in 1 and otherwise it's just a repeated multiplication, nice and simple, no problems.
    OK, so I understand that laziness is controversial. Not everyone's a huge fan of T not trying to start a war here.
    However, I will do some plugs for laziness, right?
    These infinite values that we can only get with the laziness let you avoid the cheating that's often found when you're dealing with algorithms. For example, how many of you have seen something like this before? I won't make you raise your hands, but think why yourself in your head, how many of you have done this sort of thing before, and often like you think of Dijkstra's algorithm where you need so set some upper bound and you're like, I doubt if it will ever get this high and you just put some value.
    [laughter]
    So with lazy Nats, it is actually infinite, right? And then if I were to ask you, are there more than ten people in your company, right, your workplace, or in your family, do you count them all? Right? So what if it was really expensive to count them all and you still wanted to ask this question?
    With strict natural numbers you would have to do all of the counting and then say, oh, yes, 25 million is bigger than 10, yes.
    
    [laughter]
    So lazy numbers let us compare the sizes of things without even fully computing the sizes of things.
    Because you only go as far as you need to know to know, is it bigger than 10? Once you get to 11, yeah, it doesn't matter how big it is.
    OK. So that's mostly all I'm going to say about laziness, mostly.
    OK. So how else does Nat influence API design? We talked about arithmetic. Hopefully I've convinced you those decisions aren't completely bonkers, but let's continue to do arithmetic, OK? So the size of a structure, very straightforward. So let's do a list and this is exactly what you would think, right? If it's an empty list, the size is 0 and if it's not an empty list, then you just do successor to the rest of the list. No controversial.
    What about position? So the position are function or index function is, you take some structure, and I give you a value, and I want to know at what position in the structure is that value, if it's there at all?
    Right?
    So what type should this return?
    Well, I won't make anyone answer. Let's unpack it a little. Let's do some of T so let's define position in terms of a helper function, an auxiliary function called POS, whatever that means and what we're going to say is we're going to start off at 0, right and what we're going to do is say, OK, if the element we're looking at right now is equal to the element we're searching for, then whatever number of our iteration we're at, that's our position, right? Makes sense. And otherwise, if it's not equal, we recurs, and then we increment our counter, right? But what if we reach the end of the list, right?
    So in implementations I know of, the result here is -1 and the type returned is integer, but we're saying we're not going to do that, right, R. We can't. So what are our options? Anyone want to shout out an option?
     AUDIENCE:  [inaudible]
    >> Yeah, yeah, the person who said option was listening more carefully. So you can say that it returns an option of a natural number, right and then we change the code so that if we do reach the element we're looking for, the result is some n, where n is our current count and if we don't, the result is none, because 0 is not nothing. Right?
    But there's another one, and actually, Colin in his paper didn't suggest this one, right? So we like this, because we have to be explicit about the possibility of failure. No one consuming this API can ignore it and say, oh, well, I got back a number and let me add it or whatever, they can't do that. They have to reckon with the fact that this has failed. But here's a different approach. And the reason Colin prefers this approach is because of laziness, right? So here's positions, instead of returning an option of a Nat, you just return a list of all the places where that element occurred, right? And of course if it didn't occur in the list, you just get the empty list back, so you still have to reckon with that failure, and if you're in a lazy language, you only compute as much of this as you need, so if you only ever need the first occurrence, you don't even end up recursing the whole way through the list. Right? Ooh!, very nice, OK.
    So what this shows is we have to mind the gap, right? So when it returned integer, we as a consumer of this API had to think about, what if it went wrong, I need to check it if it's less than -1, but we removed that gap. There's no values in the return type that aren't valid results of the computation we care about. Right? So there was no gap, right? When you're using integers, but only the positive values are valid results of the actual computation, you have this big gap, like literally half of your possible results. But here we made it so that we couldn't do anything, because 0 isn't nothing, right, and every other number is a valid result, so we had to think of something else. The choice of the number system forced us to be explicit about the possibility of failure.
    OK, what about sublist? So sublist you have some big list and you're like, oh, I want the list that represents the elements of 5-10, for example, anyone see a problem with this? OK, that's fine. So sublist has some invariants, right and I didn't give it a type on purpose, because in some languages, this function assumes that n is greater than m-1, and the reason is because if it's not, you would have passed take a negative argument. And you say, that's ludicrous, why would it accept negative values and I would say, ask the Haskell designers.
    So in our sib list, the list is very straightforward. If it's 0, we only have to care about two possible things. And if it's 0, then it's just take. Right, because we want everything from the beginning of the list and otherwise if there's a successor, we drop that number of elements from the list and then we take the remaining with our saturating subtraction and this gives us the behavior we want. If we ask for the range 5 to 10, but the list only has 8 elements, we get the list 5 to 8.
    Ooh, Comcast is very eager, they are a sponsor, so --
    [laughter]
    So what about implementation concerns? So a lot of people get pretty riled up when I describe the natural numbers in this way, because they say something like, you know, why don't you see Nats everywhere? And I say oh, language designers don't care about them because they're meanies, and then the language designers say, that's not fair, we actually just have performance concerns and I'm like, what do you mean? Even languages that do have them, I'll put that in scare quotes, end up tripping over themselves. So here's a true story in three pictures. So anyone know about size and then sine size because that's the world we live in. Oh, my goodness, so here's a true story in three photos.
    OK.
    [laughter]
    Then we have this, oh, by the way, I saved this screenshot as love.png, and this person is saying, actually, ints are the problem because most of the values, negative numbers have no feeling and I felt a warm feeling of deep sympathy and love. I have no idea who they are, but they're my favorite person and the person who originally filed the ticket responded with this, and said yeah, that's all nice and warm and fuzzy, but there's places all over the codebase where we're mixing signed and unsigned ints, and when I try to pay attention to warnings, we mixed results. There may be people in the audience who use this software and I'm not pointing names, because that person is not wrong.
    They're not wrong for wanting to avoid t right so is all hope lost? Well, in most languages, part of the problem is that unsigned values can be coerced away, right? So you don't have to reckon with 0, right, and we want a reckoning, folks, that's what we want!
    
    [laughter]
    So some languages do it right, in particular, Idris and Agda both compile with these Nats that I mentioned. A machine word is pretty big and it can take a long time to reach its max, there still remain issues with implementing lazy Nats, right? And in particular the issue is really bad memory usage. This has plagued lazy functional programming for decades, so what are the alternatives? We can use a machine number like they do in strict languages. We can use unevaluated computation all the time, right? We call they thunks, or and this is what Colin recommends in his paper, he says, the implementer can do fancy things behind the scenes, so that you have this nice lazy semantics, but whenever you actually use t you collapse it down to a machine integer, and what you carry around is a pair of machine integer and unevaluated lazy Nat, right, so whenever you learn some more about how big your value is, you collapse it down so you're not using all that memory, you have that value, and the full value is the sum, right? So you can still have infinity, but you might know it's definitely 5, so you store 5 as a machine word.
    Oh, that's nice. Oh, I kind of gave it all away. I got ahead of myself. So why don't we use machine words? I think it's suitable for eager languages, because you won't be able to have infinite structures anyway, but we lose infinity in lazy languages if we represent it as a machine word. Why not thunks? Well, this is why, because they use a lot of space, right? What wasn't addressed in paper, what was left as future work, is you can use all sorts of nice static analyses to help you determine when you don't need it to be a lazy number. It's possible that your program never uses the fact that it's lazy and in fact part of my Ph.D. was this analysis called strictness analysis, which in a lazy language can tell you these values have to be defined, they're not going to be lazy and so if you know that all your uses of numbers are strict, why not represent them as a machine word?
    So and then all those dirty implementation techniques are hidden from the user, right? As far as the user is concerned, they're zed and they're succ and that's all they have to worry about. So what about beyond Nat? I'm aware that most of you aren't going to go home and write Nat.js and make sure everyone uses Nat, and I get t so what's really the point, right? So what about sets, right? So here's a thing we see all the time. We have some API we're using and it returns a list. What's that saying?
    What's a list say? Does the order matter, though? I don't know. And there could also be duplicates in a list. Does that matter? It may matter. It may not matter, but the type hasn't told you anything, right?
    And so again, we recall that mind the gap, right? So if the same function returned a set, it would be signaling something, right? It would say there's no order here and it would say you don't have to worry about duplicates, right? And so every data structure we choose in our API is signaling something and what we want as API designers is that that something is as close as possible to our intent. Right?
    So asking the consumer of your API to ignore a signal from the data structure they're consuming is adding to their cognitive burden, right, you've given them a list, but then you're saying, but don't worry about the order. But it's ordered and you're asking them to just trust that they don't have to worry about that, right? So we want structures that are necessary and sufficient, no more.
    Right?
    That way, all the signals they get from the type we're giving them are things they can worry about and no more.
    So some closing thoughts: In case you haven't picked up on it, I really like the natural numbers. I encourage you to think about the natural numbers. The next time you're designing an API  -- I do this all the time where I'm writing something and I use int because it's just what's available and Nats aren't in my standard library and but I go, ooh, is that what I mean? And I have admittedly different problems most people but that's what. When I talk about this, no one seems to disagree with me. Very few people say you're wrong, and yet, we don't see Nat, Haskell, which ostensibly cares about this sort of thing, it's not in the prelude, it's not in the types of things you get when you load up the Haskell repo. It's not there. So there's this dissonance between good job, Colin, well done, and then no one uses them. Hm hm hm, that's right, something to think about. I really liked this quote because it was so optimistic and I care for Colin very deeply, but in 1989, you know, he's claiming in his paper, he goes, oh, yeah, we all know laziness is clearly superior. You know, some people may not think so, but we'll show them. And I love that optimism. There was this era of lazy programming and there were like 25 people in the world and they're like, this is the way to do programming and they felt really radical and they're like, we're going to do everything and we've got infinite structures and people now are saying, Haskell's great and whatnot, but what if it was strict. So there's one lazy language for the people who want that and people are like, let's take away the one place they can be lazy. We're like, come on, leave us one place we can be lazy and remember, Taylor Swift was born that same year. So smash that subscribe button, you can find me on Twitter, José Calderon, and remember, the natural numbers, thank you!
    [applause]
    
    We could take a couple questions.
    
    >> Was I fast?
    >> Actually perfectly timed.
    >>
     AUDIENCE:  Yeah, you said you worked on Haskell?
    >> Yeah, the committee has been very lazy
    [laughter]
    >> So I'm technically on the Haskell 2020 committee, we have not made a lot of progress. Yes. I could try. I could try to fix it.
     AUDIENCE:  What would the obstacle be?
    >> So the way the committee is organized, every committee member has a veto, so you have to have consensus. Now, that seems ludicrous, right, but that was -- OK, like history, lesson-time, the reason Haskell came into existence because in the '80s all these researchers who were interested in lazy languages, had their own lazy language and what they found was that there was a lot of churn about when you had a paper that showed this technique, first you had to make sure they were get up to speed with that technique and there was a lot of wasted energy on that, so they were like, what if we could get together and agree on the parts that are functional and that will be Haskell and we don't have to keep doing this. And that worked. I shouldn't have laughed. Sorry, that was inappropriate. They agreed to do that in 1987. The first Haskell like published report was 1998, so the noncontroversial bits took quite a while to sort out, and this is the same. So like, you know, Haskell uses all these language extensions these days and there's some of us on the committee like me that are like oh, simple!  And there's people on the committee who are like ooh, advanced types and we clash, B. sorry, that was a long answer, but too bad.
    >>
     AUDIENCE:  [inaudible]
    >> Can you repeat the question
    >> Yeah, so the question was about how a lot of these issues with underflow and overflow stem from the architecture that we're kind of running things on. Which is true. And I believe the question is kind of like, do I think that addressing it at that level would solve these problems? Yeah, but I don't see that happening. But I think it would. I also think like the way the paper started, right, like oh, we've been doing machine-independent languages and it turns out we still don't have machine-independent languages, because some of these things trickle up, and some of these want to think about the semantics that we've given this language and not worry about the machine we're running it, and we still haven't reached that panacea yet. Maybe one day I'm optimistic. Who knows.
     ZEESHAN: Optimistic is the way to end on that one. Thank you very much.
    [applause]
    So we've got about a 20-minute break and we'll have our talks here we'll say around 3:25ish.
    
    [break]
