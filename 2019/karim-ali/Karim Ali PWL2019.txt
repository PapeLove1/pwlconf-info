    Is Program Analysis the Silver Bullet Against Software Bugs?
    - Karim Ali
    
    ZEESHAN:  OK, everyone take your seats. I know the lines were long and it was hot, but how was lunch overall? Was the food good?
     AUDIENCE:  Yeah!
    [applause]
    
    >> Yeah cool, it always for the trucks, right when a bunch of people come out, they're like, oh, damn, we didn't know it was going to be like this!
    So trying to think any other logistical things. So far, how's everybody like the conference so far?
    [applause]
    
    The first two speakers any good and cheer on to say, the homework will be due in a week. So you know, if you want to send him emails directly, that would be really fun. (Shriram)
    OK, so our next speaker, Karim Ali, so I kind of like my background has been in programming language for a bit and recently as I've been at Carnegie Mellon, I've been really intrigued about program analysis. Ooh we had a speaker two years ago talking a little bit about this in terms of finding bugs, and I think now, four years later, Karim is looking at it from a different approach, looking at machine learning and things that he's looking on and for me I really wanted to see a talk program analysis. I want to see what Karim says, without further ado, Karim ally, University of Alberta.
    All right. Thanks to Zeeshan for the introduction. My name is Karim, and today's topic or today's talk I'd like to talk to you whether program analysis is the silver bullet. How many of you here has developed any kind of software, just by humming.
    Audience. Hum mmmm
    How many of you have developed bug-free software? All right, great, so we all know that software cannot be bug-free, and these bugs can range from simple programming errors like the infamous error Apple had in its codebase or it could be more fatal in the terms of some race conditions in terms of the antilock braking system in Toyota cars that unfortunately led to some fatal accidents but it also led to Toyota losing about $3 billion from recalls that had that software. Or it could be life-threatening bugs, like unencrypted, unauthenticated connections to some medical implants. So just 6 months ago from a Department of Homeland Security that basically says that some of those medical implants would allow an attacker to connect to them. Imagine and today I would like to make the bold claim that program analysis can actually detect these software bugs and even fix them in some cases and hopefully by the end of the talk today I will convince you that that is the case. So what is program analysis and how can it help us with these software bugs. So generally speaking, program analysis is a way of reasoning about the runtime behavior of a program without necessarily having to run it. So we're trying to know what the program would do when it executes, without having to execute it. So in general as Shriram said in today's talk in the morning, we can reduce that problem and Rice's theorem tells us that for any interesting property Pr of a behavior of a program, it's almost impossible to write an analysis that can tell us for sure whether that property holds or not.
    And interesting here, means the word interesting property here means that it's a property about the semantics of the code that doesn't necessarily hold to be true and it's nearly always false. So it's not something syntactic about for instance does it have an if statement, does it have a for room but it's something semantic about the program.
    So are we really doomed then? So by definition it's undecidable, so do we just give up and not do anything about it? As stubborn as we are, we don't do that and not quite, as you can see here, so we don't -- we're not going to set out for the undecide ability of program analysis. We're happy to get an approximation of that property without executing the program.
    And the whole research area of program analysis is all about trying to get that approximation as close as possible or as good as possible. So what do I mean by this? So we can have a program, you run an analysis on that program and then the analysis can try to validate whether that property exists in that program or not. So it will tell you yes, the property exists or it can tell you maybe. I don't know. So a safe approximation for that would be maybe. On the other hand, you can run the analysis to invalidate a certain property in the program in which case you're expecting the answer to be no, or the analysis can always return the answer maybe. And the goal of our community is to make those mostly either yes or no, depending on the property you're trying to verify in the program.
    So program analysis is actually quite useful and it's used in many contexts. So it's used in constant propagation, debt code elimination, static method inlining, developer support tools, code navigation if you want to use users of your methods, references of a method. Code refactoring tools or code recommender system. If anybody has used a native environment, if you hit ctrl-space, that gives you a program analysis to give you context around that piece of code that you're writing to tell you how or a list of methods that could be called. It's also used in dead code elimination. If you want to make sure that a space shuttle when it goes up in space, it doesn't blow you up so you don't have the luxury of testing this with multiple space shuttles to be able to understand that yeah, my software actually runs, or if you have an MRI machine, you don't want to try this ideally on human beings before you deploy it to patients that would use it, you want to make sure the software is correct before patients can use it. And it's also used a lot in the security domain and finding bugs, like find bugs, for example, fortify, all of these are analysis tools that try to detect security vulnerabilities in your code.
    However, throughout theer*, I realized that developers typically tried to avoid using program analysis tools in practice and the reason why I think that is the case is three fold. The first part is scalability. The second reason is precision, you get too many false positives which is annoying to developers and disrupt their workflow so they give up on using program analysis tools. And the third is usability. Maybe they have to constantly switch back and forth between tools to be able to get the results of the analysis and today I would like to talk to you about each one of these challenges and some of the contributions that my group has been doing in the last feweer to address each one of them, and I will focus more on scalability and precision, but * I will quickly touch on usability towards the end of my talk, as well.
    So before I do that I'd like to acknowledge all the collaborators with whom I have collaborated on the contributions that I will talk about today, and some these are some of my students, some of my mentors and some of my industry partners that I have worked with, and specifically Ondrej Lotuk, when I was first starting my Ph.D. at the University of Waterloo and you can tell that was ten years ago because I had a lot more hair then than now.
    The first question I had was, where do I begin? What do I do now in my Ph.D.? I have no idea what I want to do. I know I want to do something in programming languages, but I don't know what to do exactly. So his answer to that was, why don't you start reading this paper and that was that paper here. So if I focus here on the title of that paper, the title says automatic construction of accurate application call graph with library call abstraction for Java. I had no idea what that title meant. There were just too many buzzwords in the title that I didn't know what they mean. What is a call graph? What is that? Is that a special type of graph? And obviously there has to be something to do with the application, so that means is there must be something to do with the library, as well, and what does that mean? And of course it has to be accurate. Nobody wants an accurate call graph, right? What is call abstraction? What are we abstracting and how are we abstracting that call?
    Again, library shows up here. So there is an application part, there is a library part and I didn't know how they interact with each other. And of course you want to do this automatically, nobody wants to do this manually by hand and that has Java in the title, so that means this is different doing this for Java compared with other languages, so I didn't understand why are the reasons behind that. So I wanted to demystify some of these buzzwords and these terms that I didn't understand back then and one thing you do is you start reading one paper and you branch out and read more papers to try to understand even a single line or a single statement in that paper, and then you come back to that paper later on to continue where you left off, maybe even a few months later after you have compiled enough knowledge to be able to understand that paper and that's exactly what I did. The first thing I wanted to understand is what is a call graph.
    So after reading a few papers specifically, two of them that I will show here, one is the paper by Frank Tip and ... That explains different types of call graph analyses with different levels of precision. And the other paper which was also intriguing which was published a year before I started my Ph.D. that shows basically an analysis framework that uses datalog as a language to define the analysis, which was really nice because you can just focus on the logic of what your analysis is doing without having to figure out how the analysis is defining the work.
    So reading these two papers and more, I was able to understand that, well, a call graph is just a fancy name for a data structure that represents the calling relationships in your program.
    So if you have a method invocation, you want to know what are the potential calls that could happen at that method invocational call time and a call graph could tell you that. And it turns out that this is a very important data structure, because that data structure is required for every single interprocedural analysis that you may think of. An interprocedural analysis is an analysis that crosses the boundaries of your methods and once you cross the boundaries of the method, a call graph would basically will tell you, it's basically a map.
    So let's build a call graph. This is a call in Java. I have a main method -- sorry, you have a main class, which is the entry point of the program. A bunch of statements, a small abstract class and two classes that inherit from T so according to Java, the first thing that is the main method of your program is the public static void method, right? Following that node, we have an if statement, a creation of an object called circle. So we're called circle init, and when you construct an object. So in this case we're calling shape.init or object.in.it.
    Then we want to reason about all possible execution classes of the program and so far we only looked at the if statement, not the else. So now looking at the else statement we have a construction of a different object which is called square and then we're going to call shape. init, and so you see we're trying to abstract from the actual runtime behavior by including all the possible calls that can happen at run time. So we don't know the concrete shape it could take.
    And then the last method that we have, the last statement is S.0, we think or the analysis would think that there could be dispatched that method, S.draw to either circle the draw or square the draw.
    So now going back to the paper that I started everything with, now I know what a call graph is. Good.
    I know how to create that for my application, I just used the example to figure out what that means. I know how to do that to Java now, I read a whole bunch of papers that talk about call graph analysis for Java. I also know what the abstraction is, because like I say you're trying to abstract away from the behavior of the program by figuring out what other execution paths it can take.
    And I was able to do this automatically because I created or I wrote an analysis in that datalog base analysis framework dup, but the two things I didn't understand was how to do this accurately and what does the library have to do with it?
    So I thought OK, let's build a call graph for the biggest program in Java that I could think of at the time and it's java C, the Java compiler.
    So I implemented and used an open source data engine called IRSI and the Java C bytecode there wasn't that bad. Not many class that I'm dealing with, and I had a powerful machine it 8 gigabytes of RAM at the time. So I said OK, I can do this. So I ran the analysis a number of times and the problem is each one would take hours to finish and I didn't know why that was the case. But that wasn't the worst part, but the worst part every single time the program terminates with that error, out of memory error, Java heap space, regardless of how much memory I gave it.
    So I didn't know why was the case, why was it so difficult to reason about that program. But I thought OK, maybe I took it too far, maybe the Java compiler is after all a large codebase, maybe I could try something smaller, so I went to the other extreme, smallest program I could ever write in Java, hello world program, and this is how it would look in Java, just a simple class with a simple name method that prints out the string, hello, world.
    And I ran it with the same data analysis that I had, and it also took a bit of time. It wasn't as much time as the Java compiler, but it still took a lot of time. I thought maybe something is wrong with my analysis. Maybe the analysis engine itself is not optimized as much as it should be. So I thought OK, I'll try a different framework, which is Soot, one of the most prominent analysis strings in academia, and it was started at the University of McGill in Canada by Lori Hendron and her group. And I wrote an analysis in Soot. How long do you think the analysis took to finish?
     AUDIENCE:  Hours.
    
    >> Hours anybody else?
    >>
     AUDIENCE:  Microseconds.
    >> So we have hours and microseconds. Anybody in between in days? Was that days? It took roughly a little bit more than 30 seconds. So not microseconds, not hours, definitely not days, but still, like, why do I need 30 seconds to get a call graph for one statement in my code? And then I get a call graph with more than 5,000 reachable methods and more than 23,000 edges in the call graph. So looking at this code, I'm like what's happening here. I expect to only have two edges, maybe three edges at the most, what's going on there? So I realized that the code that you see up here, or up here, it's just your code, and for the analysis to be able to analyze your program, it has to program in all the library dependencies that your code depends on and because here you're calling system.out.println, it has to run every analysis and to give you a complete analysis.
    OK, and this is actually a visualization of that code here. So this is -- I kid you not. This is how you would visualize it using graph vis, for example.
    Where is your code?
    >> Maybe the two methods down there on the right-hand side or something. But all that web of calls and edges are what's happening in the library. Things that you as a developer couldn't care less what's going on there. All you need to know is which method I'm calling in the library, and that's it. I don't care what's happening in there.
    All right, so of course if you're a developer and using it in an IDE like eclipse or whatever. You don't want to say that bug there. So that's something that really bothered me so I wanted to know what is the reason and how can I solve it? Going back to my paper that I started with, now I understand what it means to have an accurate call graph. Because that call graph is not accurate. I'm just getting too much noise and I also know how the library can interact with the application in ways that can make the analysis really imprecise. How can we resolve that?
    So the first thing I asked myself, am I alone? Am I the only one who has had this problem before? So reading more papers, going to mailing lists, specifically the mailing list for Soot, I realized that lots of practitioners have bumped into the same problem before. And then you'd see things like ignoring the library would be unsound. What does unsound mean? I didn't know at the time what unsound means. But obviously all what they were asking for is a way to do partial program analysis. So how can we get a holistic view by only analyzing parts of it rather than analyzing it all including all the library dependencies.
    But the trick here is was to do this in a sound and precise way, that's what people wanted. So to clarify that, let's assume that this is the paragraph of your program, so the white circles here are the nodes or methods in your program. I just removed all the edges just to make it more clear, so ideally your ideal call graph would be a call graph here, but again, because we can reduce every program in program analysis, from the halting problem, so what typical call graph analysis do is they perform what I call whole program call graphs. Put them in together, throw them at the analysis, get the result, to give you a safe representation of the overall run time of the program.
    What if you don't want to do that? You have twoping 0s. First thing is you can say I don't care about the library, the library cannot affect my code in any way, so you get an incomplete view of what the world looks like. But you still get some result, but that result is incomplete and if you're using this call graph in a client analysis that depends on its result, then the result of that analysis is also incomplete.
    The other extreme would be to say, well, I don't know what the library is doing still, but I'm just going to assume it can do anything. Can call a new method, can create a new object, can modify any field and so on and so forth but that's really imprecise. What we really want to do is have something in the sweet spot in between, something that can still be precisely as close to the whole program analysis but still analyzing the code in the library. So to address that problem, we defined what we called the separate compilation assumption, so what that states is the following: It states that all of the library classes in a language like Java can be compiled in the absence of application classes so that's a simple, fair assumption to make, yet it's very powerful. So it defines things like what classes can exist in the library, what classes the application can inherit from the classes from the library. What objects or classes in the library the library can share and can have access to. Access to fields, array, method calls or callbacks, which is the main reason why it may be unsound or imprecise. Today I would like to highlight two of these constraints, local variables and method calls because they are the most important ones that make the separate assumption work in actual practice.
    So the first one is the library points to set. So I'm separating the code in the application versus the code in the library. The application code stays the same, so you still have all the variables, you still have the points to set that tell you what are the heap variabilities that these point to in run time. In the library, since we are not analyzing any code we don't have any local variables, but we still need that information about the heap applications they may point to in order to get a complete call graph.
    So instead we have library points to set that you can think of it as the union of all the variables in the original library code that we are not analyzing, so that's a substitution for the original library code.
    All right. Now the next constraint that's also really important is callbacks. How do we, without knowing the actual calls in the library, how do we know which methods in the application that we can callback, like handlers and so on and so forth. So it has two positions for a callback to happen. So in this case we have a A of. The first one is the library should know about that method somehow, like the library cannot call a method that it doesn't know about, so it has to know the signature of that method and the way to be able to know that, assuming there is no reflection of course, is by the application Class A extending the library class L and overriding that class M. So that way the library can know that and we can know that from looking at the class structure of the library without having to look at the code in the library itself. But that condition alone is not enough.
    The second condition which lates to the library points to-set states that there has to be an object of compatible type that is in the library points-to set. So if you have these two conditions, that allows the library to have a callback, for example if you have an application Class B but there is no object of B or a subtype of B in the application so there would be no callback. The same thing for C.M. But since there is no relationship between C and L in the class hierarchy, then there will be no callback there and these two conditions, and these two constraints, library callbacks and the library points to-set are essential.
    All right. So now that we have this separate compilation assumption, we want to enable wider use of it across any analysis framework without having to change the analysis framework and without having to change the code in the application and 0 to do that, we implemented a system called Avorros. And optionally, it can even receive information in a plain text format that tells you what are the uses of reflection in the original library that we had. So it does not analyze any code in the application for the library, it just looks at the class hierarchy, signatures of methods, signatures of feels and the entries in the constant pool, so like strings and integers and values like that to simulate or to reason about what maybe reflection could do if it concat nates a number of strings.
     [cheering]
     And the output of Avorros, creates a jar file. So you can use this jar file along with the application code in any analysis framework that analyzes Java programs and you can automatically have support for partial program analysis.
    You just need to use that jar file for any jar file in the library. So how does it fare in practice? So in the experiments that we have done, we found that they were able to reduce the size of the library by 600 times.
    Just using Avorros and that huge reduction in the size of the code that the analysis has to handle leads to the analysis -- call graph analysis to be 7 times faster than before and using 6 times less memory than before. So you can see that the reduction of the code doesn't just affect the time, but also how much memory it uses. And the most important part about this is that from our evaluation, we were able to show that the analysis produces precise and sound results, so we actually have a formal proof that says that the results are sound, and we also show, compared to the whole program analysis, that we still have an analysis that could give a reasonable decision compared to what you have in the library.
    So I'm happy to say that Avorros does work in practice. It's the analysis framework for WALA, for example, the analysis framework for IBM. And you can use those frameworks and more if you want to have support for partial analysis, without changing anything in your program analysis framework.
    So we were able to scale analyses to some codebases that we were not able to analyze before and improve the performance of call graph analysis at run time, reduce the memory requirements and hopefully scale it to even larger codebases.
    All right. So in the second part of my talk, I would like to focus more on the precision of program analysis and how that sometimes also has an effect on scalability. So usually you think that I want to make the analysis more precise so that the has to do less work and it will be more scalable, but that's sometimes not always true. I'm going to show you that in a couple of minutes here.
    So for that part of the talk I'm going to focus on security-related static analyses, such as null-pointer analysis trying to reason which parts of your code may point to null. Or taint analysis. And figure out whether there's a data leak or not, or figure out whether there's some unsanitized value in your code or not.
    Or type state analysis. It's very helpful in figuring out things like resource leaks, whether you have too many open files and without closing them at the end.
    All of these are basically static data flow analyses. But we need them to be precise, that's the key thing. So what does it mean to have a precise analysis or precise static data flow analysis? Let's assume that piece of code here, to be precise, we need to have two properties of the analysis. The first property is the analysis has to be context-sensitive.
    Meaning that I or the analysis has to differentiate between calls to the same method from different locations in the program. Each one of them would have its own context, based on the variables and the arguments that you're passing through that code. The second thing that it needs to be precise is field sensitivity. The analysis has to be able to differentiate between accesses to different fields on the same object and not feed them all the same.
    So the static data flow analysis is precise enough. So one way to obtain both context sensitivity and field sensitivity in the same analysis is to use a pushdown automaton. One represents the stack of calls, every method call you make is a push on the stack. Every return on a method call would be represented by a pop from that stack. And have another separate stack to access to the fields. And by maintaining both stacks of calls and stack of fields, your pushdown automaton can have concept sensitivity and field sensitivity.
    However, that program again is an undecidable problem. There's a very nice paper from the year 2000 that explains why this is undecidable. They think we're going to give up? No!  We need to find something else to do that.
    So what do people usually do traditionally in the program analysis community to work around that undecidability? They're just going to say the following: We're going to keep the stack of calls as is, but we're going to get rid of the stack of fields. Because that makes the analysis undecidable. And instead we're going to set it for an approximation. And that approximation uses another thing called K-limiting access paths or access graphs. The problem with that representation is that it requires what we call a K-limit. So that K-limit is how far you can represent a string, a field axis in the object. How far you can go deep in an object. And there are two problems with that kind of limitation or that kind of representation. The first problem is, how big should k be? And the answer is, well, I don't know, I'm just going to run the analysis multiple times and figure out a reasonable value for k that fits the needs that I have in the analysis that I want. So you can decide this a priori for running the analysis.
    The second problem is whatever value you choose, you definitely will have some false positives in your analysis results and these false positives may be too many if you choose a low value of k, a small value of k, they may be too few but at that time the analysis won't necessarily scale to the codebase that you're interested in. So to overcome these two problems, we developed a new abstraction, called synchronized push-down system automaton. So for sport, S PDS.
    It's both context sensitive and field sensitive. One of them is context sensitive and the other one is field sensitive. And somehow combine the results that we get from both analyses to get the benefit of both worlds and then in our POPAR paper earlier this year we showed that it was an over approximation, but we also show that the overapproximation we were able to push it to exactly one corner case in and the theory behind the analysis that theoretically does exist, but in practice in all the code that we have analyzed, we have never encountered that corner case, so practically speaking that corner case does not exist for us. We can get around without handling it.
    So OK, so how do we do that? How do we have two analyses and combine the results between both of them? So context sensitive analysis we know we're going to have a stack of calls nothing has changed there. The difference now would be in the pushdown system of fields. Because now we have a separate pushdown automaton, and the way we're going to combine the results of both analyses is we're going to do an intersection of both pushdown automaton.
    Or the path in the program that the analysis is trying to compute a value for is accepted in both push-down automaton, then we're going to say that it is an exception in our analysis so it could happen in run time and then we're going to compute the analysis for that path. So now we have an analysis that is both decidable. And the most important point is, there is no K-limiting anymore. I don't have to worry about what's the value of k or have imprecise values from the analysis, because I chose a value of K that is too low or too small.
    So how does SPDS evaluate?
    So to do the evaluation of SPDS, so here I'm showing a graph on the X axis, I'm showing the number of field accesses, on the X axis, I'm showing the runtime of the analysis, how many seconds the analysis takes to finish, and this is done on a contrived benchmark, so not a real-world application, but just to show you the various limits of K. So if you have a value of K equals 4. What if you lower the value of k so now we're getting less precise results, but hopefully you're going to get more scalable analysis it's not much better, but still a little bit more scalable. So that's OK. What if we even lower the precision more? So K = 2. So we get a bit more scalability, but again we're losing almost half the precision. What if we said K = 1? That's perfect. We get a very scalable analysis that's much better than K equals 2 or 3 or 4, but we lose almost all sensitivity. If you have an object where you're accessing a field of the field of the object, like A to B to C, your analysis will overapproximate that to A to B to start. In a regular expression basically using a regular expression syntax here.Oo so where does SPDS fall in this graph? Where do you think it falls in this diagram here? Do you think it's better than K = 1? Yes, thank you. Do you think it's worse than K = 4?
    No. Thank you. It's actually right before K = 1. So you have scalability of the analysis that is very close to the most scalable setting in the original abstraction, which is K = 1, but the thing to remember here, we don't have any K-limit in SPDS, which basically means you have a precision of K = infinity. So you get performance or scalability of the analysis that is very close to the most scalable abstraction.
    So let me think, and actually one thing I forgot to mention here. You may think that some of these cases may not be that big anyways, but we found a case in the Eclipse IDE code Base where the field access is actually needs a K value of 13. So if your K is less than 13, you're going to miss that data flow in the program and if that data flow happens to be a security vulnerability, then tough luck, you have a security vulnerability that you missed in your application, just because you don't have a value of k that is reasonable enough.
    So we all think that, well, you used a contrived experiment here, some toy examples, maybe some benchmarks, so is it really useful in practice or not?
    So we actually used this analysis the SPDS synchronized pushdown system apps the analysis engine of a framework on a system that we have called CogniCrypt, which will tell you whether you're misusing a crypto and as you can see now in the example in the short video which may not be playing for some reason. It is in it's not showing on my machine, so you can see you're writing a code, you can choose a cypher, so the analysis will tell you in the IDE that there's a problem there so you as a developer will go ahead, change the analysis of the cipher, and then it is scalable enough to run in the IDE without disrupting the workflow of the developer, so CogniCrypt is released as an open source project under the umbrella of Eclipse and we welcome contributions from the community on that system there. The other thing is we ran an analysis on all the Maven artifacts. And we found that 68% of them are insecure. Not only that, we ran it on 10,000 reason Android applications from open source Android applications, and recent that just to say that they have had the most recent security patches to them, so we're not really looking at legacy applications or applications that have been abandoned by their developers and we found that 95% are insecure, again, using our CogniCrypt system there. We were also able to find a severe security vulnerability. any can talk about it in public because it has been secured. So in the work that we have done with CogniCrypt and SPDS, that's some of the work that we have done to improve the precision of program analysis by making it more precise and less fat in the analysis itself.
    In this part of my talk I will quickly touch on usability and how it will develop developers in their normal workflow, because a typical static analysis tool, if you have used one before, you will typically get a report of such, right? , usually in some environment that is different than your development environment, so by the time you already see the analysis reports, you already got a feature that is related to that. And if you notice that report closely, you'll see that the critical issues that you need to fix is more than 4,000 issues, so you as a developer would need to fix all those issues before you can ship or deploy your application, so of course all of you are thrilled to see that kind of report. Of course not you're not.
    So what do we do? How do we make developers happier about static analysis tools? The first thing developers want and need is an analysis tool that is very precise and that is responsive so they can use it in their development environment without having to switch between different applications and also seamless integration into the IDE and something that is customized to their behavior and how they are using the analysis and the applications that they are developing.
    There are many studies that have been done in the programmings language community that talk about what are the features that developers would like to have in their tools to be able to use it in their static analysis tools to be able to use it and what are the kind of things that developers don't like to have, and make then not -- or abandon
    Just in time static analysis, pun intended, by the way in the title.
    This is a snapshot of how it looks like in the IDE. So we basically use a layered approach that basically unpeels parts of the code starting from the point where the developer is -- basically the current edit point and gives you results as it finds them along the way. So it doesn't wait for the whole analysis is done to give you the report but it reports those results as it goes and the nice thing about it is you don't have to wait for hours or minutes to start working on those bug fixes, but you can start working on them right away while the analysis is computing the rest of the analysis in the background. And in our user studies, we were actually able to show that developers are twice as fast using our cheetah, and that was really the thing that we wanted to see out of our study, is just by changing something fundamental in how the analysis works, we can make developers use it hopefully more and make the potential of program analysis more than it already is. So this is just briefly some of the things that we have worked on with respect to usability of static analysis tools. So so far I've talked about scalability, precision, and usability of static analysis tools. So where do we go from here? What does the future hold for program analysis, so some of the work that my group has been up to is the first one is building an analysis framework for SWIFT that is based on top of WALA and the nice thing about it is it has support for many different languages, and you want to reason about a program that no longer uses one compiler, so SWAN or the swift analysis framework would allow us to do something like that.
    Or using it at runtime in the JIT compiler, or the just in time compiler in a VM and use that to estimate post-inlining transformations. Using app sync application, something that I didn't think I would do a few years ago.
    Or even using terminal sis in languages that are really powerful such as Rosette that allow you to reason about symbols in your program, to fix headers that you may find in neural networks. Without having to restrain that neural network again which is some work that I've been doing with really brilliant undergrads in our department in the last year or so.
    And there's an uptick in static analysis from different companies, such as Google, for example, which is now using static application improvement just to reduce the amount of security vulnerabilities in those application, or the various analysis frameworks or tools that Facebook deploys and scale to hundreds of millions of line of code and even automatically repair those lines of code. Like Facebook SapFix, for example.
    You can also have something running in your repository like Semmle, so I think with all that work and all that has been happening, and also the challenges that have been seen, I think that the future of program analysis is really vibrant and there are a lot of interesting research challenges ahead of us that we're going to be tackling in the next few years.
    So let me quickly wrap up what I've been talking about today. So first I showed you a few examples of bugs and errors that I think program analysis can help detect and also fix, and I showed some of the context that I think program analysis is useful in, like compiler optimization, developer support tools, program authorization and security. The three main challenges that I think do not allow developers to make use of the full potential of program analysis and specifically static analysis in practice, and some of the contributions that we were able to make to improve the status quo of those three main challenges and also briefly talked about some of the future work that I see in program analysis that I think will shape the next few years in the program analysis research.
    And with that, I'd like to thank you all and I'm happy to take all of your questions.
    
    [applause]
    
    
    ZEESHAN:  We'll have one question.
    >>
     AUDIENCE:  I have a really quick question, do you know if it's possible to do this type of static analysis against WebAssembly if you use WebAssembly to compile various languages?
    >> Yes, you can do program analysis in any framework that you can think of and there are actually products that have been released in the last couple of years ago.
    ZEESHAN:  Maybe one more since that was a quick answer. Did I see one hand?
     AUDIENCE:  Am I correct in assuming the stack analysis you're doing is against source code, could you do static analysis against compiled binaries?
    >> Yes, usually what the analysis does is it converts whatever input you give it so that input could be your source code, it could be bytecode of some sort. It could be assemblies, it could be whatever you put it in. As long it can convert into some representation.
    >> Cool, also I want to say Karim is here all through Strange Loop so you can ask him a lot more. So thank you, Karim, thank you very much.
    [applause]
    
    >> So we'll start the next talk in about five minutes.
    
