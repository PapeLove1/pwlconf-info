    ZEESHAN:  All right, how's everybody doing? Last talk coming up.
    [cheers and applause]
    
    Star's gonna kill it. We have a little FYI, we have a little closing thing right after we do a couple of questions just for five minutes and talk about the after-party tonight for those going. OK, so having Star here is really cool because the way it came through, so José who talked earlier about natural numbers or integers, I can't remember, we were looking for another speaker, every Papers We Love, we have somebody who comes from a different background or technology or, José gave me some names of people and I was like, oh, yeah, I used to do a lot of physical computing stuff and you go down her website and the list is so many awesome things about teaching circuitry and things about building things and flying planes, as well, also a TacoCopter, which we could have used today for lunch. And she's also cofounded project Alloy, which have done a lot of work here --
    [applause]
    
    [cheering]
    
    And once we got started, when I was putting the schedule together I'm like, she has to give the last talk, I know it's going to be awesome, no expectations that way, but I know it's going to be awesome, and we're just so happy to have you, thank you, Star. Building personal machines.
    [applause]
    
    >> I'm happy to be here (microphone
    How about now? I'm glad that we were able to come together and solve that challenge, just to start things off. That's really exciting.
    Thank you for having me at Papers We Love. This is my first Papers We Love. And there's a paper I want to talk about that once I read, I never really stopped thinking about. It really stuck with me, and it's a robotics paper. How many people -- you -- no need to out yourselves if you did read it. It's a very short paper, so you can catch it later if you didn't. But in order to get there, you have to start at the beginning. With a question that I started with. Which is when you're building a robot, when does the pile of electronics and you know, microcontrollers, motors, you know, what have you, when does it become a robot or really when does it start to have a personality?
    So this is the robot I started with. It's a BOE, BOE bot which I think is an awful pun about the board being the Board of Education,
    [laughter]
    Yeah, I'm with you on that. It was need, though, for a high school kid to have a robot you could program in Basic, embedded basic, you don't see that too often. And if you look carefully at this picture, you can see that it's also got a bread board on top. And things like on this particular version of it, it's got feelers for sensing, so touch sensors, so it can drive forward, crash into things, reverse or if you mess it up, continue driving forward, you know how it goes and it's got light sensors, and it came along with a bunch of example projects where it would encourage you to type in or modify your own version of a program, where based on the input and the light sensors, the robot would go and do different things, and the behavior quickly came across as being personable, and it turns out that this is, you know, a core idea in robotics. It's the work of a researcher named Vladimir brightenburg, who drew these diagrams about personal robots that have their own personality so you can see if you have a light detector and you connect light sensor to proportional output of the left motor, drive, whatever, that you'd end up with a robot that will tend to drive away from a light source, and when you've typed all the code in and wired tup and you set this thing out and you have a flashlight, what it seems you have on your hands is a scared row bottom it's scuttling for the corner trying to find darkness to hide in. Conversely if you cross those outputs left and right, you end up with a robot and you I promise you can't help yourself you're like it's a friendly robot.
    In the upper right is a diagram of you can get more complex behavior, too. Robot that does figure 8s, you're like it's uncertain or one that orbits. So brightenburg blew that up into various emotions or personality you can get from programming a device like this. The bottom right image is a color-changing LED placed on a Roomba. And taken as a time lapse, so even a robot that's simply vacuuming ends up, you can see over time the way it explores.
    And so that was pretty neat and gave me a little taste but the thing that pushed me over the edge and made me obsessed with personality in robots, I'd like to show you a brief video of what I consider to be one of the most complexly personable robots in the most simple form factor. This robot is quite similar to the Boebot it's called the little yellow drummer robot and I should give you more context. This robot was designed as someone's hobby project to drive around on your counter top or with you, it has an ultrasonic sensor that it uses to point at various surfaces and it sort of samples the local area until it finds a surface that seems interesting to drum upon. And when it finds a surface that is interesting to drum upon, not only does it do the drumming, it also records that sound and jams against its own recording that it replays. So it gives me great pleasure to introduce you to my favorite robot with personality, the little yellow drum machine. I apologize, it's a little percussive. This might be loud. We did a sound check, so to begin.
    (Percussion sounds)
    (Beeps)
    (Ratchet wheel sounds)
    
    So there you have the little yellow drum machine. A round of applause.
    [applause]
    
    It goes on, there's a lot more you can find it online. One of the tricky parts I've learned about this happy little robot is apparently it was actually quite hard to film, because it was constantly trying to play on the camera.
    [laughter]
    I love that robot.
    So this is Cynthia. She's a pioneer of personal robotics. Her lab is all about building personal robots, and having built my various projects, and seeing that thing, I decided to go with. That's Cynthia with Leo, it's a very advanced robot. They had like a motion capture system that let Leo see the world and interact with people and they built software that let Leo understand if you like had an object and then you put a hat over it and left the room and someone else came in, Leo would like interact with you as though it realized that you didn't know that the object was under the hat. Which is like a cognitive development phase. It's a whole thing.
    But it was a pretty cool robot. I worked on Autumn. This is a robot that's designed to sit on your counter. It doesn't drum or play anything, it was also kind of neat. It has a head that can look at you, and this robot was built in 2007, so for the purposes of the rest of this talk, I need you to go back in your mind to a time where there weren't voice agents, there weren't Siri, Alexa, there weren't iPhones and devices like these were just starting to explore conversational interfaces but there wasn't enough power or compute onboard for this robot to be able to hear your voice. So what they did here instead, Corey Kidd is the grad student shown behind Autumn was, it would speak and you would touch your answer on the touchscreen which despite how it sounds was a remarkable interface given what was possible and all that. What's unique is you see there's a UI on the robot with three options and this robot is a companion that lives in your house, sort of a fitness health tracker kind of thing and you were intended to speak to it every about how much you'd eaten and how much you exercised. And every single prompt had three answers but the trick was these particular prompt answers are like, hi, good to see you and OK. And on this pane and most panes, any answer you chose, just takes you to the next screen. So I do that it turns out I watched people interact with this robot in demonstrations in times we took it out in public and if someone was upbeat and chipper, they wouldn't see options A and B, they would say, good to see you. Or someone could tell were a little curt and subdued, they'd say C, C, other answer and I thought it was super-neat, like this way of sort of passively collecting some information in the system about what sort of mood the person it was interacting with was in.
    Which is why, when we get to finally the paper, I was blown away. So the paper, of course, Matching in-car voice with driver state: Impact on attitude and driving performance.
    Oh, good.
    [laughter]
    I hear some chuckles. So the paper I read was written by a researcher, Leyla Takayama, does a lot of cool robotics work and I found this paper she'd written in again, 2005, so we're still in this mind set of we don't know that voice agents and iPhones are about to come out at the time this paper was written. And she asked this really interesting question. She was able to sort of foresee that we were going to be building cars with like voice interfaces and the question is, OK, you're driving, and again, none of us have ever driven with any kind of robotic voice, you're driving and there's like safety-critical information, there's something wrong, something you should know, there's traffic ahead, whatever it is. Should the voice that interacts with you to tell you this safety-critical information, should it be like really chipper and upbeat and energetic in order to get your attention? Should it be sort of flat? Should it be a little bit, do you want Marvin, right? Who knows, if it sounds really depressed, it's going to stand out. So she created an experiment, and the picture is from -- she didn't have people drive real cars, what she did is she brought people into her lab, I think about 40 people, men and women, all English speakers and I love research, she first provided an emotional manipulation to either get people be happy and upbeat or subdued by showing I think five minutes of video clips, then she surveyed them to see that people had been effectively nudged in whatever direction she hoped and she asked them to sit down and play hot pursuit on PlayStation 2. It was a particular country road, so your job with like forced feedback, driving, steering wheel, and gas pedal, brake, all that, you were just meant to drive down this road and you were allowed to answer this voice as much or as little as you cared to, which said to each participant in either a happy voice or a subdued voice, the exact same thing as you drove. And they looked at how people. Happy voice? Subdued voice? Generate some theories of. That's the main figure of the whole paper. You'll see that the results are significant, in short, people who'd been encouraged to be happy and upbeat and energetic paired with a happy voice, did fine, about with a subdued voice, and well more than twice as many accidents, and the converse is true for the upset drivers. Upset drivers matched with an energetic voice did very poorly compared to a set of drivers matched with a voice congruent response times to in-game honks, similarly, and the extent of communication with the voice.
    All of these look poor when affect and voice are mismatched.
    All right, it blew my mind. I thought that was mind-blowing, I thought that was really surprising, I thought it was going to be one or the other. I figured maybe the happy voice, you know, but this is like pretty clear result.
    And I think there are a lot of implications, especially when driving, emotion on the road is very important and so I think that the specific example is really salient, but I think it also comes into affecting machine design across the board, so this is the key sentence from this paper that stuck with me.
    Which is that you know, people find it easier and more natural to attend to voice emotions that are consistent with their own.
    So what I drew from that, and as far as it applies to the rest of robotics research or designing things for people to use in general, is that personality is actually the human API. And that we experience personality in devices to the extent that we hear our own voice or see ourselves in these machines.
    Sorry. Thank you. That's the talk.
    [applause]
    
    
    
    ZEESHAN:  Where did I see a hand? Oh, OK.
    >> Was there a control group where she didn't prime the emotions either way and then see whether they responded better to happy voice or subdued voice?
    >> There were just the two groups.
     AUDIENCE:  Since this was a little while ago, have they done any further studies on this kind of stuff?
    >> Ooh, good question, I don't know the answer to that.
    
     AUDIENCE:  Thanks, this was fantastic. Since you've obviously been thinking about this deeply for a long time, do you want to take a minute to riff on like what implications this has for personal assistants like Siri and Cortana and whoever else is making one these days or Google, or specifically in-car agents and what that means?
    >> Sure, so the questions is do I want to riff on the implications this has.
    [laughter]
    >> I think that it would be much more interesting, you know, to see this entire room of people riff on that question. But, you know, I think that insight gave me even more questions and maybe the best questions do. It sort of made me skeptical of like listening to music or, you know, effects in advertising, also, to what extent things do or do not feel congruent based on what you think or believe about yourself, but yeah, I don't know, I guess I just wish I had the rhythm ability of the little yellow drummer robot at the end of the day.
     AUDIENCE:  This kind of overlaps with the last question, but I was curious to get some more specific -- a more specific understanding of how it's affected your work in particular.
    >> So in terms of how it's affected my own work, I think it just really convinced me that we should have more UIs that are capable of you know, meeting you where you are. I haven't created or seen a lot more of those since the time I was working on Autumn. But I think I'm more attuned to be concerned when I'm listening to, you know, robot voices or you know, just sort of think about it in an ongoing way, which is my goal for this audience too, so.
     AUDIENCE:  So what are some of the ways that you think that a device or robot could perceive and try to match personalities? Is it just -- yeah, I guess what would you use for that?
    >> Yeah, so I mean in this case, the thing that really started me down this path of, you know, how do you build a UI that can detect affect or personality, was the Autumn UI and I would love -- I really wish -- I would prefer if I could to show a video of people just like stabbing the button on Autumn, but I think that there are a lot of ways that people will really remarkably sort of register, that I think that maybe UIs like that could do more -- you know, so often we see okay, and cancel, right? That tone, it doesn't take into account, there's no voice on its part or yours. So in the same way that people meet and say how are you, that's what I'd like to see more of. Cool. Thank you, guys. Oh, sorry, one more question. !
     AUDIENCE:  So this shows a performance impairment when there's a mismatch on extrema, a happy person listening to a subdued voice, a lot of times with Siri or Google Assistant, they have one voice, right, and I'm wondering has there been any extensions on this, looking for if you had to pick just one voice, like, what is the right one to kind of optimize your performance across the whole population of emotions people might have?
    >> There is a related paper that I wish I had the cite for off the top of my head, but it is a question to what extent the voices that we use in machines have, you know, become homogenous or answered this question for themselves. I'm not the first to note that Siri, Cortana, Alexa all have chosen default feminine. The interesting thing about that is the design decision comes back to one paper that we talked about at the time when I was working in that lab, which is that in the study, men listening to machine-generated voices were slightly more likely to listen to a feminine voice, and there was no effect for women, and so everybody went with feminine machine voices from that one paper. I kind of wish phones had gone with like a random as a default, and they you could say, well, I really prefer this one voice, you know, that might have been a neat alternate future that we'll never know how that experiment will go, but yeah. All right. Find me after for more questions, it's been great.
    [applause]
    
    
    ZEESHAN:  All right, one logistical thing: The after party tonight starts at 7. Yeah, at the City Museum. If you haven't been, it's a really fantastic place. There will be desserts there. That's what Strange Loop is offering, as well, gooey butter cake I was told is one of them. If you're at Strange Loop, of course, you can go to the party, if you're only at PWL Conf you can go as well. First and foremost I want to thank the speakers. They've done a great job.
    [applause]
    
    Without them, it would just be me talking the whole time, which would be terrible!   Awful.
    I really want to thank Norma, White Coat Captioning, for the amazing work that she does all the time.
    [applause]
    
    If you are looking to run a conference or an event, you need to hire her, her company, definitely do t100% recommendations here from Papers We Love. We want to thank our sponsors who give us all the swag that we have, all the speaker arrangements for them to travel, and everything that we can do, so thank you, sponsors.
    
    [applause]
    
    And I want to bring everyone up who's a Papers We Love organizer and volunteer and staff. I guess they're all in the back, they're not going to come up, but you can see them over there. They do a lot of work. They've done all the amazing work today.
    
    [applause]
    
    And most of all, thank you to you, because we don't really make any money off these conferences, we like doing it because people are happy and they talk about getting a really good experience out of this, so thank you to the audience. Thank you.
    [applause]
    
    That's all, folks.
    >> Go start your own Papers We Love chapters in your communities!  Whoo!  ...: 
