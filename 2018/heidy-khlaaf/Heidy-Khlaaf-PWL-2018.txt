    Standards We Love
    by Heidy Khlaaf
    
    >> All right!  Take your seats, the last two talks of the conference. Are we still getting a woo-hoo? Things still going well? 
    [audience whoops]
    >> So the next speaker, Heidy Khlaaf, was the first speaker that we invited and she said yes, obviously, and we were so happy about it. Heidy just finished her Ph.D. at the University of London. Which yay, finished her Ph.D. and she's a research consultant for Adelard LLC, and when we were talking about her talk, Heidi was like, I have something to say, about the area of verifications and especially, systems that have been around for a long time that obviously are very crucial. This is a talk that she worked on for a long time. We knew about it for a long time and we were like, we can't wait for this and now we get to unveil this today, which is awesome. So thank you, Heidy, and without further ado, take it away.
    [applause]
    
    >> Hi, everyone. So actually, I didn't know what the other talks were going to be, that just happened with Casey and Roopsha, but my talk is a bit of a mashup between energy and verification, and I'm glad that they went first because they gave a pretty good primer of a lot of the stuff I'm going to be talking about. So this talk is aptly named Standards We Love. I work at a company called Adelard. In the UK. I'm American, by the way, if you're wondering why I don't have a British accent. So sorry about that. I'm a research consultant, actually, and that's a bit of a vague term, but I do quite a few things that all tie in together. I evaluate, design, specify and verify safety-critical systems, I carry on assessments on a lot of safety critical systems again and I help produce standards and guidelines for safety and security-related applications and development.
    A little bit of background about myself. I finished my computer science at UCL, which is University College London in 2017, it was on temporal logic model training and verification of software systems. I'm not going to be talking about that, though. As a Ph.D. student I did collaborate with Microsoft research Cambridge to create and extend a tool T2. I did it.  TL, LTH and. I focus on verifying things like Windows device drivers to find various types of errors.
    And now I work on safety critical system which uses very different technologies for what you're probably used to. like if you work at your average tech company like Google, SalesForce, Facebook or Twitter and that's for a very good reason. If you look at a definition what a safety critical system is it's a system whose failure or malfunction may result in the following outcomes: Death or serious injury to people, loss or severe damage to equipment, property and environmental harm. When I say safety in this talk, I actually mean like people's lives.
    The consequences of a safety-critical system malfunctioning isn't the same as like your messenger app or Instagram or a website going down. I think there was a big AWS outage yesterday that happened in the UK, but still that likely isn't going to kill hundreds of people at the same time.
    And companies like Adelard exist to ensure the safety of various systems. We we work in aviation, automotive and rail and defense, medical, finance and most recently autonomous systems. We actually won a grant recently to work on verifying self-driving cars or insuring self-driving cars and it's actually a mine field on its own. So we're working on a lot of different things. So you probably know what formal verification is at this point because we did have a talk on it. But I think this quote is a really good quote to bring up and you've probably seen it before, which is "program testing can be used to show the absence of bugs, but never to show their absence," by Dijkstra. So it's always good to keep this in mind. Some of you might not know the actual formal verification. It's the process of establishing whether a systems satisfy some requirements using formal methods of mathematics and that definition is a bit vague on its own so I wanted to give an example of the process of formal verification, using a specific technique called model checking. If you had a model-checking verification tool, this is what the process would look like. You would first take your code and you would formally monitor it in some appropriate framework and then you would take your requirements and you would formally specify them in some expressive specification language. Temporal property is an example of that. And then you exhaustively explore a systems model and finally you do your analysis, so if not all the specifications were satisfied, you analyze the findings and the consequences and this is a process that people generally refer to in both industry and academia when they say formal verification and you can replace this model checking bit with any technique. It could be aspect representation, theorem proving and so on. So in academia, there's a lot of focus on models and the checkers themselves. So this bit right here, and sometimes the specification, as well, but in industry we actually spend a lot of time on the analysis stage. So I'll get more on that later on in the slides.
    So at first I wanted to talk about my experience in academia versus industry. The issues that we face in industry differ a lot from those we face in academia, which is why we have these different focuses that I talked about in a previous slide. So I created this pie chart. It's not reflective of any studies, just my own personal experience, so I spent about 5 to 6 years in academia, and I didn't find that many bugs, but relative to an academic, I did find quite a few. And in my one year of industry, I found significantly more bugs. So the bugs that I found in academia are just a fraction of a percent what I see every day in industry. And that's because in academia we're very invested in producing more nuanced and expressive formal frameworks. So the bugs we found are very specific and rare and especially if you can't run verification tools on things like 10,000 lines of code or more. And in industry we have of industrial level formal verification frameworks that are pretty good at finding thousands of bugs and millions of lines of code, but the bugs can be a bit shallow and our issues really regard how and what bugs should be fixed and their impact in security and the actual safety in the system. So given that we can't even get our printers to work, our apps crash and I'm you go telling you about these thousands of bugs that we're finding, you might be wondering, how are we not all dead yet?
    [laughter]
    
    So that's what we'll be talking about today. I'm going to be focusing on three points. How software safety critical systems is assured, verification of specific type of components and that's one of the biggest challenges in the nuclear industry and smart devices. And actually Casey talked about smart devices and the grid and finally I'm going to be talking about lessons used from standards lessons and guidelines.
    In general I would like to talk about all of the safety critical systems that I mentioned but I can only talk about one so I'm going to be focusing on the nuclear industry. But you can generally infer the process of the others from these slides.
    So a bit of physics 101. This is kind of what's inside a nuclear power reactor. You have something called a containment structure. And in that containment structure, this is where the nuclear reaction is actually happening. You have a reactor vessel and these rods come up and down and control the nuclear reaction that you have. And these orange lines actually demonstrate the water that's in the system. So there's water flow through they are that's getting hotter and hotter with the reaction and you have a * pressurizer. The reason why you need a pressurizer is because water boils and evaporates at different temperatures at different amounts of pressure. So the point of this reaction is to actually heat up another water system and this system actually the purpose of it is to boil the water, or for it to evaporate, really, and so you're creating all of this steam which goes through a turbine and then circulates through a generator and actually I'm really happy that Casey gave her talk so I don't have to explain the rest of the grid and how that works and any water left in the grid is condensed back. Please don't ask me any questions about this, I'm not a physicist. Luckily we do have physicists who know more about than I do. Everything is controlled by something called a smart device. You have it here, you have the reactor vessel, you have something that's looking at the pressure, something that's controlling the temperature, the same with this other water system, as well.
    Had so what are the smart devices? Well, they're really just embedded devices and a lot of times they look like this. They're not this huge. They're usually about this tiny, and they vary in functionality.
    And they're written in C and Assembly from the devices that I look at and they can include things like FPGAs, which is a field programmable gate arrays. If you don't know what that is, don't worry about it. There's usually no underlying operating system. They're interrupt-driven. Which means that the software directly interacts with the software. So there's no managing system to manage the interrupts or memory for you. That's all done through your code. There's other tricky bits as well, which is useful pointers in C and communication protocols.
    And the use of compilers varying depending on the processer and the hardware. I think something I had the wrong impression of when I was working in academia that everything is standardized. But it turns out these companies actually all write their own compilers and they all implement their own architectures and that's actually a mine field if you're thinking about it from a verification point of view. * and there they're really not meant to be verified, either. There's rarely specifications and requirements found and that's because the nuclear industry is a very small customer when it comes to buying smart sensors, they're everywhere in the world, they're used in industrial farms, they're used in factories, they're used in a lot of places that aren't safety critical. Companies don't see a point of writing the viable software for one very small customer and finally they can range between 10,000 lines of code to millions of lines of code.
    The device is very tiny, but they're actually very capable, as well. So they're really just embedded applications. They're pretty good at doing things like reading and writing data. Setting timer registers and adding input captures, that sort of basic stuff and I want to give you a feel of the kind of code that I spend all of my time looking at. There's something called ARMC51. And it has special data types, one called sfr and sbit. So this is a toy example. This is an example of what you would actually see in a smart device. But just to give you a feel of it here I ack sincerelying some ports and registers. You have to * know here sincerely is a timer interrupt. It's an interrupt that's triggered every X amount of sections and most of the functionality of these smart devices actually implemented within the interrupt itself. You don't have a nice control flow. You just have intros being triggered every few seconds seconds and the function for the specific interrupt does some sort of specific bit that all works together. * so here I'm initializing some Keras, temp 1 and temp 2. I'm writing data to an register right here. Writing data from a port and it's kind and finally enabling interrupts. It's kind of like writing abstractions, library, things have advanced quite a bit for normal developers, but if you're looking at sort of smart devices you're still kind of programming like it's 1970.
    So you might be wondering, well, why use these systems altogether? C is a fantastic way to shoot yourself in the foot. Well, that's actually a pretty good question. So you have to remember that a lot of nuclear power plants were built in the 1970s, and they used analog sensors. A lot of them did. And now those analog sensors are no longer found for many reasons, they're too expensive to make, they're unmaintainable. We have software, so why should everything be analog? So I think you'd rather replace something in your nuclear power plant that has software it? . You have improved functionality, microprocessors and microcontrollers give you a lot of power now, because you now have better accuracy, calibration and diagnostics. If something goes wrong, you can look at logs, you can trace it back and they're configurable but not programmable. It's actually very difficult to hack into it, you you can't reprogram it. It's tricky and there's a lot of sort of password, user password implementations that allow you to configure it, given that you have the right access, and finally, it's pretty efficient and fast. As with why most people use C, actually and you can find these anywhere. We call them commercial off the shelf because you can just buy them as is and purchase them without any specialized orders. They perform a very defined and simple function, so you have a temperature transmitter, all it does is measure the temperature. Pressure regulator or voltage regulators.
    But as it turns out we can't just install these things in any nuclear facilities please, which is very important, actually.
    So in the UK we have a office for nuclear regulation, there's an equivalent in any country and you have the IAEA. Which you've probably heard about which is international agency and you've probably see then in use with regards to policies of who can have a nuclear program, dismantling a nuclear program, all of those things. And each year they organize workshops to talk about a lot of things including the inclusion of software systems and how they should be addressed. So this is from one of the workshops in 2017. One of the things they talk about is what to prioritize and they noted that one of the most important things is that we need to focus on justification of smart devices and how we a sure them. The company I work for actually works with these agencies to come up with standards and guidelines that would make hour nuclear power plants a lot safer.
    Specifically we talk a lot about smart device design which I'll talk a lot about today.
    We've been pushing some things that we think really should be included in guidelines.
    So the ONR, which is the office of nuclear regulation has something called the safety assessment principles principles, which are defined as the primary principles that define the overall approach for nuclear installations in the UK. So of course part of that is the justification of computer software. It has three aspects. Production excellence: Demonstration of excellence in all aspects of production from the initial specification through to the finally commissioned system. And then compensation measures. Which is if you have any gaps it means you have to compensate for them. And finally independent confidence building measures. Which means that you need an independent assessor to tell that your system is also fine. If you think it's fine, I mean that's great, but you need independent assessment, as well. So the way that these are satisfied is through a lot of things. I'm going to give you some boring definitions of what they mean and talk about how we try to satisfy production excellence.
    So the definition of production excellence and for you to be able to satisfy it is application of best design practice consistent with accepted standards for the development of software for computer-based safety systems. Basically that means that you're following some standard, there's some best practice that you can prove you're following and that you fulfill all the requirements. So the standards that we most feel in the safety-critical industry is IEC of 61508. IEC is sort of international standards. You have more specific standards which are industry-specific. So you have 61513, which is for the nuclear industry, you have 26262 which is for all the motive and you have DO331 which is for aviation and they actually have the strictest standards for obvious reasons. I really hope that most people follow them. The second thing that you need to do is follow a modern standards quality management system. You might be aware that your own company actually has a qualification for ISO9001, and what that means is that you you need some traceability, right? So if you use things through version control whether it's through GitHub, clear quest or clear case, that's great. Your management system whether it's document or code, that is something really important for satisfying production excellence and finally you need a comprehensive testing program which isn't new to many of you either.
    The second bit is independent confidence-building measures, which is the complete and diverse checking of the production software by an independent team. So this involves getting an independent team on board, which Adelard is an independent team. It's important that you have an independent testing team to come on board and check your system for you.
    So I mentioned that production excellence reference standards to be followed and I'm not going to bore you with thousands of pages of standards, so how relevant is formal verification in these systems? So here's an expert from 61508. Again it's one of the most safety standards applicable to all safety critical industry. And the standard has multiple parts but this is a table of contents of the software requirements bit. And the way that 61508 views software is sort of in a development life cycle point of view. So if you notice here, it breaks them down in software safety, requirements, specification, this is when you're writing your requirements. It talks about the validation plans. So how are you going to validate your software when you implement T it has requirements about software design and development it. So it breaks things down to software detection design, tools and programming languages that you're using, detailed design and development, code implementation, software testing, software integration. There's a lot of things to unpack here. And then you also have software modification and representation procedure. And software system validation, modification, verification, and actually when we talk about verification here, they don't really mean formal verification, they're talking about in general, verifying your system. And of course software verification is part of that, but it's not all of it. So each set of life cycle requirements has TRM, and that stands for techniques and measures. they are they provide you some table which tells you OK, these are the techniques that you need to check off and in the specific case, full' look at the software design, * testing and integration, sorry, it goes on for a while. Here they mention that you need to do dynamic analysis and testing, functional black box testing, performance testing, model-based testing, interface testing and a few others, and then they talk about formal verification. You don't actually need to do formal verification all the time. It depends on the safety and integrity level of your system. So we actually categorize systems to four categories, because they're not all equally as critical or equally as important. So a lot of times you won't need to do formal methods for that type of system. The highest is 4. And in this case it's when you have a very critical system that you carry out formal verification. So here's another table from the standards, saying that static and dynamic analysis are required by most safety integrity levels for the verification states of the development life cycle. Now, that makes sense, right? You're at the verification stage you need to do some analysis. But here a it's referencing to table B.8. So let's have a look at that. And see what they say about formal verification. What's required to be carried out. Boundary valley analysis, OK. Checklists. Control and data flow analysis, a lot of that might be familiar that here. OK, fair enough. Error guessing and walk-throughs. I think that's a bit strange to put here, because you know, you wouldn't really consider a code review static analysis, I mean technically it is static, but I wouldn't consider that static analysis. You have symbolic execution. Which is not static analysis. Design review, static analysis of a runtime error behavior, which I think is a cyclical definition. If you're saying this is what static analysis is, and then you hads static analysis, I don't think that helps anyone. so if you're familiar with formal verification or static analysis in the case, you may have noticed that terminology doesn't really make some sense and it doesn't really add up. And it doesn't really match our modern terminology when we talk about these kind of systems, like are, you know, Roopsha like I said gave a pretty good primer on verification terminology and these don't seem to add up to the kind of techniques that we carry out. Well, if you look at the people references for these techniques. These are some main references about static analysis and formal verification. We have systems and software verification techniques and tools. OK, more model checking. Great, and then spin model checker. It's a classic model checker that was created. I think it was before 2003, but this was a paper referenced in 2003: So these papers have all one thing in common is that they're pretty old. Like formal verification has come a long way, but all of these papers that are at least -- the latest one you can see is from 2003, but they are pretty similar to verification, but they don't target our system. And surely verification has come a long way in 20 years and maybe standards are really slow to update.
    Well, that is true. Standards are not perfect. They are pretty slow to update. But it's important to note that all standards are, they're a consensus-building exercise. It's just a particular group of people who got together in a specific moment in time and decided this is how we should verify safety critical systems, so the next day what they decided might get outdated and what works well for predictable systems, when we have this idea of standards, when the idea of policy of standards came about, technological systems weren't kept in mind. So defining a standardized way to build software is really difficult and software systems and verification policies advance way faster than standards. It depends how fast the committees work, so they can get outdated pretty fast and interpreting a standard against each unique software system may pose difficulties. So for example, 61508, so I have a typo here, provides no guidance on when and how to apply these verification techniques and it's a checklist approach. But the thing is all of these formal verification techniques depends on the system that you have. And standards don't really guide you in that direction. And software will still have bugs. It's really important to know that even if you follow these standards, you will have bugs in your software, so it's not the end-all, be-all of what we should be following.
    But on the flip side, four the specific devices that we do look at, these smart sensors, not much has changed technologically in 20 to 30 years, which is really unfortunate.
    State of the art formal verification techniques, and that's usually all coming out of academia, assume a lot of things that we can't make assumption assumptions about, like you have a standard C compiler. GCC/LLVM are the favorite things to look at. I have yet to come across an Intel 86 device.
    I always assumed hey there's something doing everything for me, but these devices there's nothing doing everything for you, you have to do everything yourself. Which is really painful. Take for granted the scheduler, your memory manage. These are all really nice luxuries that we've been taking advantage of for the past 20 years or so.
    And we don't have any support. And it's not scalable beyond 10,000 lines of code and the minimum amount of code that I've looked at was about 12,000 and most of them are about in the hundreds of thousands. So scalability is a huge issue for us. But let's look at what some academic papers look like to give you an idea of why this is problematic for us.
    So I only picked out a few, but I want to talk about this one. Analyzing memory resource bounds for low level programs. This was an ISMM in 2008. They say that they do memory resource bounds inference for assembly-level programs. OK, great, because the kind of code that we look at is very close to assembly. They do a fixed-point analysis and formalized inference system for a small assembly language. OK, this is when things get tricky. They really mean a subets of some assembly language, really a toy example and that's not something that we can work with. And it's not really automated in the way that we had aye like it to be. So the example of code that they managersed in the paper that's analyzed * this bit right here, this is not memory safe. If it's not obvious, please don't code like this. But again, this is very different. If this is low level to you. If you remember a few slides back, we go lower than that. We're actually interacting with the hardware. But yeah, they say we do it for embedded devices but not really.
    There's no interrupts, and unfortunately most academic papers are in line with this type of analysis, even if they say we do embedded verification for safety-critical systems, they don't really. I've read through a lot of these papers when I started my job and I was very disappointed.
    But there's some work that is beneficial, right? There are some people who do try. This is an older paper from 2001. Analyzing memory research bounds for low-level programs. So I actually really enjoyed reading this paper because it was actually very relevant to the work that I do, so it did static analysis for an interrupt-driven Z86-based software with hard realtime requirements. This is great, right? It's one of the ore only papers I've ever come across, if you know any other papers, please send them to me. Z7886 is not a standard software that everyone assumes you're using. And stack analysis and interrupt-late yepsy analysis.* But then if you look at how they do it, they do it through modeling the program counter and mask architecture. So he this build not only a control flow of the code that is being used but they actually have to now reason about hardware bits, as well. They have to model the interrupt mask register and the program count iralong with the software that's being used and now your * problem of exponential state space is even bigger than before and the whole point is to record whether interrupts are enabled or disabled so this is actually targeting a very small, like niche property and we're interested in a lot more properties than that. So the state of the program, the PC, and the interrupt mask register. This is of course a very manual process, and it's not scalable. And the example here it only covers 1,000 lines of code or less. And that's pretty far from, you know, hundreds of thousands of lines of code or a million lines of code. But at least it has potential for exploration, right? You can say that there's at least some strategy out there. But smart devices are being installed and replaced every day. We really have only a few months to verify it. Sometimes less than that. Sometimes it's an emergency and it needs to be evaluated now. I can't sit here and model the architecture of every single processer I come across, look at the compiler and then model the hardware with the state space of the software, right? That's not very feasible.
    So we need a short-term solution.
    So we have two issues. How do you manage to meet what's required by the standards, especially that we don't have academic work to help us, and how can we adopt new techniques to help us assess our systems especially when ... ...
    Recently the IAEA has adopted this approach in their guidelines which is fantastic, because it's a much better way of looking at properties and formal verifications for systems verifications than the checklist approach I showed you earlier. So we view verification of properties in three different categories. You have a property-based approach and this is really focusing on the behavior of your program. Functionality, accuracy, timing, failure of integrity, noninterference and then you have vulnerability assessment. And then finally you have coding standards and complexity metrics and this has to do with the standards compliance, actually, following -- I didn't show you -- some of you might have heard of MISRA-C which is a standardized C that's used in aviation. That is subset of C when you're doing software for airplanes, so things like that that you have to follow. I think a better example is a linter. Some of you might have used PC lint. Or if you're programming in Python, Python has Python lint, as well. So we take these categories and try to engineer our own tools. So we start with available tools in academia, we have a tool called pharmacy. It's compatible with standard C, but it's a very lucrative open source framework for verifying properties. In industry, there's industry wide tools. If you have not heard of these tools before. Do take note if you're interested in automated verification on the code that you're running, but they have limited properties. They look at what we call runtime errors, so like data race, you know, uninitialized variables, that kind of stuff. It's not really looking at the behavioral and functional aspects of your program. And surprisingly both pharmacy andment ...: If I had known that, I wouldn't have done my thesis on model-checking but that's fine. I can always learn.
    
    [laughter]
    
    And we engineered these tools. So we do code transformation to reduce programs to standard C:
    So this is architecture of Frama-C, there's a lot of fortification environments. Are Frama-C is open source: It also has AST implementation so you can manipulate the AST through it and AS. And this. If you use LLVM, you might have heard of the LLVMvR ... And then on top of the Frama-C kernel, you have plugins, right so there's an API that you can use to interface with this kernel, so that way when you're implementing a tool or a plugin extension to Frama-C, you don't actually have to compile the code from scratch, you don't have to dig in and figure out what they're doing in the form of implementation, you can just call these really useful verification functionalities to implement your own verification technique and there's huge libraries that people have developed for Frama-C for the kind of code that you have. So it's extensible through new plugins that may use functionalities provided by existing plugins and the kernel and allows for plugins to be written with relatively little effort. The plugins are in OCaml and you can write almost a script-like file in OCaml. So for us we actually have to do a lot of that because we're constantly getting new devices p you know, new architectures, new C extension that is we have to tweak the plugins that we have to make it work. So we can verify a few things in Frama-C. So it's really great because it allows for rapid prototyping and deployment, so in production we have things like stack analysis, research analysis and functional analysis, this is exactly the kind of solutions that we're looking at, because otherwise we won't be able to keep up with the demand of the installation of smart devices or replacing smart devices. anyways, the same ideas that I've used at an academic level, I'm now using at industrial level. Just more adapted. So even if academic tools did solve our problems, that would be great if they did, but we're just getting started. So even if you have a nice list of results, you need manual inspection and judgment is required for every single set of results that you get.
    And we don't get just like five bugs, we get like thousands of bugs that we actually have to inspect. So our formal verification tools can just say here, I found 1,000 bugs for you. We actually are required to go through every single one of them and sentence them. And this is what we call the sentencing phase. It's actually manual labor. And that's because we have a lot of questions we need to ask. This is sort of a social-political questions, as well. This one is pretty easy. If you've used any static analysis tools you can always ask, are the bugs false positives. The bug that you get actually might be a false positive. Half the bugs that have been found, are they rel relevant to the safety or security. You have to look at the bug and say, OK, does this affect the safety of the system? We talked about modular components earlier. Can you prove noncompliance between the safety bit and this LCD display. You might have a bug in the LCD display and the safety standards don't care. Which class of bugs are more critical than others? Right? So if you have a lot of bugs that you need to look at in a short amount of time, you look at a data race, right? You look at the data requirements. What is more critical at that point in time for your application? What is the probability of an actual bug occurring and what is an acceptable level of failure? This is actually tied into the safety integrity level. Safety integrity levels aren't random. They're actually tied to the probability of failure, the minimum probability failure for your system. So for SL4, which is the most critical, the probability of failure needs to be way way lower than SL1. SL there's usually an acceptable amount of failure. It's usually not installed in a place in the nuclear facility where if it goes wrong, things will go wrong. That's usually the idea behind SLs.
    And can testing and field data demonstrate infeasibility of a bug? From earlier talks you saw that a database took out electricity for the whole northeast as an example that keeps coming up. So you see that these data races are completely legitimate. They're not false positives, but this device has been running in the field for 10 to 20 years and no errors have ever been reported. What do you do about that? I remember being a computer science student and my code was not meant to be working, but it was. So you wonder, do I submit it or do I not? So what efforts are required to repair a system? So if it's significant effort and you have to take down the system to repair it, but then the smart device ha you're looking at has been operating fine for 20 years? Is it a probability thing? It's actually difficult because these are realtime devices that you can't always predict when the bug will happen and do repairs themselves pose a safety and security threat and what is the impact? I've been in a lot of cases when I programmed where I would fix one bug and be like yes, I chased it down. This is it. I've been pulling my hair out and then 100 more bugs come out after it. And you're like, no, that was just a symptom. There's a lot of other bugs I need to go through and we have to worry about that. We can say, OK, this is a bug, what if we fix it? What if more bugs appear in the system? We have to think about the implications of every change that we make, which is why the standard focuses a lot on changes in your system.
    So verification is actually really a small part of what we do. We also look at hardware verification, specification correctness, right? Can you imagine writing precise and correct specifications for these devices? It's very difficult to do. It's always been an open problem to write correct specifications and then you also have to do standards compliance, you also have to look at where are the smart devices being implemented in the system as a whole? There's a lot more to think about than just the verification of code.
    So you might think, ah, well, thank God I didn't work on safety critical systems. I don't have to think about any of this.
    [laughter]
    
    But these concepts all tie into software dependability. It's actually the same concept phrased differently and each of you if you develop any sort of system, you should care about software dependability. And software dendability consist of three concepts, there's attributes, athletes and means. I'm only going to go through the attributes, actually. So you have availability, reliability, safety, integrity. And absence of improve system alteration, maintain ability: Ability for a process to undergo modifications and repairs. So you've actually heard all of these concepts before, just in a slightly different context.
    So the standards and the processes that I brought up all address these aspects and ensure the dependability of your software, it may just be phrased in a different language. So the questions I described earlier are we're really trying our best, but those in academia and industries, other industries, not the safety-critical industries, don't consider these issues, yet the impact of our community affects your everyday life. I mean you probably flew a plane here or drove here or used a train. You're using the electricity in this room right now. Yet we never think about it. I'm constantly overwhelmed by the new platforms that come out every single day, yet none of them care about safety-critical systems. So invite you to think about three things: if the appropriate technology exists, it will be adopted. People who assess these kind of devices know about them and can use them to their benefit. Recognize that what safety entails is always evolving and will in fact affect our views on system dependability and security, verification is just a small piece of that puzzle. A good example of that is that we have autonomous systems everywhere. What does that mean for safety? Assurance and verification? So our idea of safety isn't static. We actually change the standards to keep up with new technologies.
    And that standards and guidelines already exist increases some security and safety, applying best practices where applicable will depend on dependability. The amount of research and work that has gone into these standards actually could really benefit the whole technological community, because they talk about things that we don't think about until later and vice versa, right so you might be surprised by what you can learn from a standard or a guideline for a safety-critical system. So I hope you learned something today and if you have any questions, please don't hesitate to ask. This is my Twitter handle, so in case you'd like to reach out at a later time, I'd be happy to talk, I'd be happy to chat and thanks for being here.
    [applause]
    
    >> Awesome, thank you, Heidy, thanks for coming all the way from the UK. OK, last talk is in like ten minutes from now. 4:25, all right? So be about graphics, it will be fun ... ...
