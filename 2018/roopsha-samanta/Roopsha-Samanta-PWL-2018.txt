    Computer-Aided Concurrent Programming
    by Roopsha Samanta
    
    OK, I think we're ready for our first talk. So Roopsha can come up. I think she's already ready. So when we were curating this conference, you know, we had a lot of initial ideas, and actually Roopsha, who's an assistant professor at Purdue, has does a lot of great work with verification around concurrency. She'll talk about computer programming today, but she kind of came through a lot of people who recommended. People will find me and others will be like, you should get this person, this is the person you should have, and that was really cool, so and we're really excited to have Roopsha, I've now looked at her work and oh, yeah, this is going to be awesome, so without further ado, Clint can make the switch and Roopsha Samanta, thank you everybody.
    [applause]
    
    
    
    >> Don't forget to check out the Papers We Love play list on Spotify. PWL Conf No. 3!  It's a goodlist,
    >> Wicked good!
    [applause]
    
    
    >> Sorry about that. I bet that was a concurrency bug, as well. Thanks Zeeshan for the introduction. It's my pleasure to be here in front of this amazing audience in this beautiful venue and to kick off PWL2018. So I have a lot of interesting stuff that I want to share with you. So let's get started without further ado. 
    >> Sometimes concurrent programs are ubiquitous, they are everywhere from our cell phones 0 to your cloud platforms to our device drivers and so on and unfortunately concurrent programs are really hard to reason about and the main reason is that there's a combinatorial explosion that one has to think of. Which is leading to the explosion problem in concurrent programs. So unsurprisingly, concurrence by bugs are you write a program, and you introduce a bug and these are fairly subtle and hard to debug. In fact, two of the most catastrophic failures that we have seen due to software bugs, the teerac-25 radiotherapy machine overdose disaster and so you might already know this, but it's interesting to actually note that many of these bugs, such as atomicity violations, race conditions, deadlocks, livelocks and so on are actually due to errors in synchronization. So wouldn't it be wonderful if you as a programmer just had to write an unsynchronized program, with no synchronization and only make sure that the program is correct, that it meets the specification when the threads are executed sequentially and then this magical synchronization synthesizing the steps in, inserts synchronization and makes sure that the final concurrent program is correct with respect to the specification.
    So this is the green of computer aided concurrent programming and it is something that has captured the imagination of multiple researchers over several decades, going back to the early 1980s, and this is the focus of today's talk.
    So I'm going to take you through three papers, only one of which is mine. The first an old seminal paper from 1981, which was one of the early papers, if only the earliest paper, on synchronization synthesis. The second is a relatively recent paper from 2010, with a particularly cute idea that I like, and the final, the modern approach is work that I have completed fairly recently with my collaborators.
    So let's get started with the seminal paper.
    So this is a paper that I'm hoping many of you have at least heard of. So this is design and synthesis of synchronization skeletons using branching-time temporal logic by Ed Clarke and Emerson from 1981, this is the paper that introduced model checking which was later recognized with a Turing Award in 2007 and has nearly 4,000 citations if you check online. So if you actually look at the title, OK, and if you take a look at this paper, however, you will notice that this is almost entirely a synchronization synthesis paper.
    OK? So here is an instance of a paper that set out to solve the synchronization synthesis problem and ended up presenting a really elegant solution for it, but along the way, founded this Turing Award-winning area as it was solving the synchronization synthesis problem, so funny how certain things can turn out. Another fun fact is that this paper was actually Alan Emerson's Ph.D. thesis, which he did with super visor, Ed Clarke so that's super amazing and Alan who was also my Ph.D. adviser always mentioned the burden of having a famous first paper that's something that's always hard to live up to. It's an interesting thought. I think I could have lived with that.
    [laughter]
    
    So let me give you a bird's eye view of this paper. One-line summary of this paper. This presents an algorithmic framework to check and synthesize synchronization for temporal properties of finite state transition sis sems. So that's the one-line summary. Let's take a closer look. Processes in this paper are what I'll call finite-state synchronization skeletons, which are systems that essentially capture the concurrency in the program and suppress details that are irrelevant to synchronization. For instance, a process could be doing a bunch of local computation which is irrelevant to synchronization and that is abstracted away into one node in this finite label transition system. The communication model is a shared memory model. Which we all know, it's also interleaving-based so let me describe that. So this means it consists of an interleaving of events from the threads processes where exactly one thread takes a step at a time, OK? These are threads might be scheduled by nondeterministic scheduler, but exactly one process makes a step at a time.
    The specification is temporal logic and I will give you a brief preview of that shortly. The synchronization is synthesized in the form of what are call guarded commands, which are essentially atomic actions labeling the transitions of the transition system where the semantics is that the transition can be taken only if the guard is true and in which case the command, which is essentially an assignment of values to some variables is executed with the transition, and finally the procedure is tableau-based and I'll show you a preview of that, as well.
    Here is the paper in action on an example that the authors worked out in the paper. So this is what I mean by a process' synchronization skeleton. It consists of three states and is fairly simple. Here is the complete specification of the expected concurrent behavior of this program in temporal logic, OK? So this includes global properties expected of the concurrent program, such as mutual solution or starvation freedom. Also it includes a precise characterization of each process' behavior in temporal logic. So it says things like each process is in exactly one of these three nodes or code regions at any given point of time and so on.
    So this is a complete specification of expected behavior, and what this paper generates, given this input, is a final completed synchronized skeleton that looks like these. So these are two processes where the transitions are labeled with guarded commands. For instance, here, this guarded command means that this transition can be taken when tried to holds and this transition sets the value of the turn variable to 2, OK? The concurrent composition of these two synthesized processes is guaranteed to satisfy this temporal logic specification.
    So this paper I just want to emphasize goes much beyond synchronization synthesis. It's helpful to recognize that it makes several ks, such as introduced a new branching time temporal logic called computation-free logic CTL, which we will see next. It introduced the modern checking procedure which is a procedure for checking if a finite-state transition system satisfies certain properties. It also introduced a general procedure for synthesizing a structure that satisfies a CTL formula.
    So let's do a quick primer on temporal logic, then. Tem practical logic these processes are modeled, these programs are modeled using what are called Kripke structures. Where the nodes are labeled with atomic computations. So where does the infinite computation crop up? Well, if you unwind this Kripke structure and look at all the computation that is can happen in this structure starting from the initial node, you get this infinite computation tree, which is called -- infinite tree that's called a computation tree:
    So what does CTL look like? CTL is just one class of one kind of temporal logic. There are many others out there in the field. So this is what a CTL formula looks like. A CTL formula is something that holds in a state of a Kripke structure and is built from atomic propositions, boolean logical connectives, and these quantifiers which you see on the right which are called path quantifiers, refer to always and exist. OK, so informally speaking, we say a state satisfies the formula AF if aharm all parts start interesting that state, F holds. Similarly we say a state satisfies Ef, if there is some. So what is f now? It is there in this grammar. F is what we call a path formula, which is a formula that holds in a path starting from a certain state. It is built using these operators called temporal operators. This is the next time, eventually, global and until operators, and I don't want to dive into the formal semantics of CTL. What I want to do is just show you some simple examples of what it means for a structure to satisfy a CTL formulas with some simple examples using our computation tree from before.
    OK? So here's the first temporal logic formula, I want to show you which is AGq, which essentially holds in any state from which allow all parts that are starting at the state, q holds along every state in each part, OK? So this essentially means that q holds everywhere. And all the nodes in this computation tree that satisfy all the states in this computation tree that satisfy AGq are marked in yellow. So along each part, q holds everywhere? OK, that's AGq. Another formula is EFp, which holds in a state, if there exists a path start interesting that state along which p holds eventually, OK? We again remember that we are reasoning about infinite executions and that's why we can say things such as eventually.
    So these are all the states in this computation tree that satisfy EFp. Because from all of these states there exists a path to a state that satisfies p, either because the state already satisfies p, or there exists a transition from a state into a state satisfying p.
    Let's look at one final formula, which is slightly more complex, which is essentially a nesting of the previous two from those that we saw, which is EF AGq and r, along which you can reach a state from which AGq holds eventually. And this is a nesting and all of the states in this computation tree satisfy this formula. How do you see that? This is a little bit complex and in fact a modern checking procedure can really detect this automatically. Here is how what you do, you essentially look at the smallest subformulas and build up the set of state satisfying the formula. So you start with states that satisfy q and r, the simplest subformula in this formula, OK? These are all the states. And then you build a set of states that say AGq and r, which in this case happens to be the exact same set of states and then you can figure out the set of states that satisfy EF AGq and r. OK, so that's EF AGq and r, so that's all I have to say about temporal logic. I don't have time into the modeling procedure or also the CTL procedure in depth. So this is a decision procedure. Which means it gives a yes-or-no answer to the question. The question is is this CTL formula satisfiable? Well then the answers know if the formula is not satisfiable and yes if it is satisfiable, in which case it also returns a model or a structure that witnesses satisfaction of the formula, OK? So these are the basic steps of the CTL decision procedure, where the first step you construct which is called a tableau, which in some way encodes all the possible models of the CTL formula in question. OK? This is an initial tableau. Then in the next step, you check this tableau for inconsistencies and delete any inconsistent portions of the tableau. If you end up deleting the root node then you declare the formula to be unsatisfiable. Otherwise, you can abstract a model of the CTL formula from this tableau and return that the formula is satisfiable and return the model.
    OK? So here is what a tableau looks like. So this is a tableau for this CTL formula, EFp and EF not p, I'm not going to explain how to build this. The construction is very mechanical, but it's unfortunately out of scope for this talk. I need you to note two things: This tableau is essentially a directed graph consisting of what are called odd nodes and and nodes. And the semantics of the tableau is that every node when interpreted as a state in some suitable structure, like the final model, essentially satisfies all the formulas that are -- that the node is labeled with. OK? So these, for instance, these labels, these are the "and" node successors and they're all the ways in which this CTL formula can be satisfied, OK? But I want to leave it at that and move on to the next step, which is deleting inconsistent portions. This is just one node here which has an inconsistency. It requires satisfaction of both P and not P in the same node. So we delete that. Everything else is fine and can stay on. The root node is still hanging in there so we can abstract a model from this tableau and here is how you can construct a model. Well, you can start from any of the remaining and nodes. You can take your pick. You don't have a unique model. There can be several. Let's say we pick this one. In this node, p holds. So p is satisfied, and hence EFp is satisfied, but is it says we still have an obligation to satisfy not EFp in the next step. And it says follow the successors and obtain this node in which not p is satisfied.
    So we're making sure all the eventualities are fulfilled to capture the model.
    This is what the final model likes like. It's fairly simple, but there was a complex path to arriving at this model.
    So that is CTL will synthesis, but I was actually going to tell you about synchronization synthesis, so how do we go from here to synchronization synthesis.
    You use the previous tableau-based method to construct a global model of the concurrent program, and finally you factor this out into these completed synchronization skeletons where the foundations are labeled with guarded commands, OK? So it is really the ability to do synchronization synthesis and the ability to specific your entire logic in CTL.
    So that concludes my discussion of the first paper which really was remarkable in many ways, especially for its time, but it had some fallacies, which actually came to light later. In particular, this complete specification that I have been emphasizing on is a very strict requirement.
    It's not always possible to justify the need for writing a complete specification in all scenarios, for instance, a programmer may not be an expert in temporal logic, or despite his or her best efforts, a programmer may, due just to oversight skip describing the intended behavior in temporal logic. So this is a problem in general. Also, the framework only applies to finite state process, and it doesn't really address the interleaving explosion problem, because the tableau actually builds the entire global model which corresponds to the product graph of all the processes which is all the interleavings in the program.
    So let's see now if our next paper manages to address some of these issues.
    So this is a paper by Martin Vechev, Yahav, and Yorsh, the one-line summary of this paper is that it's an abstraction-based approach to infer synchronization to ensure safety properties of infinite state concurrent programs. So hopefully you've already noticed some differences from the previous paper. So here is a closer look. This addresses the restriction that the previous paper had about the systems being finite state and tackles infinite state programs. The specification is no longer a general temporal property, but is simply a safety property, and this really simplifies the process. Also the specification does not need to be complete. OK? Which is a win.
    The synchronization is now synthesized in the form of atomic sections. I guess you are familiar with atomic sections. And finally, the procedure which we look at is based on abstraction, refinement and counter-examples.
    So here is again the paper in action on an example from the paper, where you have a concurrent program, a multi-threaded program with the safety property expressed as an assertion within the program, OK? The synchronization synthesis procedure here apart from the program and the specification also takes in some abstract domains to help with the abstraction refinement and generates a concurrent program with atomic sections, so here is a representation of an atomic section which says that these two events in thread 2 have to execute atomically, OK? And this is again guaranteed to satisfy this assertion over here.
    So that's the snapshot. And again, this paper has contributions that go beyond just synchronization synthesis. For instance, abstraction-guided synthesis is an idea that I really like in this paper that applies to a much broader context, they have a great instantiation of it in this paper, of course, but it can apply to other domains, as well. This paper presents clever instantiations of two examples that are not necessarily new in this paper is that a counter-example guided synthesis or synthesis as repair and then quantitative synthesis where you also add a quantitative objective to help choose between multiple possible candidates for your synthesis, OK?
    You should keep an eye out for these concepts when I'm explaining this.
    So the main idea of this paper is that it combines two classical frameworks for verification and for repair or synthesis in a neat way.
    So here is a classical framework for verifying infinite-state programs, using what is called counter-example-guided abstraction refinement. So what's the basic idea here? Well, you take a program and a specification and some abstraction which results in a sound over approximation the program and try to verify this abstract program. If you succeed then you're done. Otherwise you get what's called an abstract counterexample which may or may not correspond to a concrete counterexample from the former program. In fact it may be because your abstract domain was really coarse and so what you do is refine the domain in order to eliminate this counterexample and repeat the process until your verification succeeds, OK? Now here is a somewhat analogous loop for doing ... where the goal is to modify an incorrect program or a partial program in a way such that is it ultimately satisfies a specification and you do this using counterexamples. You try to verify it, it fails. You get a counterexample, you modify the program, you abstract some constraints that correspond to the program's restriction that can eliminate the counterexample and repeat the process until your verification succeeds. Once it succeeds, you take all of these accumulated constraints and then implement them in the original program to get your final modified program.
    OK? So that's counterexample guided repair, and this is the combination of these two approaches that is presented in this paper, which is abstraction-guided -- sorry, counterexample -- how do I say this? This is abstract counterexample guided synthesis, OK?
    So here the goal is essentially to get to an abstract program that satisfies a specification. So what you do is when you get an abstract counterexample, you exercise one of two choices: You either refine the abstraction in order to eliminate the counterexample, or you modify the program in order to eliminate the counterexample, OK? So the advantage of this over the previous two independent techniques is you get more degrees of freedom. You could get stuck if you were just doing one of these loops, because you could reach a juncture where you cannot refine the abstraction anymore or you can reach a juncture where you could not restrict the program anymore and this just enables you to have more degrees of freedom and ultimately get into an abstract program that satisfies a specification, OK? So this is the cool idea in this paper. I think my slides are running amok.
    OK. Let me slow down a bit there. So the framework here is a broad framework that has nothing do do with synchronization synthesis so far, the way it is instantiated for synchronization synthesis is the way these constraints are gathered. Everything else can be generic. This can take generic abstracts, generic techniques for doing refinement and so on, the way it does this restriction is essentially take an abstract counterexample and gather some atomicity restraint. So let's see how that might work. Here is a visualization of an abstract counterexample. Hopefully this is not too abstract. Here are two threads, OK? And these are the events in the first thread. These are the events in the second thread. Here is the interleaving between the events of the different threads. So there are three contact switches here, and this is an error state, OK, so this is an abstract counterexample. Now now, if you want to eliminate the counterexample and there are actually many ways to do this, but one way to do this is by essentially disallowing these three contextses which happen here? And you can disallow this by essentially enclosing the events between which a contact switch happens within an atomic section, OK? So here are three atomicity corresponding to the restraints that ensure that these two events introduce atomicity and so on for the others.
    So there are three essential contexts in which the interleaving the last one wasn't really matter and the overall aso this says that you can eliminate this counterexample by enforcing the first atomic section or the second one or the third one.
    Either -- I mean no matter what you do, you would have eliminated this particular interleaving of events.
    OK? In general, an atomicity constraint is actually a bunch of conjunctions over such disjunctions, because are accumulating these over multiple counterexamples, but let's leave it at that.
    So that's how you get an atomicity constraint, but perhaps you might notice that an atomic section is actually fairly conservative, so apart -- besides eliminating the counterexample that you're interested in eliminating, it also ends up eliminating many traces that are possibly correct, OK?
    So what this framework does, is once it has accumulated all the atomicity restraints that can ensure correctness, it poses a quantitative synthesis problem that tries to synthesize a number -- not a minimum number, but it tries to minimize the size of the atomic sections so that it permits as many behaviors as possible.
    So that's this abstraction-guided synthesis framework, and here is again the framework in action, now perhaps these abstract domains make more sense, so you need to supply the initial abstract domain and all the abstract domains you might want to go through in your refinement, as well.
    Again, this is a neat idea, but it has some shortcomings. It also actually does not do anything to tackle the interleaving explosion problem, OK? So it does handle infinite state concurrent programs, which is great. It does not require a complete specification, which is great, but it still does not really tackle the interleaving explosion problem. It uses abstract domains to actually target the state explosion problem. Your data might have an infinite domain and it enables you to tackle that, but it does not do anything to tackle the interleaving explosion problem. Also, even though someone still needs to write these assertions. Someone still needs to figure out what is the intended behavior of the program, and this can be challenging in some contexts.
    Finally, as we discussed, atomic sections are not very permissive. OK?
    So let's see now if the work that I did recently with my collaborators can address some of these issues. So this was work done with a great group of collaborators as part of my post doc, which I did with Tom Henzinger in Austria. Oh, you might notice also Ed Clarke who is one of the coauthors. So this is the same Ed Clarke in the first paper. So he has been doing synchronization synthesis from 1981 to 2016. this was actually part of a small series of papers that included a paper from 2016, as well, called succinct representation of concurrent trace sets and a subsequent follow-up journal paper published in formal methods in system design last year. So some of the ideas that I'm going to present are not just from the cap paper but from these two papers, as well.
    So here's the one-line summary of this paper. This is trace generalization based framework to infer synchronization for an implicit specification for infinite state concurrent programs, OK?
    So this is trying to tackle some of the problems of the previous papers. Let's take a closer look, so the processes on communication model are exactly the same as the previous paper. Infinite state, shared memory, interleaving-based. The specification now can definitely be a safety property, but we also have the ability to reason about a specific kind of implicit specification, which was suitable for our application domain of device drivers, where the specification is essentially based on behaviors that are possible under a friendly nonpreemptive scheduler.
    The synchronization here is a richer set, so we can infer now locks, barriers, wait-note if I synchronization primitives and our system is also based on counterexamples, but it also generates counterexamples, which is a key to tackling interleaving explosion.
    So OK. I expected something to show up here. Let me just take a quick look. Hm. I will skip that. I was actually going to show you a preview of our technique, but that's OK. So there are broader contributions in our paper, as well, this notion of genderrerrizing counterexamples is generally application. This notion of doing specification-free synthesis, where we do not require a user to specify properties and finally this also included a verification based on language inclusion which can really be used independently. So I'm sorry that I'm missing that slide, but let me give you a hint of why I need a language inclusion verification. What we are trying to do in this specification ... ... preempt the program at yield statements or when a thread completes execution, OK? So then the programmer makes sure that everything is working fine in this setting, and then the property of -- the implicit specification tries to be sure that any arresting behavior that is possible, based on a preemptive prescheduler is contained in the set of behaviors that have been ensured by the programmer to be correct under the friendly scheduler, OK? So there is a language inclusion problem here where you want to ensure that the set of behaviors possible is included in the set of behaviors possible under a friendly scheduler. This is a much more challenging verification task than the usual reasonable task.
    So this is something you are already familiar with. This is the counterexample-guided repair and synthesis framework, now I want to show you another way to do this. So notice that in this one, this is sort of a lazy insertion of program modifications. You are accumulating constraints corresponding to program modification but not really changing the program in each iteration and you're waiting until the very end to actually implement all these constraints and changes in the program. OK? So this is a lazy insertion of synchronization. Another way you could do this is by modifying the program eagerly in each iteration, OK? And then checking if the verification succeeds.
    So this is a simpler version of the counterexample guided loop and I'm just going to use this to explain some of the ideas.
    Now, if you stare at this for a bit, you will notice that this can be too slow. You are only eliminating one counterexample in each iteration, OK? This can quickly become intractable. There can be really exponentially many counterexamples. So our solution is to introduce a way to accelerate this iterative process by in each step generalizing a counterexample to many other related counterexamples, and then eliminating all of these in one fell swoop in each iteration, OK? And then in the next iteration, we look for a new unrelated counterexample.
    So you don't just eliminate one counterexample at a time, you eliminate a generalization of the counterexample.
    So how do we do this? Let's first look at this generalization procedure. I mentioned related traces. So what do I mean by that? Recall that an interleaving or a trace is essentially a sequence of events from different threads. And I say a trace is related to a given trace if -- one is obtained by the another by a reordering of events from different threads. OK, so the set of events stays the same.
    So given a trace, we want to -- we seek to essentially generalize it to obtain all other incorrect traces over the same set of events that can be obtained by some reordering, OK? But this can be a large set of related counterexamples. So the key challenge is to actually obtain a way to succinctly represent this entire set of related counterexamples, so that then you can manipulate this in various ways, you can use it for synchronization synths sis, bug summerrization, and so on. So our main goal here was to come up with a representation which is called happens before formula for capturing this entire set of counterexamples that are relate today a trace. This is essentially a boolean combination of happens before operations. And I'll show you what I mean with an example.
    So here is I think the final example we will see in today's talk. So this is a simple banking example with an initialization thread that initializes the account balance to some value in it and the pink thread here and the blue thread here are essentially just incrementing the account balance by making some deposits of 10 and 20, using the local variables, B1 and B2 essentially. The new value of the balance must equal the initial value of the balance plus the sum of two deposits. Now, clearly this is not correct. So here is one counterexample, one trace which exhibits this failure. This is where essentially the event B1 of the blue thread happens before the initialization event which then happens before B2 and so on, OK? So the HB form La, the happens before formula is essentially a conjunction of ordering constraints between events in different threads. So this says B1 happens before C1, B2 happens before C1 and ... In the same thread implicit, because this is always fixed. OK, so this is an exact representation of this trace. Now what our trace generalization technique has the capability to do is take this trace and then generalize it to a very succinct happens before formula, which captures the essence of why this counter example was incorrect. Which is simply that this event B1 happened before the initialization event. Now, notice that this doesn't correspond to a single counterexample anymore. It corresponds to a whole bunch of counterexamples or interleaving which satisfy this ordering constraint. So I won't have time to tell you how we do the trace generalization. Let me just mention that we make sure we preserve the data flow into the editor and we use some minimal computations to arrive at this trace generalization. So so going back to our example, once we have run through all the iterations of the loop and we are able to succeed with the verification, this is the final happens before formula that we obtain which essentially captures, a different mode of failure of the concurrent program. So these are two symmetric ordering violations and this one is essentially a race condition or more broadly speaking an atomicity violation. So these happens before formulas have many nice properties. The most important one is that it captures all incorrect related traces in the sense that I said traces were related over the same set of events, and it does not include any correct related traces. OK? So this is a great succinct summarization of the error in a counterexample and in our examples we found that in most cases these formulas were very small. So that was trace generalization. Let's look at this step, which is how do we go from a happens before formula to inferring synchronization?
    OK. So the main idea here is to essentially identify patterns of happens before formulas that correspond to various synchronization primitives, formulate various rewrite rules that capture these patterns and then syntactically apply these rewrite rules to transform these patterns into synchronization primitives, OK? So let me just show you one example rewrite rule, which is used to infer locks. So here is a cross-pattern between events in different threads, and the rewrite rules say that if you see this pattern, then you rewrite it into a lock. Now, to get some insight on how we infer this, just note that this represents bad traces. To eliminate them, we can flip these ordering constraints and get this, which essentially says that either Bk has to happen before A1 or Ak has to happen before B1, that's what you get from flipping which is essentially the mutual exclusion that can be enforced by these locks, so we have a bunch of other read-write rules and we are able to use this to infer synchronization for this original program, and notice that this is fairly nontrivial synchronization, and it's amazing that we are able to do a simple syntactic inference of synchronization for this program once we have a happens before formula.
    I won't go into the verification technique, which is something that we developed for handling implicit specifications. The rest of the stuff that I explain works with either an implicit specification or a safety property. So that wraps up my discussion of these papers.
    I cleverly managed to skip the drawbacks of my own paper.
    [laughter]
    
    So it has drawback, it shares several drawbacks with other papers, but there are some that are its own, which are that you cannot always find an implicit specification, we were able to find one in our domain, which is great, but this is not universally. It's very computationally expensive. So with that, let me just wrap up by saying that we have come a long way in the area of concurrent programming. We can handle diverse specifications from temporal logic to implicit specification, we can infer diverse synchronization primitives, we can do some kind of performance aware quantitative synthesis and we have definitely pushed the boundaries of scalability, OK, but there is a lot are more to do, which is great, because that means that there are several more Ph.D. theses to be written up.
    [laughter]
    
    So most of the work I mentioned really many focused on assumes sequential consistency. There is work to be done in that space. The program models, the performance models are fairly simply particular. No recursion, no aliasing: This has to be improved. There is no optimistic concurrency control. No matter what I said about scalability, it still remains a challenge despite our best efforts and finally all of these programs actually assume a fixed number of threads. And I want to mention this challenge, because in my last slide, I'll just leave it there. We are trying to target this problem of removing this restriction of having a fixed number of threads in a distributed setting, so we are trying to synthesize coordination for distributed systems, such that the synthesized system is correct for an arbitrary unbounded number of processes in the system. With that I'll conclude.
    [applause]
    
    
    >> Thank you, Roopsha, that was awesome. Thank you. We'll start back at 11:05 with Frank Pfenning. A couple of notes I'll do this now, that management of Papers We Love has told me. We have a lot of seats up front. We're a sold-out show, so you can make friends and take up the seats and up front is the best place to be for talks, I think, and the other thing is that for anyone wanting to do a Papers We Love chapter locally and you want to talk to us, please talk to organizers with the road crew shirts on. Thanks. See you in a few!
    [break]
