1
00:00:05,549 --> 00:00:10,370
Today's talk is going to be a talk that I
haven't done before with parts of the talk

2
00:00:10,370 --> 00:00:13,230
that I'd actually promised to do for the conference.

3
00:00:13,230 --> 00:00:14,230
If I can get there.

4
00:00:14,230 --> 00:00:15,850
We'll see how far we get in the slides.

5
00:00:15,850 --> 00:00:18,590
Let me just find that PDF.

6
00:00:18,590 --> 00:00:21,580
Where is it?

7
00:00:21,580 --> 00:00:25,570
Oh, there it is.

8
00:00:25,570 --> 00:00:27,080
Generated 15 minutes ago.

9
00:00:27,080 --> 00:00:29,439
[laughter]
All right.

10
00:00:29,439 --> 00:00:40,210
So Zeeshan reached out to me because I had
referenced a talk at a conference I gave called

11
00:00:40,210 --> 00:00:42,110
Eyeo.

12
00:00:42,110 --> 00:00:48,820
Largely people who are working in the design
fields, along with artists, working with code

13
00:00:48,820 --> 00:00:56,809
and data as creative materials, so I'm going
to do a talked that's related to that.

14
00:00:56,809 --> 00:01:00,609
But I went down a social justice rabbit hole
last night, so there's going to be loads of

15
00:01:00,609 --> 00:01:04,970
stuff that I've been thinking about for a
long time but haven't necessarily articulated

16
00:01:04,970 --> 00:01:08,640
in a complete talk but it has plenty of examples.

17
00:01:08,640 --> 00:01:14,441
So we also agreed that I would do this bit
that I call shock and awe, which I'll explain

18
00:01:14,441 --> 00:01:21,610
in a moment, which is essentially all of my
work for 18 years in about four minutes.

19
00:01:21,610 --> 00:01:24,790
[laughter]
No, I'm not kidding.

20
00:01:24,790 --> 00:01:32,600
So the title for this talk, in case you weren't
aware of it, comes from a piece by Jenny Holzer

21
00:01:32,600 --> 00:01:38,360
called truisms, which are these series of
texts or things that sound like commercial

22
00:01:38,360 --> 00:01:43,040
slogans, almost, but they're kind of hard-hitting
and quite deep.

23
00:01:43,040 --> 00:01:49,210
You know, the idea of abuse of power comes
as no surprise, so you know, paraphrasing

24
00:01:49,210 --> 00:01:54,210
that, I call this talk abuse of an algorithm
comes as no surprise and I think most of you

25
00:01:54,210 --> 00:01:55,830
can probably agree with me.

26
00:01:55,830 --> 00:02:02,470
So this is the one from the same series, I
just like this one a lot, because cruelty

27
00:02:02,470 --> 00:02:05,700
is always possible later.

28
00:02:05,700 --> 00:02:11,560
And finally, if you aren't political, then
your personal life should be exemplary.

29
00:02:11,560 --> 00:02:16,440
You know, in these election times, I don't
know if being political is quite the sort

30
00:02:16,440 --> 00:02:17,450
of copout.

31
00:02:17,450 --> 00:02:23,230
So my name is Marius, I'm from Norway, I never
went to art school, but I'm an artist.

32
00:02:23,230 --> 00:02:29,630
I never completed a degree, but I teach people
at master's and Ph.D. level sometimes and

33
00:02:29,630 --> 00:02:36,140
I started coding on TRS-80 color computer
back in the '80s, but then I got to computer

34
00:02:36,140 --> 00:02:40,200
science which I always thought this is what
I'm going to do and I realized at least computer

35
00:02:40,200 --> 00:02:45,540
science, at least the way it was taught in
the early to mid '90s is the most boring thing

36
00:02:45,540 --> 00:02:48,530
I could do with a computer.

37
00:02:48,530 --> 00:02:53,500
Traveling salesman has its place, but it didn't
connect with me.

38
00:02:53,500 --> 00:02:55,790
I hope you don't find that statement offensive.

39
00:02:55,790 --> 00:03:02,620
It's obviously not that I think computer science
has no potential, it's just more the things

40
00:03:02,620 --> 00:03:06,840
that I was interested really in doing it with
just wasn't being taught.

41
00:03:06,840 --> 00:03:10,540
So I dropped out of computer science and started
working with graphic designers, trying to

42
00:03:10,540 --> 00:03:17,709
apply ideas from code to design problems and
to artistic problems, which meant that in

43
00:03:17,709 --> 00:03:22,770
'94, there was nowhere you could go for that,
so I started designing rave posters, doing

44
00:03:22,770 --> 00:03:29,100
these crazy 3D rendered things that were all
generated with code, so that means that you

45
00:03:29,100 --> 00:03:35,540
know, 20 years later I find myself as an autodidact
coming from a quite sort of mutant hybrid

46
00:03:35,540 --> 00:03:36,970
practice.

47
00:03:36,970 --> 00:03:39,099
So today I do all kinds of things.

48
00:03:39,099 --> 00:03:44,440
I'm an artist, I'm a creative technologist,
you can hire me, I call myself an aesthetist

49
00:03:44,440 --> 00:03:52,880
for hire and also I teach workshops and also
at proper educational institutions.

50
00:03:52,880 --> 00:03:59,709
That's me with my original color-basic instruction
book and if you look in the corner, you can

51
00:03:59,709 --> 00:04:02,840
see that's actually the TRS-80.

52
00:04:02,840 --> 00:04:06,120
I don't think it works anymore.

53
00:04:06,120 --> 00:04:08,020
And my work looks like this.

54
00:04:08,020 --> 00:04:13,760
This is basically what you'll get if you Google
image search me and to give you the complete

55
00:04:13,760 --> 00:04:19,720
overview, I will open processing.

56
00:04:19,720 --> 00:04:23,009
And do -- show all of it.

57
00:04:23,009 --> 00:04:24,009
Let's see.

58
00:04:24,009 --> 00:04:26,139
I need 3.

59
00:04:26,139 --> 00:04:28,830
Does everyone know Processing?

60
00:04:28,830 --> 00:04:29,830
Yeah?

61
00:04:29,830 --> 00:04:36,849
So Processing is the kind of tool which might
be a little superfluous to requirements for

62
00:04:36,849 --> 00:04:42,520
proper coders in quotation marks, but it takes
away all the tedium of creating projects and

63
00:04:42,520 --> 00:04:44,110
setting up Java, etc.

64
00:04:44,110 --> 00:04:48,740
It's essentially a Java framework for creative
coding.

65
00:04:48,740 --> 00:04:50,309
And it works quite well.

66
00:04:50,309 --> 00:04:57,120
So this piece is, let me just check my display
settings, because this piece was created a

67
00:04:57,120 --> 00:05:06,659
few years ago, so this was like 1024X768,
so I've got to optimize a little bit.

68
00:05:06,660 --> 00:05:14,190
Essentially I collected all the images I could
find of my work and made a playback system

69
00:05:14,190 --> 00:05:20,419
that plays is back in -- let me see what's
going on here.

70
00:05:20,419 --> 00:05:24,779
Oh, that's the wrong -- this is the wrong
version.

71
00:05:24,779 --> 00:05:28,460
I fixed that yesterday.

72
00:05:28,460 --> 00:05:35,199
But now it's going to be full screen.

73
00:05:35,199 --> 00:05:38,070
Where is it?

74
00:05:38,070 --> 00:05:42,879
That should be the one.

75
00:05:42,879 --> 00:05:46,680
This would be much cooler at a conference
where you don't know code.

76
00:05:46,680 --> 00:05:54,539
[laughter]
(Chuckling) they would be impressed.

77
00:05:54,539 --> 00:05:55,800
Here it's not so much.

78
00:05:55,800 --> 00:05:58,279
Let's see if we can run this.

79
00:05:58,400 --> 00:06:04,800
So if we can get the lights, and and turn
up the volume

80
00:06:08,640 --> 00:07:26,289
[Electronic music]

81
00:07:26,289 --> 00:08:25,169
Booming booming booming booming booming booming].

82
00:08:25,169 --> 00:10:32,529
[Electronic Music]

83
00:10:33,600 --> 00:10:39,200
[cheers]
[applause]

84
00:10:39,600 --> 00:10:48,960
All right, so that was 919 slides if you
watched the meter, and of course it accelerates.

85
00:10:49,000 --> 00:10:54,560
All of that is from '94.

86
00:10:54,560 --> 00:11:04,680
So I have to give credit to Golan Levin, one
of the people who's supported open frameworks

87
00:11:04,680 --> 00:11:05,680
into existence.

88
00:11:05,680 --> 00:11:09,250
He's also been involved in processing, but
he suggested to me, because he knew that I

89
00:11:09,250 --> 00:11:14,660
had thousands of images just lying around,
why don't I do an interactive presentation

90
00:11:14,660 --> 00:11:20,329
where I just skip through all my slides and
have people say wait, wait, wait.

91
00:11:20,329 --> 00:11:22,250
But he trusts audiences way more than I do.

92
00:11:22,250 --> 00:11:26,079
So I did the noninteractive version.

93
00:11:26,080 --> 00:11:31,980
And now for something slightly less pleasant,
but way more important.

94
00:11:31,980 --> 00:11:39,100
>> So when I talk about political issues to
people who are not necessarily interested

95
00:11:39,100 --> 00:11:43,392
-- it's not that the audience isn't interested,
it's more that but this isn't what I do, why

96
00:11:43,392 --> 00:11:45,390
do I have to listen to this?

97
00:11:45,390 --> 00:11:48,569
I feel compelled to include a lot of disclaimers.

98
00:11:48,569 --> 00:11:52,670
Some of the ideas I'm going to talk about
might seem controversial.

99
00:11:52,670 --> 00:12:01,939
I don't think they are, but the basic argument
is technology is p not a neutral thing.

100
00:12:01,939 --> 00:12:12,700
Technology is never a neural a priori innocent
object, because technology is always applied

101
00:12:12,700 --> 00:12:19,240
for the users and the people who create tools,
hence whether you -- you created an infrared

102
00:12:19,240 --> 00:12:25,180
sensor or a machine learning algorithm, there
are implications of these tools that you cannot

103
00:12:25,180 --> 00:12:26,180
disavow.

104
00:12:26,180 --> 00:12:27,220
You got that?

105
00:12:27,220 --> 00:12:28,220
(Chuckling).

106
00:12:28,220 --> 00:12:32,440
Is that a fair statement so far?

107
00:12:32,440 --> 00:12:38,360
Because I think as coders, it's very easy
to become seduced by this idea of a perfect

108
00:12:38,360 --> 00:12:43,829
world of algorithms, which is kind of like
the way that economists become seduced by

109
00:12:43,829 --> 00:12:48,420
the perfect world of the market, you know,
the invisible hand, the banks will regulate

110
00:12:48,420 --> 00:12:51,610
themselves, it's going to be great.

111
00:12:51,610 --> 00:12:56,339
Which turns out to not be true and when you
combine banks and algorithms, you get some

112
00:12:56,339 --> 00:13:00,589
really profound fuckery going on.

113
00:13:00,589 --> 00:13:02,329
Because coders are people, right?

114
00:13:02,329 --> 00:13:07,949
I mean who in here can say that like I've
never written a flawed and stupid project

115
00:13:07,949 --> 00:13:09,639
in my life, right?

116
00:13:09,639 --> 00:13:12,519
Like it's just a thing.

117
00:13:12,519 --> 00:13:16,579
When you think about sort of the real-time
mission critical stuff, like you know, things

118
00:13:16,579 --> 00:13:21,509
that have to control space stations, that
stuff becomes terrifying and in fact, you

119
00:13:21,509 --> 00:13:25,990
know, how many space missiles have they lost
because of you know, flotation mark errors?

120
00:13:25,990 --> 00:13:28,579
Like, it happens.

121
00:13:28,579 --> 00:13:35,000
So the goal in putting out the following sort
of concerns more than conclusions.

122
00:13:35,000 --> 00:13:39,800
Is that I'm interested in not in political
correctness, because that's kind of bullshit,

123
00:13:39,800 --> 00:13:50,910
because it's just a pseudonym for decency,
when you consider

124
00:13:50,910 --> 00:14:01,660
IBM's Watson, where he's moved up from playing
Jeopardy to figuring out your medical charts

125
00:14:01,660 --> 00:14:06,800
and who knows, maybe Watson is excellent at
that, but all these trends that we're seeing

126
00:14:06,800 --> 00:14:11,860
right now are deeply linked to understanding
and facilitating human experiences.

127
00:14:11,860 --> 00:14:18,339
That means that you're moving into a whole
other area than optimizing traveling salesmen,

128
00:14:18,339 --> 00:14:20,990
you know?

129
00:14:20,990 --> 00:14:25,880
But the corollary is also that even the most
unassuming software developer has the power,

130
00:14:25,880 --> 00:14:29,040
particularly in this day and age, to affect
society.

131
00:14:29,040 --> 00:14:37,449
If Uber is able to disrupt labor politics
primarily through an API, and a set of protein

132
00:14:37,449 --> 00:14:44,120
columns, and Airbnb is able to pretty much
undermine urban planning, why shouldn't your

133
00:14:44,120 --> 00:14:54,139
apps and APIs be able to effect change in
a positive way?

134
00:14:54,139 --> 00:14:58,390
Before I begin again, you know, I'm a white
guy.

135
00:14:58,390 --> 00:15:07,709
I'm a Caucasian male, born in Norway provided
with a safety network just by default.

136
00:15:07,709 --> 00:15:11,399
In Norway, we don't believe in people living
under bridges.

137
00:15:11,399 --> 00:15:14,749
You're not going to drop out of society just
because you've got a health problem.

138
00:15:14,749 --> 00:15:20,480
And I'm not trying to say anything about America
like based on that...

139
00:15:20,480 --> 00:15:24,380
[laughter]
...but let me just say that I'm aware of the

140
00:15:24,380 --> 00:15:25,509
privilege that that comes with.

141
00:15:25,509 --> 00:15:31,399
Also, most of the issues that I'm talking
about here, you know, the overlived reality

142
00:15:31,399 --> 00:15:36,860
that I don't share, so I don't presume to
talk for anyone else, but I can talk about

143
00:15:36,860 --> 00:15:43,040
it from the perspective of being a developer
who hopes that the tools I create won't impact

144
00:15:43,040 --> 00:15:45,339
other people negatively.

145
00:15:45,339 --> 00:15:54,259
So this is an email from Dave Schroeder from
the Eyeo Festival, and it's not an exciting

146
00:15:54,259 --> 00:16:01,490
email, but the first is all about content
and in that email he stipulates you can swear

147
00:16:01,490 --> 00:16:08,399
on stage, but you cannot denigrate the experience
of someone who is other than you.

148
00:16:08,399 --> 00:16:13,509
You know, or even, you know, it could be another
white male, you just, you know, be a decent

149
00:16:13,509 --> 00:16:20,860
human being and consider the political implications
of being onstage.

150
00:16:20,860 --> 00:16:22,680
I don't get these emails a lot.

151
00:16:22,680 --> 00:16:27,800
I know that Papers We Love have a code of
conduct, right, which is something that came

152
00:16:27,800 --> 00:16:33,639
out of our conferences not having them and
failing miserably at addressing harassment

153
00:16:33,639 --> 00:16:37,370
and abuse happening at conferences, because
it happens.

154
00:16:37,370 --> 00:16:42,279
So if we're awesome, why does the word brogrammer
even exist?

155
00:16:42,279 --> 00:16:48,130
When I heard that word, I was so upset.

156
00:16:48,130 --> 00:16:50,519
Do you want to identify with that word?

157
00:16:50,519 --> 00:16:59,089
It's a word that separates people in different
groups and reinforces the power of one.

158
00:16:59,089 --> 00:17:04,830
If we are all you know, potentially geniuses,
why haven't we been able to get rid of brogrammer?

159
00:17:04,830 --> 00:17:08,689
I mean it's such a stupid word, too.

160
00:17:08,689 --> 00:17:16,240
So you know, the fact that people are smart
does not mean they're automatically good.

161
00:17:16,240 --> 00:17:21,370
That's been pretty well proven through history
and I'm not saying that the present audience

162
00:17:21,370 --> 00:17:27,170
is racist or sexist, in fact I have a pretty
good idea that you're very unlike to be, but

163
00:17:27,170 --> 00:17:32,440
stupid shit does go on and sometimes good
friends of yours surprise you negatively.

164
00:17:32,440 --> 00:17:37,520
So the geek feminism wiki has a lot of these.

165
00:17:37,520 --> 00:17:47,210
One of them I think was the fork joke at the
Python Conf, like I don't remember when.

166
00:17:47,210 --> 00:17:50,620
But if you look at this, and it starts in1973.

167
00:17:50,620 --> 00:17:57,580
Does everybody know the lemma image?

168
00:17:57,580 --> 00:18:03,810
The sort of canonical image, the same way
that the Utah teapot is the Te test measurement

169
00:18:03,810 --> 00:18:05,650
for three meshes.

170
00:18:05,650 --> 00:18:12,320
It's a Playboy centerfold, you know, that's
fairly implicitly sexist right there.

171
00:18:12,320 --> 00:18:17,390
The image is not offensive itself, but it
says something from the cultural context it

172
00:18:17,390 --> 00:18:23,420
comes from and it also goes on and updates
it and it's quite depressing to read, but

173
00:18:23,420 --> 00:18:28,110
the fact that this exists means that we can
do better.

174
00:18:28,110 --> 00:18:37,920
So Moritz Stefaner, he's a speaker at conferences
regularly so he created a data visualization

175
00:18:37,920 --> 00:18:42,650
to look at him or the group that he belongs
to which happens to be the group that I also

176
00:18:42,650 --> 00:18:43,910
belong to.

177
00:18:43,910 --> 00:18:49,330
And figure out you know, what is the gender
balance at these conferences and this is where

178
00:18:49,330 --> 00:18:59,490
transparency is actually based on databases,
if we have a dataset that represents participants

179
00:18:59,490 --> 00:19:05,390
and winners of awards, then we might actually
be able to look at what are the facts in our

180
00:19:05,390 --> 00:19:11,170
own community, and if we can't do better,
then how can we expect anyone else to?

181
00:19:11,170 --> 00:19:16,650
This is quite funny article that came out
which just, you know, purely using the, you

182
00:19:16,650 --> 00:19:21,911
know, mathematical facts, it proves that the
idea that you would have an all-male panel

183
00:19:21,911 --> 00:19:28,550
at a mathematics conference is just, you know,
astronomically improbable, and this is one

184
00:19:28,550 --> 00:19:34,210
area where I think that like these tools which
technically, like you know historically have

185
00:19:34,210 --> 00:19:38,350
served sort of the status quo and the technocrats.

186
00:19:38,350 --> 00:19:44,170
What if you turned that data back on them,
well, that's really unlikely that you would

187
00:19:44,170 --> 00:19:46,400
have an all-male panel.

188
00:19:46,400 --> 00:19:51,600
I've sat on all-male panels and I've vowed
to not do it again because it's pretty uncomfortable.

189
00:19:51,600 --> 00:20:07,120
Here's another good piece by Aandand Prasad,
it stipulates that in a random selection of

190
00:20:07,120 --> 00:20:16,030
20 people, women represent 20% of speakers
these are the more likely distributions that

191
00:20:16,030 --> 00:20:17,030
you would get.

192
00:20:17,030 --> 00:20:30,410
In fact, it would be more statistically so
any time you go to an architecture conference,

193
00:20:30,410 --> 00:20:35,950
and there are no women on stage, you know
there's something that prevents them from

194
00:20:35,950 --> 00:20:37,170
being there.

195
00:20:37,170 --> 00:20:43,530
So that's cultural buy y that's one of those
big bugbears that we can't agree on and we

196
00:20:43,530 --> 00:20:48,990
all agree that it sounds like a good idea
but I don't even know how do I invite more

197
00:20:48,990 --> 00:20:52,100
women or people of color etc. if I were organizing
a conference?

198
00:20:52,100 --> 00:20:56,050
What if I don't know them so it's a very complex
issue.

199
00:20:56,050 --> 00:21:05,870
But there is a simpler issue which is what
happens when technology becomes the bias?

200
00:21:05,870 --> 00:21:10,570
When the bias is actually coded into the technology?

201
00:21:10,570 --> 00:21:15,650
Which I think, you know, that's one of those
similarly to that statistics exercise, it

202
00:21:15,650 --> 00:21:23,050
becomes hard to argue that you know, like
a facial recognition software that can't identify

203
00:21:23,050 --> 00:21:28,390
non-Caucasians is not hard to say that that's
biased and coded in algorithm.

204
00:21:28,390 --> 00:21:30,390
So can an algorithm be racist?

205
00:21:30,390 --> 00:21:31,390
Yeah.

206
00:21:31,390 --> 00:21:33,070
Take a wild guess.

207
00:21:33,070 --> 00:21:35,030
Why should it be any different?

208
00:21:35,030 --> 00:21:38,920
However, you can say that like, oh, but software
developers can't be expected to deal with

209
00:21:38,920 --> 00:21:45,390
these issues because they're so deeply ingrained
in like social and historical factors, but

210
00:21:45,390 --> 00:21:56,510
we adhere to principles of connect accessibility
on a routine basis.

211
00:21:56,510 --> 00:22:01,910
The first time that I really came across a
story that sort of involved this was 2009.

212
00:22:01,910 --> 00:22:07,290
HP released some laptops with webcams that
had face-tracking, you know, an algorithm

213
00:22:07,290 --> 00:22:15,890
that we now take for granted, every camera
has it, but two coworkers tried it out, and

214
00:22:15,890 --> 00:22:22,830
the black coworker realized that the image
tracking stopped working as soon as the white

215
00:22:22,830 --> 00:22:28,130
woman moved out of the picture and HP at the
time came out with the by-now-traditional

216
00:22:28,130 --> 00:22:34,550
statement like, we don't know what's going
on, it's an algorithm, we don't really know.

217
00:22:34,550 --> 00:22:39,200
And at the time, I remember like seeing it
and thinking like, well, I can see how that

218
00:22:39,200 --> 00:22:40,760
would happen.

219
00:22:40,760 --> 00:22:46,130
I don't see how, like HP could like support
it continuing to happen, but it sort of, you

220
00:22:46,130 --> 00:22:49,080
know, early days of technology or whatever.

221
00:22:49,080 --> 00:22:54,350
You know, in terms of like the offensiveness
rank, it's like, yeah, that's really not great,

222
00:22:54,350 --> 00:23:00,260
but no one's going to die from not being tracked
on this webcam on a laptop.

223
00:23:00,260 --> 00:23:03,400
But then I read this article, which is an
amazing article that I recommend everyone

224
00:23:03,400 --> 00:23:04,400
to read.

225
00:23:04,400 --> 00:23:10,610
I will publish this PDF with these links as
hyperlinks, and this is a link -- this is

226
00:23:10,610 --> 00:23:18,790
a story by Syreeta McFadden about white balance
and her roam of self-perception and also like

227
00:23:18,790 --> 00:23:26,350
her family memories, because white balance
was developed to be calibrated to be like

228
00:23:26,350 --> 00:23:27,350
guess who?

229
00:23:27,350 --> 00:23:32,680
Yeah, people who look like me, which means
they did a get job of representing people

230
00:23:32,680 --> 00:23:39,740
like me, but they didn't do a good job of
people with color and so the white balance

231
00:23:39,740 --> 00:23:44,471
was thrown out, and reading just these paragraphs
is kind of heartbreaking because the idea

232
00:23:44,471 --> 00:23:51,760
that you learn at an early age that you cannot
be represented in a photograph, that's some

233
00:23:51,760 --> 00:23:52,880
bad technology.

234
00:23:52,880 --> 00:23:55,010
And some bad cultural impacts.

235
00:23:55,010 --> 00:23:59,360
It's a fantastic article, I recommend reading
it.

236
00:23:59,360 --> 00:24:03,880
But then you look at these things that color
balance was based on and you start realizing

237
00:24:03,880 --> 00:24:05,980
why something like this comes about.

238
00:24:05,980 --> 00:24:09,740
I love sort of the basic look and the kitsch
of these.

239
00:24:09,740 --> 00:24:11,640
These are generally called Shirley cards.

240
00:24:11,640 --> 00:24:13,920
It was Canon who came out with them.

241
00:24:13,920 --> 00:24:19,630
And I think the one on the left is actually
an composition of several decades, I'm assuming

242
00:24:19,630 --> 00:24:25,610
the bottom right is the '80, it looks very
plausible to me.

243
00:24:25,610 --> 00:24:31,820
And the one on the right, while, you know,
just exciting in its own right.

244
00:24:31,820 --> 00:24:33,780
[laughter]
Is obviously excellent for color calibration,

245
00:24:33,780 --> 00:24:37,780
because it has all of them.

246
00:24:37,780 --> 00:24:43,020
So just the fact that the word normal is implied
there, it implies a normal.

247
00:24:43,020 --> 00:24:46,800
There is a technological normal when it comes
to photo reproduction.

248
00:24:46,800 --> 00:24:51,090
Now, I'm not saying that the people who developed
the chemical processes involved in making

249
00:24:51,090 --> 00:24:56,600
film were like, dyed in the wood racists and
wanted to prevent people of color to be represented

250
00:24:56,600 --> 00:24:58,360
well in photography.

251
00:24:58,360 --> 00:24:59,530
Not at all.

252
00:24:59,530 --> 00:25:01,000
That seems unlikely.

253
00:25:01,000 --> 00:25:06,300
Most of these examples are very unlikely to
be you know, the result of like a conscious

254
00:25:06,300 --> 00:25:11,600
bias, however there's a cognitive bias and
basically like you -- you have your known

255
00:25:11,600 --> 00:25:17,090
unknowns and you have your unknown unknowns,
and that's where bias hides.

256
00:25:17,090 --> 00:25:20,600
Your known unknowns might be if you're organizing
a conference and you know you should invite

257
00:25:20,600 --> 00:25:29,930
more women, but you don't know any more women
to invite, that's your known unknown.

258
00:25:29,930 --> 00:25:35,620
So this is a slightly more upgraded one which
reminds me slightly of the announcement of

259
00:25:35,620 --> 00:25:41,730
this series of underwear where nude is a color
for everyone.

260
00:25:41,730 --> 00:25:46,080
[laughter]
So this is a project by a student of mine,

261
00:25:46,080 --> 00:25:56,850
Greg DorsaInville and he did a project where
he was able to use open CV to track basically

262
00:25:56,850 --> 00:26:03,840
face in prime time television and then he
was able to do an indexing to find out what

263
00:26:03,840 --> 00:26:08,910
is the color representation or the ethnicity
representation in an evening of programming,

264
00:26:08,910 --> 00:26:11,840
so he did this sort of index-based thing.

265
00:26:11,840 --> 00:26:15,060
To me this is an excellent use of data.

266
00:26:15,060 --> 00:26:20,770
To use it to express something which is, you
know, of course an implied and experienced

267
00:26:20,770 --> 00:26:24,830
reality as a hard observable fact.

268
00:26:24,830 --> 00:26:28,240
That works.

269
00:26:28,240 --> 00:26:29,860
This is for light relief, actually.

270
00:26:29,860 --> 00:26:34,940
You might have seen this, this is Adam Harvey
with a project that essentially, it gives

271
00:26:34,940 --> 00:26:40,130
you instructions on how to avoid being recognized
by a facial recognition software.

272
00:26:40,130 --> 00:26:43,030
So it's the opposite problem.

273
00:26:43,030 --> 00:26:51,570
Essentially since open CV identifies certain
ridges and certain configurations of features.

274
00:26:51,570 --> 00:26:58,470
If you can demolish those features, remove
your eyebrows and make things sideways, it

275
00:26:58,470 --> 00:26:59,470
works.

276
00:26:59,470 --> 00:27:01,360
OK, this should not be here.

277
00:27:01,360 --> 00:27:08,620
I'll just turn off the internet or turn this
thing off.

278
00:27:08,620 --> 00:27:13,120
So it's the opposite problem and I'm sure
you've seen it referred to.

279
00:27:13,120 --> 00:27:18,060
For instance, I tweeted about this, which
is from elementary, you know, the Sherlock

280
00:27:18,060 --> 00:27:19,060
Holmes show.

281
00:27:19,060 --> 00:27:25,970
It's a really stupid implementation compared
to I think Adam's implementation is way more.

282
00:27:25,970 --> 00:27:30,690
This is like the worst punk you ever met.

283
00:27:30,690 --> 00:27:35,530
[laughter]
But he -- it's also included in Minority Report

284
00:27:35,530 --> 00:27:42,490
where one of the people has facial tattoos
to obscure facial recognition.

285
00:27:42,490 --> 00:27:50,850
It's rare, but it's happened a couple of times
where I know people's work had ended up on

286
00:27:50,850 --> 00:27:55,130
sciencey procedural shows.

287
00:27:55,130 --> 00:28:01,250
But if, you know, simple image processing
can be, you know, problematic in itself, mostly

288
00:28:01,250 --> 00:28:05,570
due to calibration errors, what about machine
learning?

289
00:28:05,570 --> 00:28:09,980
Machine learning just to me is just, you know,
like I have so many friends who are extremely

290
00:28:09,980 --> 00:28:17,340
excited about it, this entire new genres of
work popping up and yet we know, you know,

291
00:28:17,340 --> 00:28:23,460
they're fuzzy and they're prone to like error,
and you know, we like them because they have

292
00:28:23,460 --> 00:28:28,190
some reliability, but in the real world, if
you're going to make this into a product,

293
00:28:28,190 --> 00:28:31,440
then some of that just gets terrifying.

294
00:28:31,440 --> 00:28:37,190
Because -- and you know, an object recognizer,
which is hilarious in like misidentifying

295
00:28:37,190 --> 00:28:44,370
objects while you're debugging could misidentify
objects or people while not debugging and

296
00:28:44,370 --> 00:28:47,510
just, you know, have a huge impact on your
brand.

297
00:28:47,510 --> 00:28:51,350
If you're just an app developer, I mean if
you're Google, you can survive it but if you're

298
00:28:51,350 --> 00:28:56,060
like a single app developer, probably that
social media firestorm is something you couldn't

299
00:28:56,060 --> 00:28:57,410
survive.

300
00:28:57,410 --> 00:29:05,270
Because this is the Daily Mail, not my favorite
newspaper, had an article about Flickr introducing

301
00:29:05,270 --> 00:29:09,130
autotagging in a very rushed way around the
same time that Google Photo started doing

302
00:29:09,130 --> 00:29:16,500
it, and this is the gate to Dachau concentration
camp, which it has labeled "support architecture

303
00:29:16,500 --> 00:29:21,800
jungle gym," which of course as technologists,
we could all see, yeah, I could see how that

304
00:29:21,800 --> 00:29:29,110
makes sense, but as your grandma end user,
that's not going to fly.

305
00:29:29,110 --> 00:29:38,060
This one is not hard to catch I'm not saying
they're trivial.

306
00:29:38,060 --> 00:29:43,720
But this is some fucked-up shit.

307
00:29:43,720 --> 00:29:48,870
Finding a tag album in your Google photos
where you and your friend have been tagged

308
00:29:48,870 --> 00:29:53,230
as gorillas, that's not forgivable.

309
00:29:53,230 --> 00:29:54,230
You know.

310
00:29:54,230 --> 00:29:58,240
Those software engineers and those project
managers should get -- yeah.

311
00:29:58,240 --> 00:30:01,250
Hm, heads should roll.

312
00:30:01,250 --> 00:30:05,870
My favorite thing about this, not that there
are any favorite things about this, but the

313
00:30:05,870 --> 00:30:11,330
bottom is like, he says, what kind of sample
image data you collected that resulted in

314
00:30:11,330 --> 00:30:12,580
this, son?

315
00:30:12,580 --> 00:30:13,580
(Chuckling).

316
00:30:13,580 --> 00:30:16,820
Which, yeah, is exactly the problem.

317
00:30:16,820 --> 00:30:22,010
So but that's, you know, the -- again, like
still, this is simpler stuff, this is about

318
00:30:22,010 --> 00:30:28,150
like tagging images, sure it's hugely offensive,
but it's unlikely to ruin your life.

319
00:30:28,150 --> 00:30:32,350
This should have is likely to ruin someone's
life.

320
00:30:32,350 --> 00:30:37,660
This is an article from ProPublica, which
has been dealing a lot lately with the idea

321
00:30:37,660 --> 00:30:43,900
of bias in algorithms, particularly in machine
learning etc., and this about crime risk assessment

322
00:30:43,900 --> 00:30:51,030
software where the risk of someone repeat
offending is calculated, based on various

323
00:30:51,030 --> 00:30:55,870
factors, factors that we as the -- you know,
the public are a loud to know, because it's

324
00:30:55,870 --> 00:31:00,640
a proprietary commercial software.

325
00:31:00,640 --> 00:31:05,890
And they weigh different factors in different
ways and sure enough, it seems to have a predilection

326
00:31:05,890 --> 00:31:11,480
for identifying white offenders as unlikely
to reoffend and other, you know, people of

327
00:31:11,480 --> 00:31:14,400
other colors, likely to offend.

328
00:31:14,400 --> 00:31:17,140
This is, you know, this is real.

329
00:31:17,140 --> 00:31:22,100
The fact that this is out there, in courts
right now, the fact that people don't necessarily

330
00:31:22,100 --> 00:31:27,970
have the right to address, you know, what
-- how is that judgment being made, there's

331
00:31:27,970 --> 00:31:31,050
no good systems of recourse.

332
00:31:31,050 --> 00:31:34,020
I mean try critiquing an algorithm in a courtroom.

333
00:31:34,020 --> 00:31:36,720
We all know how that went for Apple and Google.

334
00:31:36,720 --> 00:31:44,380
If you follow the transcripts of Apple versus
Google trial, half of it was people explaining

335
00:31:44,380 --> 00:31:52,130
expression algorithms to lay audiences so
good luck trying to explain the machine learning

336
00:31:52,130 --> 00:31:56,700
system which we all believe in, has been trained
badly.

337
00:31:56,700 --> 00:32:03,150
What about the machine-learning algorithm
that the CIA, etc., like, uses to identify

338
00:32:03,150 --> 00:32:11,510
terrorists, when they admitted that we only
have training data based on 20 or 30 known

339
00:32:11,510 --> 00:32:20,600
terrorists, too and that thing is used to
inform drone strikes, that's some bad technology!

340
00:32:20,600 --> 00:32:24,830
So that's probably the biggest one of all.

341
00:32:24,830 --> 00:32:30,160
A more seemingly trivial one is the idea of
data collection.

342
00:32:30,160 --> 00:32:34,980
If you're writing a social media app, you're
going to write into it all kinds of ways of

343
00:32:34,980 --> 00:32:41,150
recording interactions and metadata, you know,
again, metadata is also a favorite of people

344
00:32:41,150 --> 00:32:46,590
trying to track terrorists and the reason
why is because metadata is extremely revealing.

345
00:32:46,590 --> 00:32:53,970
If I can see all the text messages you sent
to different people, I can tell who these

346
00:32:53,970 --> 00:32:59,430
people likely are in your life.

347
00:32:59,430 --> 00:33:04,860
If there's a gap in your message data like
one of my students had, then there's a reason

348
00:33:04,860 --> 00:33:16,360
why you weren't there, if you were in hospital,
or you were traveling, or as happened in this

349
00:33:16,360 --> 00:33:19,230
case, someone was stalking you.

350
00:33:19,230 --> 00:33:25,420
We accept that corporations have the right,
or the privilege to collect and correlate,

351
00:33:25,420 --> 00:33:30,640
whereas in Europe, there's actually very strict
restrictions on -- you might be able to collect

352
00:33:30,640 --> 00:33:34,490
this data over here, but you can't correlate
it to that over there.

353
00:33:34,490 --> 00:33:39,340
In America, how often do you hear of a corporation
being defeated from doing that?

354
00:33:39,340 --> 00:33:45,850
If you've ever passed a car with an automated
number plate reader, like on the street, then

355
00:33:45,850 --> 00:33:50,630
you know that that -- you know, that car is
just sucking down number plates that it sees

356
00:33:50,630 --> 00:33:51,630
it.

357
00:33:51,630 --> 00:33:57,010
Might do it directly for law enforcement like
they do in England or it might just be agglomerated

358
00:33:57,010 --> 00:34:02,980
into a distributed database using the idea
of distributed data collection, which they

359
00:34:02,980 --> 00:34:07,340
then can sell to whoever for whatever purpose.

360
00:34:07,340 --> 00:34:13,630
So not only is the issue of what data is collected
and is it controlled, but how is it collected?

361
00:34:13,630 --> 00:34:15,409
Do you collect more than you need?

362
00:34:15,409 --> 00:34:19,429
You know, how many times have you turned off
permissions for an app where like, I don't

363
00:34:19,429 --> 00:34:21,870
think you need my contacts.

364
00:34:21,870 --> 00:34:24,169
You're a picture-taking thing.

365
00:34:24,169 --> 00:34:26,360
Also, which fields are being collected?

366
00:34:26,360 --> 00:34:32,580
OK, thankfully we do have some restrictions
on that, like very few social messaging apps

367
00:34:32,580 --> 00:34:37,850
are going to ask you about your sexual preference,
unless it's Grindr.

368
00:34:37,850 --> 00:34:43,100
But you know, for the most part, we know that
some stuff that they're not supposed to do.

369
00:34:43,100 --> 00:34:49,310
But our tolerance level for like clicking
yes is remarkably high and on the developer

370
00:34:49,310 --> 00:34:53,830
side, who's going to tell the developer not
to collect more than you need?

371
00:34:53,830 --> 00:34:56,280
Which should be the principle, right?

372
00:34:56,280 --> 00:35:01,580
You know, collect what you need, don't collect
more than you do.

373
00:35:01,580 --> 00:35:06,120
Because down the road, you know, all the hacks
that we have, etc., people can start correlating

374
00:35:06,120 --> 00:35:11,330
data from multiple services, I mean let's
face it, no service is essentially unhackable.

375
00:35:11,330 --> 00:35:18,330
You know, they might be less likely to be,
but I think the interference of hackers from

376
00:35:18,330 --> 00:35:23,570
wherever in the current American election
is kind of showing us a new future in which,

377
00:35:23,570 --> 00:35:28,430
yeah, just assume that data that you are collecting
as part of your algorithm or your database

378
00:35:28,430 --> 00:35:33,410
might get out there and what might that mean
for the people whose data that is?

379
00:35:33,410 --> 00:35:40,190
And similarly, what in society, where data
is missing, you know, what could be the plausible

380
00:35:40,190 --> 00:35:44,670
reason for that when you know, the government
collects all kinds of other data, and of course

381
00:35:44,670 --> 00:35:51,240
I'm talking about the horrific recent streak
of -- well, this is not recent, it's the discussion

382
00:35:51,240 --> 00:35:54,250
about it is recent.

383
00:35:54,250 --> 00:36:01,140
You know, the impunity by which the police
is able to act violently and kill members

384
00:36:01,140 --> 00:36:08,850
of society, primarily African American, without
any real legal repercussions, and without

385
00:36:08,850 --> 00:36:13,180
that even being recorded by society.

386
00:36:13,180 --> 00:36:17,580
That absence of data isn't -- there's not
an accident.

387
00:36:17,580 --> 00:36:22,700
You know, if they collect all your tax records,
but they happen to not collect data on like

388
00:36:22,700 --> 00:36:29,240
potential ties between politicians and like
tax havens, like that wouldn't be a mistake.

389
00:36:29,240 --> 00:36:31,940
That would be by design.

390
00:36:31,940 --> 00:36:33,050
So there's multiple of these.

391
00:36:33,050 --> 00:36:35,580
This is the counted, by the guardian.

392
00:36:35,580 --> 00:36:40,970
Which again is ironic, which is a liberal
UK newspaper.

393
00:36:40,970 --> 00:36:41,970
They've done a pretty good job.

394
00:36:41,970 --> 00:36:48,990
They've had an open data record related to
data journalism which is exciting on its own.

395
00:36:48,990 --> 00:36:53,550
But there are also some others, fatal encounters
is probably the most important one.

396
00:36:53,550 --> 00:37:07,600
They started tracking around 2002 ... Ferguson,
which is not far from here, happened.

397
00:37:07,600 --> 00:37:19,780
Fatal encounters formed the kernel for the
data collection for the Washington Post.

398
00:37:19,780 --> 00:37:25,520
So if you're doing open data, your success
might be someone else's Pulitzer.

399
00:37:25,520 --> 00:37:29,520
That might taste a little funny.

400
00:37:29,520 --> 00:37:34,080
[laughter]
So the idea is that power comes with responsibility,

401
00:37:34,080 --> 00:37:39,710
which is something that we know and in general
we accept this, but I think when you're developing

402
00:37:39,710 --> 00:37:44,920
technology, the you know for the same reason
why drone pilots and fighter pilots in general

403
00:37:44,920 --> 00:37:51,810
have like an easier time killing whoever,
than someone who has to do with it with bayonet

404
00:37:51,810 --> 00:37:57,180
up close, like there's research that most
soldiers will not shoot a human being at like

405
00:37:57,180 --> 00:37:58,780
less than 30 yards.

406
00:37:58,780 --> 00:38:00,050
They will shoot in the air.

407
00:38:00,050 --> 00:38:03,900
There will be a small percentage that will
actually shoot.

408
00:38:03,900 --> 00:38:10,040
But if you're just operating some drone platform,
you know, you're at a great, great distance

409
00:38:10,040 --> 00:38:15,420
and in the same way I would argue that the
distance that we have to the people that we

410
00:38:15,420 --> 00:38:22,040
collect data on, for instance, if we're creating
apps, it kind of trivializes in the moment

411
00:38:22,040 --> 00:38:27,710
the idea of what we're you know, what could
be the potential harmful implications for

412
00:38:27,710 --> 00:38:28,920
them.

413
00:38:28,920 --> 00:38:33,930
I have no doubt that the developers who wrote
that algorithm for prime risk assessment were

414
00:38:33,930 --> 00:38:35,480
not necessarily bad people.

415
00:38:35,480 --> 00:38:40,581
You know, maybe they're slightly leaning more
towards law and order, but I'm sure that they

416
00:38:40,581 --> 00:38:44,180
think their crime assessment algorithm is
relevant.

417
00:38:44,180 --> 00:38:51,290
The problem is, could, without transparency,
and first of all, you can ask is that ever

418
00:38:51,290 --> 00:38:53,100
a relevant way of doing justice?

419
00:38:53,100 --> 00:38:59,510
Do we want a drudge dread type of justice
where we have yes or no the moment you walk

420
00:38:59,510 --> 00:39:02,680
in the door?

421
00:39:02,680 --> 00:39:09,370
And if we do accept automated systems as part
of our crime system, how do we review them?

422
00:39:09,370 --> 00:39:14,100
You know, what is the transparency aspect
when the algorithms are proprietary.

423
00:39:14,100 --> 00:39:20,830
So I would say in general, you know, collect
only what you need, don't overstate your intentions,

424
00:39:20,830 --> 00:39:28,550
don't assume that what works for you works
for everyone else and don't blame the algorithm.

425
00:39:28,550 --> 00:39:34,190
By which I mean it's your fault, not the algorithm.

426
00:39:34,190 --> 00:39:40,230
So acknowledging that you have bias or privilege
is not the same as admitting fault or guilt.

427
00:39:40,230 --> 00:39:47,140
That's a very unproductive feeling, which
is, you know, a very reasonable thing to think.

428
00:39:47,140 --> 00:39:51,240
Particularly if you're confronted with something
that you are not necessarily proud of.

429
00:39:51,240 --> 00:39:55,270
But, you know, your intentions were good or
whatever.

430
00:39:55,270 --> 00:40:01,430
But acknowledging that you have bias is acknowledging
that you're alive, that you're a human being,

431
00:40:01,430 --> 00:40:05,370
like every human being has bias of one form
or another.

432
00:40:05,370 --> 00:40:07,350
You know, you select your friends based on
who you like.

433
00:40:07,350 --> 00:40:08,350
That's a bias.

434
00:40:08,350 --> 00:40:09,350
If you select them based on, you know, skin
color, that's a slightly different bias, but

435
00:40:09,350 --> 00:40:10,350
you know, they're related.

436
00:40:10,350 --> 00:40:11,350
So again, they're both known unknowns and
unknown unknowns, so acknowledging the fact

437
00:40:11,350 --> 00:40:14,850
that you might not know everything there is
to know is kind of a good place to start,

438
00:40:14,850 --> 00:40:17,730
and when all else fails, apologize.

439
00:40:17,730 --> 00:40:26,360
And make it a real apology, don't prevaricate
and say you know, we don't know how it was

440
00:40:26,360 --> 00:40:28,280
calibrated, all that stuff.

441
00:40:28,280 --> 00:40:35,480
But stereotypes in general are seductive because
they have an parent ability to explain behavior,

442
00:40:35,480 --> 00:40:42,680
right, they reaffirm our beliefs about the
world and they allow us to extrapolation decisions

443
00:40:42,680 --> 00:40:44,590
about people we meet.

444
00:40:44,590 --> 00:40:48,430
Most of the time that's fun and easy.

445
00:40:48,430 --> 00:40:57,070
All Europeans have this shared thing where
I meet a French person and I can be like you're

446
00:40:57,070 --> 00:41:05,700
surely a snob, right and they could say back
to me, oh, you're a Norwegian, you're a dark,

447
00:41:05,700 --> 00:41:13,860
depressive soul and I could be like yeah,
I'm not saying you have to eradicate the biases

448
00:41:13,860 --> 00:41:15,300
from your life.

449
00:41:15,300 --> 00:41:19,140
Just figure out when you should have them.

450
00:41:19,140 --> 00:41:27,290
I'm willing to offend a French person and
I know there's French people in here but I

451
00:41:27,290 --> 00:41:30,650
own that and so apologies in advance.

452
00:41:30,650 --> 00:41:38,800
And finally one thing that comes up constantly
and when I talk to women who are important

453
00:41:38,800 --> 00:41:45,520
researchers and creators within the field,
for instance I worked with this woman Madeline

454
00:41:45,520 --> 00:41:53,680
Gannon and she works with robot arms, and
upon first meeting I said, so, you're a woman

455
00:41:53,680 --> 00:41:57,510
in robotics, that's really not a useful phrase.

456
00:41:57,510 --> 00:42:03,270
That implies that you're something that's
not the regular thing in robotics.

457
00:42:03,270 --> 00:42:09,510
So expecting people who are potentially being
harmed or discriminated against to explain

458
00:42:09,510 --> 00:42:18,140
to you what is happening or stopping you from
doing it, is kind of -- that's a really bad

459
00:42:18,140 --> 00:42:19,140
solution.

460
00:42:19,140 --> 00:42:25,750
You know, like when the -- all eyes in the
room turned to the one Norwegian guy in the

461
00:42:25,750 --> 00:42:30,280
room and say, like, so how can we include
Norwegians in this conversation?

462
00:42:30,280 --> 00:42:33,320
That shouldn't be mine to answer.

463
00:42:33,320 --> 00:42:38,430
So to quote a good friend of mine, you can
either be the best or you can be the worst.

464
00:42:38,430 --> 00:42:39,480
All right.

465
00:42:39,480 --> 00:42:40,730
How are we on time?

466
00:42:40,730 --> 00:42:43,610
We've got, that was 45, right?

467
00:42:43,610 --> 00:42:46,980
ZEESHAN: You've got some time for questions.

468
00:42:46,980 --> 00:42:54,740
>> I did have the talk like that I actually
like promised is also in here.

469
00:42:54,740 --> 00:42:58,920
How many have implemented circle packing?

470
00:42:58,920 --> 00:43:00,530
Not more?

471
00:43:00,530 --> 00:43:03,130
How many computational geometry fans in the
room?

472
00:43:03,130 --> 00:43:04,810
Oh, that's better.

473
00:43:04,810 --> 00:43:19,110
Yeah, I heard some Martian cubes being mentioned.

474
00:43:19,110 --> 00:43:20,900
So this is circle packing.

475
00:43:20,900 --> 00:43:23,610
This is what you get when you just search
for circle packing.

476
00:43:23,610 --> 00:43:26,330
This is circle packing architecture.

477
00:43:26,330 --> 00:43:31,650
Architecture is great for looking at sort
of algorithms as creative cliches, because

478
00:43:31,650 --> 00:43:33,850
they're just cycling through them really rapidly.

479
00:43:33,850 --> 00:43:47,150
If you know what Grasshopper is, which is
like a patching based writing for Rino that's

480
00:43:47,150 --> 00:43:48,670
a good CAD tool.

481
00:43:48,670 --> 00:43:51,450
This is my circle packing project.

482
00:43:51,450 --> 00:43:56,150
I can like claim, well, I did it in 2007,
you know, in this field that's like a long

483
00:43:56,150 --> 00:43:57,310
time ago.

484
00:43:57,310 --> 00:44:01,799
[laughter]
Voronoi, also very popular.

485
00:44:01,799 --> 00:44:03,890
This will be in the PDF.

486
00:44:03,890 --> 00:44:04,890
This is a fun one.

487
00:44:04,890 --> 00:44:10,231
This is the grasshopper forums and it's a
request for quod to help them do what you

488
00:44:10,231 --> 00:44:13,450
see on the right which of course is my work.

489
00:44:13,450 --> 00:44:21,690
And I guess -- and inadvertent kudos, like
you know, the most sincere form of flattery

490
00:44:21,690 --> 00:44:24,340
is copying?

491
00:44:24,340 --> 00:44:30,990
I can't do this justice, I hope the justice
talk was worth it.

492
00:44:30,990 --> 00:44:37,590
I find those examples terrifying partly because
I work with a lot of people who might be the

493
00:44:37,590 --> 00:44:39,510
creators of these things.

494
00:44:39,510 --> 00:44:46,310
That's why I talk about it from my perspective
that I'm terrified of being a creator of a

495
00:44:46,310 --> 00:44:48,040
tool that's destructive.

496
00:44:48,040 --> 00:44:53,790
One thing -- I should include another half
to complement that which would be the tools

497
00:44:53,790 --> 00:44:58,810
that actually do the opposite that support
the people in taking back the power.

498
00:44:58,810 --> 00:45:01,070
And so on and so forth.

499
00:45:01,070 --> 00:45:02,070
And there are those tools.

500
00:45:02,070 --> 00:45:03,630
But I think it's not going to happen on its
own.

501
00:45:03,630 --> 00:45:04,630
Questions?

502
00:45:04,630 --> 00:45:05,710
I have a remark.

503
00:45:05,710 --> 00:45:23,040
From what I understand there is a new European
Union regulation coming up that is supposed

504
00:45:23,040 --> 00:45:31,690
to make the algorithm for the creators to
be able to provide explanation for their decisions,

505
00:45:31,690 --> 00:45:37,720
so it will be a pretty exciting change for
all the algorithm designers.

506
00:45:37,720 --> 00:45:39,660
>> Yeah, that's like page rank.

507
00:45:39,660 --> 00:45:52,270
Page rank is proprietary and maybe for good
reason.

508
00:45:52,270 --> 00:45:56,410
I haven't followed the sort of the recent
politics in Europe, living in Brooklyn, but

509
00:45:56,410 --> 00:46:00,860
in general, you know, like over here, when
Germany says to Facebook you're not allowed

510
00:46:00,860 --> 00:46:06,850
to collect that data, you know, the media
here is like, look at those crazy Germans,

511
00:46:06,850 --> 00:46:08,940
why shouldn't you be able to?

512
00:46:08,940 --> 00:46:22,660
Anyone else?

513
00:46:22,660 --> 00:46:36,390
Yup?

514
00:46:36,390 --> 00:46:37,390
>> I was wondering if in your experience,
you had run across anyone, who had, you know,

515
00:46:37,390 --> 00:46:38,390
noticed a problem and then corrected it in
some sense, like better requirements or better

516
00:46:38,390 --> 00:46:39,390
testing or more --
>> I wish I had some great examples.

517
00:46:39,390 --> 00:46:40,390
>> Can you repeat the question.

518
00:46:40,390 --> 00:46:44,670
>> Yeah, the question is am I aware of anyone
who's figured out that like whatever service

519
00:46:44,670 --> 00:46:48,930
they're working on has that kind of bias issue
and then corrected themselves.

520
00:46:48,930 --> 00:46:51,270
I'd have to say I'm not aware of it.

521
00:46:51,270 --> 00:46:57,380
I fervently hope that just like alien life
in the universe exists, I hope that it exists.

522
00:46:57,380 --> 00:46:58,390
I would love to hear that story.

523
00:46:58,390 --> 00:47:00,960
>> Airbnb is in the middle of working their
--

524
00:47:00,960 --> 00:47:02,829
>> Oh, right.

525
00:47:02,829 --> 00:47:08,430
>> So that there's less racial bias.

526
00:47:08,430 --> 00:47:13,680
>> That's an interesting case, because Airbnb
and that other neighborhood watch service,

527
00:47:13,680 --> 00:47:21,210
I don't remember the name of it, Next Door,
primarily the changes are more about terminology

528
00:47:21,210 --> 00:47:29,160
and reporting and it's like -- I used to have
to argue to my students that the web page

529
00:47:29,160 --> 00:47:33,750
design of Facebook is the same thing as like
designing like the house you're going to like

530
00:47:33,750 --> 00:47:39,230
act in, but even that kind of stuff, which
is fairly trivial to even like imagine and

531
00:47:39,230 --> 00:47:46,920
you'd wonder why they wouldn't do it, like,
in the first place, that stuff should get

532
00:47:46,920 --> 00:47:49,400
done.

533
00:47:49,400 --> 00:47:56,920
OkCupid is an interesting case there, where
you know, is everybody familiar with OkCupid?

534
00:47:56,920 --> 00:47:59,480
It's like datalog, etc.

535
00:47:59,480 --> 00:48:03,990
It's an interesting one because they have
a real dataset that represents dating.

536
00:48:03,990 --> 00:48:11,440
The problem is they have one article which
primarily shows, like, who is preferred, you

537
00:48:11,440 --> 00:48:14,810
know, as groups on OkCupid, but that's sort
of the problem.

538
00:48:14,810 --> 00:48:19,740
I don't show that to students because I don't
want to reinforce in my students' head that

539
00:48:19,740 --> 00:48:20,859
is how it is.

540
00:48:20,859 --> 00:48:23,130
That is one of the problems with data collection.

541
00:48:23,130 --> 00:48:25,810
You tend to believe that that's how it is.

542
00:48:25,810 --> 00:48:30,290
And you are not necessarily considering what
are the affordances that might lead to to

543
00:48:30,290 --> 00:48:31,920
be true.

544
00:48:31,920 --> 00:48:38,040
I'm not saying you should have some didactic,
you know, part of OkCupid like encouraging

545
00:48:38,040 --> 00:48:44,430
you to date along gender and racial lines,
that would be a little patronizing, but to

546
00:48:44,430 --> 00:48:53,440
think that it is just true that there is that
divide is not understanding the data.

547
00:48:53,440 --> 00:48:58,570
So yeah?

548
00:48:58,570 --> 00:49:02,830
>> So to piggyback on this idea that we as
developers should really be thinking actively

549
00:49:02,830 --> 00:49:07,300
about these questions, how much data do I
collect, what does it mean, I've kind of heard

550
00:49:07,300 --> 00:49:15,000
this similar thought with do-good data for
social justice and they find that people just

551
00:49:15,000 --> 00:49:18,660
don't understand the impacts of their data
or what kind of data they have.

552
00:49:18,660 --> 00:49:23,980
Can you recommend any resources that are good
for people to digest perhaps as an introduction

553
00:49:23,980 --> 00:49:27,170
to saying, these are actually the data.

554
00:49:27,170 --> 00:49:34,080
I'm curious if there's anything you can hem
that can help start that discourse.

555
00:49:34,080 --> 00:49:38,640
>> I can't recommend one offhand but I know
in the process of teaching one of the things

556
00:49:38,640 --> 00:49:43,820
that I've found is teaching students about
data, using like an existing big dataset is

557
00:49:43,820 --> 00:49:45,270
pretty much useless.

558
00:49:45,270 --> 00:49:52,109
Make them travel like their own travel through
the city, that gets creepy real fast.

559
00:49:52,109 --> 00:49:59,480
There was someone who actually did that at
SVA, where they collected the moves app, they

560
00:49:59,480 --> 00:50:04,070
collected all the moves data for all the students
in the class and sure enough, they found out

561
00:50:04,070 --> 00:50:10,880
who was dating, that stuff gets creepy because
of what you can see, and most people accept

562
00:50:10,880 --> 00:50:15,920
in the abstract that sure, you might be able
to see this, but I think most users are simply

563
00:50:15,920 --> 00:50:18,640
not aware of the power of it.

564
00:50:18,640 --> 00:50:20,320
>> Thank you.

565
00:50:20,320 --> 00:50:22,000
Thank you.

566
00:50:22,000 --> 00:50:24,890
>> Any final fun exciting questions?

567
00:50:24,890 --> 00:50:29,359
I think I need to lighten up that talk.

568
00:50:29,359 --> 00:50:36,020
[laughter]
>> I mean it's real, but it's a little -- for

569
00:50:36,020 --> 00:50:40,350
the record I should say I personally don't
develop any social justice tools, I'm not

570
00:50:40,350 --> 00:50:43,670
saying that, like, I know how to fix this.

571
00:50:43,670 --> 00:50:49,150
All I know is I have a lot of people who are
very good people and very smart developing

572
00:50:49,150 --> 00:50:52,620
tools, and I'm -- yeah, kind of terrified
of seeing where this goes.

573
00:50:52,620 --> 00:50:58,010
There have been multiple -- there are actually
some good examples where -- there's a tool

574
00:50:58,010 --> 00:51:03,390
called flood watch from the office for creative
research which helps you track like online,

575
00:51:03,390 --> 00:51:07,901
you know, tracking software, etc., that's
one of those areas where there is quite a

576
00:51:07,901 --> 00:51:08,990
lot of research.

577
00:51:08,990 --> 00:51:15,170
Ironically privacy in data has become sort
of the cool macho thing, which is like -- which

578
00:51:15,170 --> 00:51:19,380
is ironic, because that's actually a fairly
easy thing to track.

579
00:51:19,380 --> 00:51:22,970
Bias is a way more powerful and harder thing
to track.

580
00:51:22,970 --> 00:51:28,630
So Adam Harvey who I showed you earlier is
he's like really in the privacy thing.

581
00:51:28,630 --> 00:51:29,700
In an interesting way.

582
00:51:29,700 --> 00:51:33,859
Along with that project where you kind of
obscure your face from a facial recognition,

583
00:51:33,859 --> 00:51:41,130
he also has an anti-drone burka, which is
essentially a heat reflective material which

584
00:51:41,130 --> 00:51:45,550
does make you invisible to infrared cameras,
it's pretty amazing.

585
00:51:45,760 --> 00:51:47,160
Recommend looking at it

586
00:51:49,300 --> 00:51:49,800
ZEESHAN: Thank you very much.

587
00:51:49,800 --> 00:51:50,740
>> All right.

588
00:51:50,740 --> 00:51:51,460
Thank you.

589
00:51:51,460 --> 00:51:52,060
[applause]

